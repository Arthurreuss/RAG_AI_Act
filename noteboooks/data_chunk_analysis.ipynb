{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a662d6c9",
   "metadata": {},
   "source": [
    "# Dataset Chunking Strategy & Analysis\n",
    "\n",
    "### 1. Context & Objectives\n",
    "\n",
    "This notebook outlines the semantic chunking strategy employed to process the EU AI Act. Our primary objective was to transform the raw legal text into high-quality, retrieval-ready units.\n",
    "\n",
    "Legal texts present a unique challenge for RAG (Retrieval-Augmented Generation) pipelines: precision is paramount. A simple character-split would sever critical references (e.g., separating \"Article 5\" from its sub-paragraphs). Therefore, we adopted a structure-aware approach to ensure every chunk remains self-contained and semantically meaningful while adhering to token limits for optimal embedding performance.\n",
    "\n",
    "### 2. Methodology\n",
    "\n",
    "**Step 1: Structural Extraction**\n",
    "We began by parsing the official AI Act text into its native hierarchical format, separating the document into **Articles, Recitals, and Annexes**. Crucially, this extraction captured the full depth of the legal structure, nesting all sub-points (e.g., \"1.\", \"2.\") and sub-sub-points (e.g., \"(a)\", \"(b)\") as children of their overlying parent elements.\n",
    "\n",
    "* *Source:* `ai_act_extracted.json`\n",
    "* *Outcome:* A clean JSON hierarchy reflecting the complete official legal document structure, preserving parent-child relationships.\n",
    "\n",
    "**Step 2: Contextual Flattening**\n",
    "A major issue in legal chunking is \"orphan text.\" For example, a sub-point labeled \"(a)\" is meaningless without the preceding clause \"1. High-risk AI systems shall...\".\n",
    "To solve this, we implemented a **concatenation strategy**. We flattened the hierarchy by prepending parent headers to child elements.\n",
    "\n",
    "* *Source:* `ai_act_chunks.json`\n",
    "* *Transformation:* `Header` + `Sub-point` → `Unified Chunk`\n",
    "* *Outcome:* Every chunk now carries its full context, preventing semantic ambiguity.\n",
    "\n",
    "**Step 3: Size Optimization (Refining Oversized Chunks)**\n",
    "Initial profiling revealed a discrepancy in text density. While Articles are typically concise, Recitals (the explanatory background text) are often verbose. To prevent token overflow and context dilution, we applied a secondary **word-based split** specifically targeting these outliers.\n",
    "\n",
    "* *Source:* `ai_act_chunks_split.json`\n",
    "* *Transformation:* Any chunk exceeding **250 words** was split into smaller segments of roughly **150 words** with a **30-word overlap** to maintain narrative continuity.\n",
    "* *Outcome:* Smaller chunks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e460a716",
   "metadata": {},
   "source": [
    "### 3. Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fe67b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "from src.analysis.analyze_dataset import analyze_chunks\n",
    "\n",
    "\n",
    "project_root = Path.cwd().parent \n",
    "bin_size = 50\n",
    "max_threshold = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c643c6ce",
   "metadata": {},
   "source": [
    "**Phase 1: Initial Distribution (Before Splitting)**\n",
    "Our initial pass yielded **1,389 chunks**. The structural extraction worked well for the binding laws, but the interpretative sections were disproportionately heavy.\n",
    "\n",
    "* **Articles & Annexes:** These were naturally concise, averaging roughly **57–62 words** per chunk.\n",
    "* **Recitals (The Problem Area):** The 180 Recitals averaged **193 words**, significantly longer than the rest of the text. Crucially, **42 Recitals exceeded 250 words**, creating \"big-chunks\" that would likely degrade retrieval performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7779ba54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from: /Users/arthurreuss/code/Arthurreuss/Python/RUG/LTP/RAG_AI_Act/data/json/ai_act_chunks.json\n",
      "Analyzing 1389 chunks...\n",
      "\n",
      "TYPE                      | 0-50      | 50-100    | 100-150   | 150-200   | 200-250   | 250-300   | 300-350   | 350-400   | 400-450   | 450-500   | 500+      | TOTAL\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Annexes                   | 100       | 105       | 14        | 3         | .         | 2         | .         | .         | .         | .         | .         | 224\n",
      "Articles                  | 408       | 450       | 110       | 8         | 6         | 2         | .         | .         | 1         | .         | .         | 985\n",
      "Recital                   | 9         | 32        | 43        | 33        | 20        | 12        | 7         | 5         | 7         | 10        | 2         | 180\n",
      "\n",
      "============================================================\n",
      "AVERAGE LENGTHS BY TYPE\n",
      "============================================================\n",
      "TYPE                      | AVG WORDS  | AVG CHARS \n",
      "------------------------------------------------------------\n",
      "Annexes                   | 57.4       | 359.4     \n",
      "Articles                  | 62.4       | 391.5     \n",
      "Recital                   | 193.0      | 1266.6    \n",
      "\n",
      "================================================================================\n",
      "    OVERSIZED CHUNKS (> 500 words)\n",
      "================================================================================\n",
      "TYPE                 | ID / CITATION                            | WORDS    | CHARS   \n",
      "-------------------------------------------------------------------------------------\n",
      "Recital              | Recital 53                               | 731      | 4455    \n",
      "Recital              | Recital 29                               | 579      | 3717    \n"
     ]
    }
   ],
   "source": [
    "file = project_root / \"data\" / \"json\" / \"ai_act_chunks.json\"\n",
    "analyze_chunks(file, bin_size=bin_size, max_threshold=max_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4156b720",
   "metadata": {},
   "source": [
    "**Phase 2: Final Distribution (After Splitting)**\n",
    "After applying our size optimization logic, the dataset grew slightly to **1,518 chunks**, achieving a much healthier distribution.\n",
    "\n",
    "* **Normalization Success:** The average length of Recitals dropped from 193 words to a manageable **129 words**.\n",
    "* **Eliminating Outliers:** We successfully eliminated all chunks over 250 words. The bulk of the dataset now sits in the **50–200 word range**.\n",
    "* **Stability:** The Articles and Annexes remained largely untouched (increasing by only ~11 chunks total), confirming that our splitting logic correctly targeted only the problematic sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3de8288e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from: /Users/arthurreuss/code/Arthurreuss/Python/RUG/LTP/RAG_AI_Act/data/json/ai_act_chunks_split.json\n",
      "Analyzing 1518 chunks...\n",
      "\n",
      "TYPE                      | 0-50      | 50-100    | 100-150   | 150-200   | 200-250   | 250-300   | 300-350   | 350-400   | 400-450   | 450-500   | 500+      | TOTAL\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Annexes                   | 102       | 105       | 15        | 6         | .         | .         | .         | .         | .         | .         | .         | 228\n",
      "Articles                  | 409       | 452       | 110       | 15        | 6         | .         | .         | .         | .         | .         | .         | 992\n",
      "Recital                   | 29        | 47        | 65        | 137       | 20        | .         | .         | .         | .         | .         | .         | 298\n",
      "\n",
      "============================================================\n",
      "AVERAGE LENGTHS BY TYPE\n",
      "============================================================\n",
      "TYPE                      | AVG WORDS  | AVG CHARS \n",
      "------------------------------------------------------------\n",
      "Annexes                   | 57.1       | 356.9     \n",
      "Articles                  | 62.2       | 390.3     \n",
      "Recital                   | 129.3      | 847.1     \n",
      "\n",
      "================================================================================\n",
      "    OVERSIZED CHUNKS (> 500 words)\n",
      "================================================================================\n",
      "Good news! All chunks are within the limit.\n"
     ]
    }
   ],
   "source": [
    "file2 = project_root / \"data\" / \"json\" / \"ai_act_chunks_split.json\"\n",
    "analyze_chunks(file2, bin_size=bin_size, max_threshold=max_threshold)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-ai-act",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
