[
    {
        "question": "What are the primary purposes and intended outcomes of this Regulation concerning AI systems in the Union?",
        "ground_truth_answer": "The primary purposes of this Regulation are to improve the functioning of the internal market by establishing a uniform legal framework for the development, placing on the market, putting into service, and use of AI systems in the Union. It also aims to promote the uptake of human-centric and trustworthy artificial intelligence while ensuring a high level of protection of health, safety, fundamental rights (including democracy, the rule of law, and environmental protection), to protect against the harmful effects of AI systems, and to support innovation. Furthermore, the Regulation ensures the free movement, cross-border, of AI-based goods and services.",
        "context_id": "recital_1",
        "metadata_type": "recital"
    },
    {
        "question": "Which article of the Treaty on the Functioning of the European Union (TFEU) serves as the basis for laying down uniform obligations for operators and guaranteeing the uniform protection of overriding reasons of public interest and personal rights within the internal market for AI systems and related products?",
        "ground_truth_answer": "Article 114 of the Treaty on the Functioning of the European Union (TFEU) serves as the basis for these actions.",
        "context_id": "recital_3_part_2",
        "metadata_type": "recital"
    },
    {
        "question": "What is the primary purpose of the Union legal framework establishing harmonised rules on AI?",
        "ground_truth_answer": "The primary purpose is to foster the development, use, and uptake of AI in the internal market, while simultaneously meeting a high level of protection of public interests such as health and safety, and the protection of fundamental rights, including democracy, the rule of law, and environmental protection, and ensuring the smooth functioning of the internal market.",
        "context_id": "recital_8",
        "metadata_type": "recital"
    },
    {
        "question": "What is the relationship between the harmonised rules for high-risk AI systems laid down in this Regulation and existing Union law?",
        "ground_truth_answer": "The harmonised rules laid down in this Regulation are complementary to existing Union law, particularly on data protection, consumer protection, fundamental rights, employment, protection of workers, and product safety, and apply without prejudice to these existing Union laws.",
        "context_id": "recital_9_part_1",
        "metadata_type": "recital"
    },
    {
        "question": "What is the intent of the EU AI Act regarding existing Union law rights and remedies for consumers and other persons on whom AI systems may have a negative impact?",
        "ground_truth_answer": "The intent is that all rights and remedies provided for by Union law to consumers and other persons on whom AI systems may have a negative impact, including as regards the compensation of possible damages pursuant to Council Directive 85/374/EEC, remain unaffected and fully applicable.",
        "context_id": "recital_9_part_2",
        "metadata_type": "recital"
    },
    {
        "question": "What is the purpose of this Regulation concerning existing rights and remedies, especially regarding transparency, technical documentation, and record-keeping of AI systems?",
        "ground_truth_answer": "This Regulation aims to strengthen the effectiveness of existing rights and remedies by establishing specific requirements and obligations, including in respect of the transparency, technical documentation, and record-keeping of AI systems.",
        "context_id": "recital_9_part_3",
        "metadata_type": "recital"
    },
    {
        "question": "What is the relationship between the EU AI Act and existing Union law governing the processing of personal data, as outlined in Recital 10?",
        "ground_truth_answer": "The EU AI Act does not seek to affect the application of existing Union law governing the processing of personal data, including the tasks and powers of the independent supervisory authorities competent to monitor compliance with those instruments.",
        "context_id": "recital_10_part_1",
        "metadata_type": "recital"
    },
    {
        "question": "What is the intended effect of the harmonised rules for AI systems established under this Regulation concerning data subjects' rights?",
        "ground_truth_answer": "The harmonised rules for the placing on the market, the putting into service, and the use of AI systems established under this Regulation should facilitate the effective implementation and enable the exercise of the data subjects' rights.",
        "context_id": "recital_10_part_2",
        "metadata_type": "recital"
    },
    {
        "question": "What key characteristic distinguishes AI systems from simpler traditional software systems or programming approaches as defined in this Regulation, and what does this characteristic involve?",
        "ground_truth_answer": "AI systems are distinguished by their capability to infer. This capability involves the process of obtaining outputs such as predictions, content, recommendations, or decisions, which can influence physical and virtual environments, and also the capability to derive models or algorithms, or both, from inputs or data.",
        "context_id": "recital_12_part_1",
        "metadata_type": "recital"
    },
    {
        "question": "What does the term ‘machine-based’ refer to concerning AI systems, as described in Recital 12 (Part 2)?",
        "ground_truth_answer": "The term ‘machine-based’ refers to the fact that AI systems run on machines.",
        "context_id": "recital_12_part_2",
        "metadata_type": "recital"
    },
    {
        "question": "What examples of outputs can be generated by an AI system, reflecting the functions it performs?",
        "ground_truth_answer": "Outputs generated by an AI system, reflecting different functions it performs, can include predictions, content, recommendations, or decisions.",
        "context_id": "recital_12_part_3",
        "metadata_type": "recital"
    },
    {
        "question": "What type of AI systems are explicitly excluded from the notion of ‘biometric identification’ as defined in this Regulation?",
        "ground_truth_answer": "AI systems intended for biometric verification are excluded. This includes authentication, where the sole purpose is to confirm that a specific natural person is the person they claim to be, or to confirm the identity of a natural person for the sole purpose of accessing a service, unlocking a device, or gaining security access to premises.",
        "context_id": "recital_15",
        "metadata_type": "recital"
    },
    {
        "question": "What conditions must a biometric categorisation system meet to be considered a purely ancillary feature and therefore excluded from the rules of the EU AI Act?",
        "ground_truth_answer": "To be considered a purely ancillary feature, a biometric categorisation system must be intrinsically linked to another commercial service, meaning it cannot be used without the principal service for objective technical reasons, and its integration must not be a means to circumvent the applicability of the Regulation's rules.",
        "context_id": "recital_16",
        "metadata_type": "recital"
    },
    {
        "question": "How is a 'remote biometric identification system' functionally defined in this Regulation?",
        "ground_truth_answer": "A 'remote biometric identification system' is functionally defined as an AI system intended for the identification of natural persons without their active involvement, typically at a distance, through the comparison of a person’s biometric data with the biometric data contained in a reference database, irrespective of the particular technology, processes or types of biometric data used.",
        "context_id": "recital_17_part_1",
        "metadata_type": "recital"
    },
    {
        "question": "Why are certain biometric identification systems, specifically those confirming identity for access purposes, excluded from the stricter rules of the EU AI Act?",
        "ground_truth_answer": "Such systems are excluded because they confirm the identity of a natural person for the sole purpose of having access to a service, unlocking a device, or having security access to premises, and are likely to have a minor impact on fundamental rights of natural persons compared to remote biometric identification systems which may be used for processing biometric data of a large number of persons without their active involvement.",
        "context_id": "recital_17_part_2",
        "metadata_type": "recital"
    },
    {
        "question": "What is the EU AI Act's intent regarding the use of minor delays for AI systems considered 'real-time'?",
        "ground_truth_answer": "The intent is that there should be no scope for circumventing the rules of the Regulation on the ‘real-time’ use of the AI systems concerned by providing for minor delays.",
        "context_id": "recital_17_part_3",
        "metadata_type": "recital"
    },
    {
        "question": "What criteria define an 'emotion recognition system' under this Regulation, and what specific types of systems are explicitly excluded from this definition?",
        "ground_truth_answer": "An 'emotion recognition system' is defined as an AI system for the purpose of identifying or inferring emotions or intentions of natural persons on the basis of their biometric data. Systems explicitly excluded from this definition include those used for detecting physical states like pain or fatigue (such as systems for professional pilots or drivers to prevent accidents), and the mere detection of readily apparent expressions, gestures, or movements, unless these are utilized for identifying or inferring emotions.",
        "context_id": "recital_18",
        "metadata_type": "recital"
    },
    {
        "question": "What criteria define a 'publicly accessible space' for the purposes of this Regulation, and what types of spaces are included?",
        "ground_truth_answer": "A 'publicly accessible space' is defined as any physical space accessible to an undetermined number of natural persons, irrespective of whether it is privately or publicly owned, or the activity for which it is used. This includes spaces used for commerce (e.g., shops, restaurants), services (e.g., banks, hospitality), sport (e.g., swimming pools, stadiums), transport (e.g., stations, airports), entertainment (e.g., cinemas, museums), or leisure (e.g., public roads, parks). A space is also considered publicly accessible even if access is subject to certain predetermined conditions, regardless of potential capacity or security restrictions.",
        "context_id": "recital_19_part_1",
        "metadata_type": "recital"
    },
    {
        "question": "Under what circumstances should a space be classified as publicly accessible, even if there are capacity or security restrictions?",
        "ground_truth_answer": "A space should be classified as publicly accessible if, regardless of potential capacity or security restrictions, access is subject to certain predetermined conditions which can be fulfilled by an undetermined number of persons, such as the purchase of a ticket or title of transport, prior registration, or having a certain age.",
        "context_id": "recital_19_part_2",
        "metadata_type": "recital"
    },
    {
        "question": "Which types of physical spaces are explicitly considered not to be publicly accessible according to the EU AI Act's Recital 19?",
        "ground_truth_answer": "Physical spaces with indications or circumstances suggesting restricted access, such as signs prohibiting or limiting entry, are not considered publicly accessible. This also includes company and factory premises, offices and workplaces intended only for relevant employees and service providers, prisons, and border control areas. Online spaces are also explicitly excluded as they are not physical spaces.",
        "context_id": "recital_19_part_3",
        "metadata_type": "recital"
    },
    {
        "question": "Under what circumstances should certain AI systems, even if not placed on the market, put into service, or used in the Union, fall within the scope of this Regulation?",
        "ground_truth_answer": "This is the case, for example, where an operator established in the Union contracts certain services to an operator established in a third country in relation to an activity to be performed by an AI system that would qualify as high-risk. In those circumstances, the AI system used in a third country by the operator could process data lawfully collected in and transferred from the Union, and provide to the contracting operator in the Union the output of that AI system resulting from that processing, without that AI system being placed on the market, put into service or used in the Union.",
        "context_id": "recital_22_part_1",
        "metadata_type": "recital"
    },
    {
        "question": "Under what specific conditions does the EU AI Act not apply to public authorities of a third country and international organisations, and what is the rationale behind this exemption?",
        "ground_truth_answer": "The EU AI Act should not apply to public authorities of a third country and international organisations when they are acting in the framework of cooperation or international agreements concluded at Union or national level for law enforcement and judicial cooperation with the Union or the Member States. This exemption is intended to take into account existing arrangements and special needs for future cooperation with foreign partners with whom information and evidence is exchanged.",
        "context_id": "recital_22_part_2",
        "metadata_type": "recital"
    },
    {
        "question": "What is the responsibility of authorities competent for supervision of law enforcement and judicial authorities concerning international frameworks or agreements for cooperation?",
        "ground_truth_answer": "The authorities competent for supervision of the law enforcement and judicial authorities under this Regulation should assess whether those frameworks for cooperation or international agreements include adequate safeguards with respect to the protection of fundamental rights and freedoms of individuals.",
        "context_id": "recital_22_part_3",
        "metadata_type": "recital"
    },
    {
        "question": "What is the justification for excluding AI systems used for military and defence purposes from the scope of this Regulation?",
        "ground_truth_answer": "The exclusion for military and defence purposes is justified both by Article 4(2) TEU and by the specificities of the Member States’ and the common Union defence policy covered by Chapter 2 of Title V TEU. These policies are subject to public international law, which is considered the more appropriate legal framework for the regulation of AI systems in the context of military and defence activities, including the use of lethal force.",
        "context_id": "recital_24_part_1",
        "metadata_type": "recital"
    },
    {
        "question": "Under what circumstances would an AI system initially developed for military, defence, or national security purposes fall within the scope of the EU AI Act?",
        "ground_truth_answer": "An AI system developed for military, defence, or national security purposes would fall within the scope of the EU AI Act if it is used outside those contexts, either temporarily or permanently, for other purposes, such as civilian, humanitarian, law enforcement, or public security purposes. In such a scenario, the entity using the AI system for these other purposes must ensure its compliance with the Regulation.",
        "context_id": "recital_24_part_2",
        "metadata_type": "recital"
    },
    {
        "question": "How does the EU AI Act address AI systems that serve both military, defence, or national security purposes and non-excluded purposes like civilian or law enforcement applications?",
        "ground_truth_answer": "AI systems placed on the market or put into service for both an excluded purpose, such as military, defence, or national security, and one or more non-excluded purposes, like civilian or law enforcement, fall within the scope of this Regulation. Providers of such systems must ensure compliance with the Regulation. However, this inclusion does not affect the possibility for entities to use AI systems for national security, military, and defence purposes.",
        "context_id": "recital_24_part_3",
        "metadata_type": "recital"
    },
    {
        "question": "Under what conditions is an AI system, initially made available for civilian or law enforcement purposes, exempt from the scope of this Regulation?",
        "ground_truth_answer": "An AI system placed on the market for civilian or law enforcement purposes, which is used with or without modification for military, defence or national security purposes, should not fall within the scope of this Regulation, regardless of the type of entity carrying out those activities.",
        "context_id": "recital_24_part_4",
        "metadata_type": "recital"
    },
    {
        "question": "What is the intended impact of the EU AI Act on scientific research and development activity involving AI systems and models?",
        "ground_truth_answer": "The EU AI Act is intended to support innovation, respect freedom of science, and not undermine research and development activity. Consequently, AI systems and models specifically developed and put into service for the sole purpose of scientific research and development are excluded from its scope. Additionally, the Regulation should not otherwise affect scientific research and development activity on AI systems or models prior to being placed on the market or put into service, including product-oriented research, testing, and development.",
        "context_id": "recital_25_part_1",
        "metadata_type": "recital"
    },
    {
        "question": "What general principle applies to AI systems used in research and development activities regarding their compliance with the EU AI Act?",
        "ground_truth_answer": "While AI systems specifically developed and put into service for the sole purpose of scientific research and development are subject to an exclusion, any other AI system that may be used for research and development activities remains subject to the provisions of the EU AI Act. Furthermore, any research and development activity must be carried out in accordance with recognised ethical and professional standards for scientific research and applicable Union law.",
        "context_id": "recital_25_part_2",
        "metadata_type": "recital"
    },
    {
        "question": "What key regulatory measures are deemed necessary under the risk-based approach for AI systems?",
        "ground_truth_answer": "Under the risk-based approach for AI systems, it is necessary to prohibit certain unacceptable AI practices, to lay down requirements for high-risk AI systems and obligations for their relevant operators, and to establish transparency obligations for certain AI systems.",
        "context_id": "recital_26",
        "metadata_type": "recital"
    },
    {
        "question": "Why should certain AI practices, such as manipulative or exploitative ones, be prohibited under the EU AI Act?",
        "ground_truth_answer": "Certain AI practices, like manipulative, exploitative, and social control practices, should be prohibited because they are particularly harmful and abusive. They contradict Union values such as respect for human dignity, freedom, equality, democracy, and the rule of law, as well as fundamental rights enshrined in the Charter, including the right to non-discrimination, data protection, privacy, and the rights of the child.",
        "context_id": "recital_28",
        "metadata_type": "recital"
    },
    {
        "question": "What characteristics make certain AI systems that materially distort human behaviour particularly dangerous and subject to prohibition?",
        "ground_truth_answer": "AI systems are deemed particularly dangerous and should be prohibited when they have the objective or effect of materially distorting human behaviour, leading to significant harms, in particular sufficiently important adverse impacts on physical, psychological health, or financial interests.",
        "context_id": "recital_29_part_1",
        "metadata_type": "recital"
    },
    {
        "question": "What mechanisms and types of vulnerabilities might AI systems leverage to employ manipulative or deceptive techniques that impair a person's autonomy or free choice?",
        "ground_truth_answer": "AI systems could facilitate such manipulative or deceptive techniques through mechanisms like machine-brain interfaces or virtual reality, which allow for a higher degree of control over presented stimuli, potentially materially distorting behaviour in a significantly harmful manner. Additionally, they may exploit vulnerabilities arising from a person's age, disability within the meaning of Directive (EU) 2019/882, or a specific social or economic situation that makes them more vulnerable to exploitation.",
        "context_id": "recital_29_part_2",
        "metadata_type": "recital"
    },
    {
        "question": "Under what circumstances might an intention to distort behaviour not be assumed in relation to an AI system?",
        "ground_truth_answer": "An intention to distort behaviour may not be assumed when the distortion results from factors external to the AI system which are outside the control of the provider or the deployer, namely factors that may not be reasonably foreseeable.",
        "context_id": "recital_29_part_3",
        "metadata_type": "recital"
    },
    {
        "question": "Is it necessary for the provider or the deployer of an AI system to have the intention to cause significant harm for the prohibitions against manipulative or exploitative AI-enabled practices to apply?",
        "ground_truth_answer": "No, it is not necessary for the provider or the deployer to have the intention to cause significant harm, provided that such harm results from the manipulative or exploitative AI-enabled practices.",
        "context_id": "recital_29_part_4",
        "metadata_type": "recital"
    },
    {
        "question": "Which specific types of practices are explicitly stated not to be affected by the prohibitions on manipulative and exploitative practices within this Regulation, provided they comply with applicable law and standards?",
        "ground_truth_answer": "The prohibitions of manipulative and exploitative practices should not affect lawful practices in the context of medical treatment, such as psychological treatment of a mental disease or physical rehabilitation, when carried out in accordance with applicable law and medical standards (e.g., explicit consent). Additionally, common and legitimate commercial practices, such as in advertising, that comply with applicable law, should not be regarded as harmful manipulative AI-enabled practices.",
        "context_id": "recital_29_part_5",
        "metadata_type": "recital"
    },
    {
        "question": "According to Recital 30 of the EU AI Act, what specific types of biometric categorisation systems are prohibited, and what lawful activities are excluded from this prohibition?",
        "ground_truth_answer": "Recital 30 prohibits biometric categorisation systems that use natural persons’ biometric data, such as an individual’s face or fingerprint, to deduce or infer an individual’s political opinions, trade union membership, religious or philosophical beliefs, race, sex life or sexual orientation. However, this prohibition does not cover the lawful labelling, filtering or categorisation of biometric data sets acquired in line with Union or national law based on biometric data, like sorting images by hair colour or eye colour, which can be used in areas such as law enforcement.",
        "context_id": "recital_30",
        "metadata_type": "recital"
    },
    {
        "question": "What are the primary reasons for prohibiting AI systems that provide social scoring of natural persons by public or private actors?",
        "ground_truth_answer": "AI systems providing social scoring may lead to discriminatory outcomes and the exclusion of certain groups. They may violate the right to dignity and non-discrimination, and undermine the values of equality and justice.",
        "context_id": "recital_31",
        "metadata_type": "recital"
    },
    {
        "question": "What are the key concerns and risks associated with the use of AI systems for real-time remote biometric identification of natural persons in publicly accessible spaces for law enforcement?",
        "ground_truth_answer": "The use of such systems is particularly intrusive to rights and freedoms, potentially affecting the private life of a large population, evoking constant surveillance, and indirectly dissuading the exercise of freedom of assembly and other fundamental rights. Technical inaccuracies can also lead to biased and discriminatory effects, particularly concerning age, ethnicity, race, sex, or disabilities. Furthermore, the immediacy of impact and limited opportunities for checks or corrections in real-time operations carry heightened risks for concerned individuals.",
        "context_id": "recital_32",
        "metadata_type": "recital"
    },
    {
        "question": "What is the general approach to using certain AI systems for law enforcement, and under what conditions can exceptions be considered?",
        "ground_truth_answer": "The use of those systems for the purpose of law enforcement should generally be prohibited. Exceptions are permitted only in exhaustively listed and narrowly defined situations where the use is strictly necessary to achieve a substantial public interest, the importance of which outweighs the risks.",
        "context_id": "recital_33_part_1",
        "metadata_type": "recital"
    },
    {
        "question": "What is the purpose of establishing a threshold for custodial sentences or detention orders in national law?",
        "ground_truth_answer": "The purpose of establishing such a threshold is to ensure that the offence should be serious enough to potentially justify the use of ‘real-time’ remote biometric identification systems.",
        "context_id": "recital_33_part_2",
        "metadata_type": "recital"
    },
    {
        "question": "What types of critical infrastructure disruption are considered to pose an imminent threat to life or the physical safety of natural persons?",
        "ground_truth_answer": "An imminent threat to life or the physical safety of natural persons could result from a serious disruption or destruction of critical infrastructure, as defined in Article 2, point (4) of Directive (EU) 2022/2557, if it causes an imminent threat to life or physical safety, including through serious harm to the provision of basic supplies to the population or to the exercise of the core function of the State.",
        "context_id": "recital_33_part_3",
        "metadata_type": "recital"
    },
    {
        "question": "Under what specific circumstances can law enforcement, border control, immigration, or asylum authorities utilize information systems to identify individuals during identity checks without needing prior authorization from this Regulation?",
        "ground_truth_answer": "Law enforcement, border control, immigration, or asylum authorities can use information systems to identify individuals during identity checks without requiring prior authorization from this Regulation when a person either refuses to be identified or is unable to state or prove their identity. This includes scenarios such as a person involved in a crime, or being unwilling or unable to disclose their identity due to an accident or a medical condition.",
        "context_id": "recital_33_part_4",
        "metadata_type": "recital"
    },
    {
        "question": "What key considerations are important for ensuring the responsible and proportionate use of systems in exhaustively listed and narrowly defined situations?",
        "ground_truth_answer": "To ensure responsible and proportionate use, it is important to take into account the nature of the situation giving rise to the request, the consequences of the use for the rights and freedoms of all persons concerned, and the safeguards and conditions provided for with the use.",
        "context_id": "recital_34",
        "metadata_type": "recital"
    },
    {
        "question": "What type of authorization and from which authorities is required for the use of 'real-time' remote biometric identification systems in publicly accessible spaces for law enforcement purposes?",
        "ground_truth_answer": "Each use of a 'real-time' remote biometric identification system in publicly accessible spaces for the purpose of law enforcement should be subject to an express and specific authorisation by a judicial authority or by an independent administrative authority of a Member State whose decision is binding.",
        "context_id": "recital_35_part_1",
        "metadata_type": "recital"
    },
    {
        "question": "What are the requirements for law enforcement authorities when requesting authorization after the urgent use of an AI system, and what immediate actions must be taken if such an authorization is rejected?",
        "ground_truth_answer": "In situations of urgency, the law enforcement authority should request authorization without undue delay and at the latest within 24 hours, providing the reasons for not having been able to request it earlier. If the authorization is rejected, the use of real-time biometric identification systems linked to that authorization must cease with immediate effect, and all related data, including input data directly acquired and the results and outputs, should be discarded and deleted.",
        "context_id": "recital_35_part_2",
        "metadata_type": "recital"
    },
    {
        "question": "What kind of decision should not be based solely on the output of a remote biometric identification system?",
        "ground_truth_answer": "No decision producing an adverse legal effect on a person should be taken based solely on the output of the remote biometric identification system.",
        "context_id": "recital_35_part_3",
        "metadata_type": "recital"
    },
    {
        "question": "How do the rules of the EU AI Act interact with Directive (EU) 2016/680 regarding the processing of biometric data for real-time remote biometric identification by law enforcement in publicly accessible spaces?",
        "ground_truth_answer": "The rules of the EU AI Act, which prohibit, subject to certain exceptions, the use of AI systems for real-time remote biometric identification of natural persons in publicly accessible spaces for law enforcement, apply as lex specialis in respect of the rules on the processing of biometric data contained in Article 10 of Directive (EU) 2016/680, thereby regulating such use and processing in an exhaustive manner.",
        "context_id": "recital_38_part_1",
        "metadata_type": "recital"
    },
    {
        "question": "When real-time remote biometric identification systems are used in publicly accessible spaces for purposes other than law enforcement, including by competent authorities, what is the regulatory intent regarding their coverage under this Regulation's specific framework for law enforcement use and the requirement for authorisation?",
        "ground_truth_answer": "The intent is that such use should not be covered by the specific framework regarding real-time remote biometric identification for the purpose of law enforcement set by this Regulation. As a result, it should not be subject to the requirement of an authorisation under this Regulation or the applicable detailed rules of national law that may give effect to that authorisation.",
        "context_id": "recital_38_part_2",
        "metadata_type": "recital"
    },
    {
        "question": "What is the primary reason Ireland is not bound by specific provisions of the EU AI Act related to personal data processing in certain cooperation areas?",
        "ground_truth_answer": "Ireland is not bound by specific provisions of the EU AI Act, such as certain points of Article 5(1), Article 5(2) to (6), and Article 26(10), to the extent they relate to the processing of personal data by Member States carrying out activities within the scope of Chapter 4 or Chapter 5 of Title V of Part Three of the TFEU, and where Ireland is not bound by the rules governing judicial or police cooperation in criminal matters requiring compliance with provisions based on Article 16 TFEU. This is in accordance with Article 6a of Protocol No 21 on the position of the United Kingdom and Ireland in respect of the area of freedom, security and justice.",
        "context_id": "recital_40",
        "metadata_type": "recital"
    },
    {
        "question": "Which provisions of the EU AI Act are not binding on Denmark, in accordance with Protocol No 22 annexed to the TEU and TFEU, when related to police and judicial cooperation in criminal matters?",
        "ground_truth_answer": "In accordance with Protocol No 22, Denmark is not bound by rules laid down in Article 5(1), first subparagraph, point (g) (to the extent it applies to the use of biometric categorisation systems for activities in the field of police cooperation and judicial cooperation in criminal matters), Article 5(1), first subparagraph, point (d) (to the extent it applies to the use of AI systems covered by that provision), Article 5(1), first subparagraph, point (h), (2) to (6) and Article 26(10) of this Regulation, when these relate to the processing of personal data by Member States carrying out activities falling within the scope of Chapter 4 or Chapter 5 of Title V of Part Three of the TFEU.",
        "context_id": "recital_41",
        "metadata_type": "recital"
    },
    {
        "question": "Which types of risk assessments concerning natural persons are prohibited in the Union, and on what fundamental principle is this prohibition based?",
        "ground_truth_answer": "Risk assessments carried out with regard to natural persons in order to assess the likelihood of their offending or to predict the occurrence of an actual or potential criminal offence based solely on profiling them or on assessing their personality traits and characteristics are prohibited. This prohibition is based on the principle that natural persons in the Union should always be judged on their actual behaviour, rather than on AI-predicted behaviour based solely on their profiling, personality traits, or characteristics without reasonable suspicion of criminal activity based on objective verifiable facts and human assessment.",
        "context_id": "recital_42",
        "metadata_type": "recital"
    },
    {
        "question": "What is the rationale behind prohibiting AI systems that create or expand facial recognition databases by untargeted scraping of facial images from the internet or CCTV footage?",
        "ground_truth_answer": "The prohibition is because this practice adds to the feeling of mass surveillance and can lead to gross violations of fundamental rights, including the right to privacy.",
        "context_id": "recital_43",
        "metadata_type": "recital"
    },
    {
        "question": "What are the core concerns and justifications behind the prohibition of AI systems intended to detect the emotional state of individuals in workplace and education settings?",
        "ground_truth_answer": "The prohibition is justified by serious concerns regarding the scientific basis of AI systems that aim to identify or infer emotions, noting their limited reliability, lack of specificity, and limited generalisability, given how emotion expressions vary across cultures, situations, and individuals. Such systems, particularly when using biometric data, can lead to discriminatory outcomes and intrude upon the rights and freedoms of individuals. Moreover, the power imbalance in work and education contexts, combined with the intrusive nature of these systems, could result in detrimental or unfavourable treatment for natural persons or groups thereof.",
        "context_id": "recital_44",
        "metadata_type": "recital"
    },
    {
        "question": "When classifying an AI system as high risk, what specific factor related to fundamental rights is considered particularly relevant?",
        "ground_truth_answer": "The extent of the adverse impact caused by the AI system on the fundamental rights protected by the Charter is of particular relevance.",
        "context_id": "recital_48",
        "metadata_type": "recital"
    },
    {
        "question": "What is the intended purpose of amending existing regulations and directives, such as Regulation (EC) No 300/2008, regarding high-risk AI systems?",
        "ground_truth_answer": "The intended purpose is to ensure that the Commission takes into account the mandatory requirements for high-risk AI systems laid down in the AI Regulation when adopting any relevant delegated or implementing acts on the basis of those existing acts. This must be done on the basis of the technical and regulatory specificities of each sector, and without interfering with existing governance, conformity assessment, and enforcement mechanisms and authorities established therein.",
        "context_id": "recital_49",
        "metadata_type": "recital"
    },
    {
        "question": "Under what specific conditions are AI systems classified as high-risk if they are safety components of products or products themselves, according to Recital 50?",
        "ground_truth_answer": "AI systems are classified as high-risk if they are safety components of products, or which are themselves products, falling within the scope of certain Union harmonisation legislation listed in an annex to this Regulation, provided the product concerned undergoes the conformity assessment procedure with a third-party conformity assessment body pursuant to that relevant Union harmonisation legislation.",
        "context_id": "recital_50",
        "metadata_type": "recital"
    },
    {
        "question": "What is the rationale for classifying stand-alone AI systems as high-risk?",
        "ground_truth_answer": "Stand-alone AI systems are classified as high-risk if, in light of their intended purpose, they pose a high risk of harm to the health and safety or the fundamental rights of persons, taking into account both the severity of the possible harm and its probability of occurrence, and they are used in a number of specifically pre-defined areas specified in this Regulation.",
        "context_id": "recital_52",
        "metadata_type": "recital"
    },
    {
        "question": "For the purposes of the EU AI Act, how is an AI system that does not materially influence the outcome of decision-making to be understood?",
        "ground_truth_answer": "An AI system that does not materially influence the outcome of decision-making should be understood to be an AI system that does not have an impact on the substance, and thereby the outcome, of decision-making, whether human or automated.",
        "context_id": "recital_53_part_1",
        "metadata_type": "recital"
    },
    {
        "question": "Under what conditions might an AI system, even when employed in a context listed as high-risk, pose only limited risks that are not increased by its use?",
        "ground_truth_answer": "An AI system might pose only limited risks that are not increased when used in a high-risk context if it is intended to perform a narrow procedural task, such as transforming unstructured data into structured data, classifying incoming documents into categories, or detecting duplicates among applications, because these tasks are of such a narrow and limited nature. The second condition is if the AI system's task is intended to improve the result of a previously completed human activity that may be relevant for the purposes of high-risk uses.",
        "context_id": "recital_53_part_2",
        "metadata_type": "recital"
    },
    {
        "question": "Under what conditions would an AI system's risk be considered lowered, particularly when it interacts with previously completed human activities or decision-making patterns?",
        "ground_truth_answer": "An AI system's risk would be lowered if it is intended to improve the result of a previously completed human activity, acting as an additional layer, or if it is intended to detect decision-making patterns or deviations from prior decision-making patterns, where its use follows a previously completed human assessment which it is not meant to replace or influence without proper human review.",
        "context_id": "recital_53_part_3",
        "metadata_type": "recital"
    },
    {
        "question": "What is the intent behind the fourth condition for AI systems that perform only preparatory tasks for assessments relevant to systems listed in an annex to the EU AI Act?",
        "ground_truth_answer": "The intent of the fourth condition is to ensure that the possible impact of the output of such AI systems is very low, thereby representing a minimal risk for the assessment that is to follow.",
        "context_id": "recital_53_part_4",
        "metadata_type": "recital"
    },
    {
        "question": "For what purpose should a provider draw up documentation if they assess an AI system as not high-risk?",
        "ground_truth_answer": "A provider should draw up documentation of the assessment if an AI system is considered not high-risk to ensure traceability and transparency.",
        "context_id": "recital_53_part_5",
        "metadata_type": "recital"
    },
    {
        "question": "Why are remote biometric identification systems classified as high-risk under the EU AI Act?",
        "ground_truth_answer": "Remote biometric identification systems are classified as high-risk because biometric data constitutes a special category of personal data, and technical inaccuracies of these AI systems can lead to biased results and entail discriminatory effects, particularly with regard to age, ethnicity, race, sex or disabilities, thereby posing significant risks.",
        "context_id": "recital_54",
        "metadata_type": "recital"
    },
    {
        "question": "For what reasons are AI systems intended to be used as safety components in the management and operation of critical infrastructure, such as critical digital infrastructure, road traffic, and the supply of water, gas, heating, and electricity, classified as high-risk?",
        "ground_truth_answer": "AI systems intended to be used as safety components in the management and operation of such critical infrastructure are classified as high-risk because their failure or malfunctioning may put at risk the life and health of persons at a large scale and lead to appreciable disruptions in the ordinary conduct of social and economic activities.",
        "context_id": "recital_55",
        "metadata_type": "recital"
    },
    {
        "question": "What is the rationale behind classifying certain AI systems used in education or vocational training as high-risk under the EU AI Act?",
        "ground_truth_answer": "Certain AI systems in education or vocational training are classified as high-risk because they may determine a person’s educational and professional course of life, thereby affecting their ability to secure a livelihood. Improper design and use of such systems can be particularly intrusive, violate the right to education and training, the right not to be discriminated against, and perpetuate historical patterns of discrimination.",
        "context_id": "recital_56",
        "metadata_type": "recital"
    },
    {
        "question": "Why are AI systems used in employment, workers management, and access to self-employment classified as high-risk?",
        "ground_truth_answer": "AI systems in these areas are classified as high-risk because they may have an appreciable impact on future career prospects, livelihoods of individuals, and workers’ rights. Additionally, such systems can perpetuate historical patterns of discrimination and may undermine fundamental rights to data protection and privacy.",
        "context_id": "recital_57",
        "metadata_type": "recital"
    },
    {
        "question": "Why are AI systems used by authorities to manage benefits and services, as well as those used to evaluate credit scores or creditworthiness of natural persons, classified as high-risk under the EU AI Act?",
        "ground_truth_answer": "AI systems used by authorities to manage benefits and services are classified as high-risk because they may significantly impact persons’ livelihood and infringe their fundamental rights, such as the right to social protection, non-discrimination, human dignity, or an effective remedy. Similarly, AI systems used to evaluate the credit score or creditworthiness of natural persons are classified as high-risk because they determine those persons’ access to financial resources or essential services like housing, electricity, and telecommunication services.",
        "context_id": "recital_58_part_2",
        "metadata_type": "recital"
    },
    {
        "question": "Which specific types of AI systems, despite their relation to financial services, are explicitly excluded from being classified as high-risk under the EU AI Act?",
        "ground_truth_answer": "AI systems provided for by Union law for the purpose of detecting fraud in the offering of financial services and for prudential purposes to calculate credit institutions’ and insurance undertakings’ capital requirements are not considered high-risk under this Regulation.",
        "context_id": "recital_58_part_3",
        "metadata_type": "recital"
    },
    {
        "question": "What is the justification for classifying AI systems used to evaluate and classify emergency calls, dispatch emergency first response services, or for emergency healthcare patient triage as high-risk?",
        "ground_truth_answer": "AI systems used for these purposes should be classified as high-risk because they make decisions in very critical situations for the life and health of persons and their property.",
        "context_id": "recital_58_part_4",
        "metadata_type": "recital"
    },
    {
        "question": "Why is it appropriate to classify certain AI systems intended for use in the law enforcement context as high-risk?",
        "ground_truth_answer": "It is appropriate to classify certain AI systems as high-risk in the law enforcement context because accuracy, reliability, and transparency are particularly important to avoid adverse impacts, retain public trust, and ensure accountability and effective redress. This classification also addresses the concern that fundamental rights, such as the right to an effective remedy, a fair trial, the right of defence, and the presumption of innocence, could be hampered, especially if such AI systems are not sufficiently transparent, explainable, and documented.",
        "context_id": "recital_59_part_2",
        "metadata_type": "recital"
    },
    {
        "question": "For which specific types of administrative proceedings are AI systems mentioned in this recital intended to be used?",
        "ground_truth_answer": "AI systems are specifically intended to be used for administrative proceedings by tax and customs authorities, as well as by financial intelligence units carrying out administrative tasks.",
        "context_id": "recital_59_part_3",
        "metadata_type": "recital"
    },
    {
        "question": "What are some of the intended applications of AI systems in the fields of migration, asylum, and border control management, according to this recital?",
        "ground_truth_answer": "AI systems in the fields of migration, asylum, and border control management are intended for uses such as polygraphs and similar tools for assessing risks posed by natural persons entering a Member State or applying for visa or asylum. They are also meant to assist competent public authorities in the examination of applications for asylum, visa, and residence permits, including assessing the reliability of evidence, to establish the eligibility of applicants. Additionally, these systems serve the purpose of detecting, recognising, or identifying natural persons in this context, excluding the verification of travel documents.",
        "context_id": "recital_60_part_2",
        "metadata_type": "recital"
    },
    {
        "question": "What international obligations should AI systems used in migration, asylum, and border control management not be used to circumvent?",
        "ground_truth_answer": "AI systems used in migration, asylum, and border control management should not be used to circumvent international obligations under the UN Convention relating to the Status of Refugees done at Geneva on 28 July 1951 as amended by the Protocol of 31 January 1967.",
        "context_id": "recital_60_part_3",
        "metadata_type": "recital"
    },
    {
        "question": "What is the rationale for classifying certain AI systems intended for the administration of justice and democratic processes as high-risk?",
        "ground_truth_answer": "These AI systems are classified as high-risk due to their potentially significant impact on democracy, the rule of law, individual freedoms, and the right to an effective remedy and a fair trial. The classification also aims to address the risks of potential biases, errors, and opacity associated with such systems.",
        "context_id": "recital_61",
        "metadata_type": "recital"
    },
    {
        "question": "What is the primary rationale for classifying AI systems intended to influence the outcome of an election or referendum, or the voting behaviour of natural persons, as high-risk?",
        "ground_truth_answer": "AI systems intended to influence the outcome of an election or referendum or the voting behaviour of natural persons are classified as high-risk to address the risks of undue external interference with the right to vote enshrined in Article 39 of the Charter, and of adverse effects on democracy and the rule of law.",
        "context_id": "recital_62",
        "metadata_type": "recital"
    },
    {
        "question": "What is the intended nature and purpose of the risk-management system for high-risk AI systems?",
        "ground_truth_answer": "The risk-management system should consist of a continuous, iterative process that is planned and run throughout the entire lifecycle of a high-risk AI system, primarily aimed at identifying and mitigating relevant risks of AI systems on health, safety, and fundamental rights.",
        "context_id": "recital_65_part_1",
        "metadata_type": "recital"
    },
    {
        "question": "What considerations should a provider of high-risk AI systems include when identifying reasonably foreseeable misuse, even if not directly covered by the intended purpose?",
        "ground_truth_answer": "When identifying the reasonably foreseeable misuse of high-risk AI systems, the provider should cover uses which, while not directly covered by the intended purpose and provided for in the instruction for use, may nevertheless be reasonably expected to result from readily predictable human behaviour in the context of the specific characteristics and use of a particular AI system.",
        "context_id": "recital_65_part_2",
        "metadata_type": "recital"
    },
    {
        "question": "What specific aspect should data governance and management practices encompass when dealing with personal data to facilitate compliance with Union data protection law?",
        "ground_truth_answer": "To facilitate compliance with Union data protection law, such as Regulation (EU) 2016/679, data governance and management practices should include, in the case of personal data, transparency about the original purpose of the data.",
        "context_id": "recital_67_part_1",
        "metadata_type": "recital"
    },
    {
        "question": "What key characteristics should data sets for high-risk AI systems exhibit, and what specific concern regarding these data sets warrants particular attention?",
        "ground_truth_answer": "Data sets for high-risk AI systems should have appropriate statistical properties, including as regards the persons or groups of persons in relation to whom the high-risk AI system is intended to be used. Specific attention should be given to the mitigation of possible biases in these data sets, because such biases are likely to affect the health and safety of persons, have a negative impact on fundamental rights, or lead to discrimination prohibited under Union law, especially where data outputs influence inputs for future operations (feedback loops).",
        "context_id": "recital_67_part_2",
        "metadata_type": "recital"
    },
    {
        "question": "What specific factors should data sets for AI systems consider, based on their intended purpose, regarding the environment in which the AI system will be used?",
        "ground_truth_answer": "To the extent required by their intended purpose, data sets should take into account the features, characteristics, or elements that are particular to the specific geographical, contextual, behavioural, or functional setting where the AI system is intended to be used.",
        "context_id": "recital_67_part_3",
        "metadata_type": "recital"
    },
    {
        "question": "What fundamental rights related to data must be guaranteed throughout the entire lifecycle of an AI system, and which Union data protection principles are applicable when personal data are processed?",
        "ground_truth_answer": "The right to privacy and to protection of personal data must be guaranteed throughout the entire lifecycle of the AI system. When personal data are processed, the principles of data minimisation and data protection by design and by default, as set out in Union data protection law, are applicable.",
        "context_id": "recital_69",
        "metadata_type": "recital"
    },
    {
        "question": "What public interest justification enables providers to process special categories of personal data for high-risk AI systems, and for what specific aim must this processing be strictly necessary, according to Recital 70?",
        "ground_truth_answer": "The processing is enabled as a matter of substantial public interest within the meaning of Article 9(2), point (g) of Regulation (EU) 2016/679 and Article 10(2), point (g) of Regulation (EU) 2018/1725. It must be strictly necessary for the purpose of ensuring bias detection and correction in relation to high-risk AI systems.",
        "context_id": "recital_70",
        "metadata_type": "recital"
    },
    {
        "question": "What is the primary reason why comprehensible information regarding the development and performance of high-risk AI systems is considered essential?",
        "ground_truth_answer": "Comprehensible information on how high-risk AI systems have been developed and how they perform throughout their lifetime is essential to enable traceability of those systems, verify compliance with the requirements under this Regulation, as well as monitoring of their operations and post market monitoring.",
        "context_id": "recital_71",
        "metadata_type": "recital"
    },
    {
        "question": "What are the primary reasons for requiring transparency for high-risk AI systems before they are placed on the market or put into service?",
        "ground_truth_answer": "Transparency should be required to address concerns related to opacity and complexity of certain AI systems and to help deployers fulfil their obligations under this Regulation.",
        "context_id": "recital_72_part_1",
        "metadata_type": "recital"
    },
    {
        "question": "What is the intended purpose for designing and developing high-risk AI systems with human oversight?",
        "ground_truth_answer": "High-risk AI systems should be designed and developed to enable natural persons to oversee their functioning, ensure they are used as intended, and address their impacts over the system’s lifecycle.",
        "context_id": "recital_73_part_1",
        "metadata_type": "recital"
    },
    {
        "question": "What enhanced human oversight requirement is deemed appropriate for certain biometric identification systems and why, according to Recital 73 (Part 2)?",
        "ground_truth_answer": "Enhanced human oversight is considered appropriate for certain biometric identification systems due to the significant consequences for persons in the case of an incorrect match. This requirement stipulates that no action or decision may be taken by the deployer based on the identification resulting from such a system unless it has been separately verified and confirmed by at least two natural persons.",
        "context_id": "recital_73_part_2",
        "metadata_type": "recital"
    },
    {
        "question": "Under what specific conditions and in which areas does the requirement for verification by at least two natural persons not apply?",
        "ground_truth_answer": "The requirement for verification by at least two natural persons does not apply in the areas of law enforcement, migration, border control, and asylum where Union or national law considers the application of that requirement to be disproportionate.",
        "context_id": "recital_73_part_3",
        "metadata_type": "recital"
    },
    {
        "question": "What is the primary reason why technical robustness is a key requirement for high-risk AI systems?",
        "ground_truth_answer": "Technical robustness is a key requirement for high-risk AI systems to ensure they are resilient against harmful or undesirable behaviour that may arise from system limitations or environmental factors like errors, faults, inconsistencies, or unexpected situations.",
        "context_id": "recital_75",
        "metadata_type": "recital"
    },
    {
        "question": "Why is cybersecurity considered crucial for AI systems according to Recital 76 of the EU AI Act?",
        "ground_truth_answer": "Cybersecurity is considered crucial because it ensures that AI systems are resilient against malicious third parties attempting to alter their use, behaviour, performance, or compromise their security properties by exploiting system vulnerabilities. Providers of high-risk AI systems are expected to take suitable measures to achieve an appropriate level of cybersecurity.",
        "context_id": "recital_76",
        "metadata_type": "recital"
    },
    {
        "question": "How can high-risk AI systems that are also subject to a horizontal cybersecurity regulation for products with digital elements demonstrate compliance with the cybersecurity requirements of the EU AI Act?",
        "ground_truth_answer": "High-risk AI systems which fall within the scope of a horizontal cybersecurity regulation for products with digital elements can demonstrate compliance with the cybersecurity requirements of the EU AI Act by fulfilling the essential cybersecurity requirements set out in that horizontal regulation. This compliance should be demonstrated in the EU declaration of conformity or parts thereof issued under that horizontal regulation, and the associated cybersecurity risk assessment must consider risks to cyber resilience, including AI-specific vulnerabilities like data poisoning or adversarial attacks, as well as risks to fundamental rights.",
        "context_id": "recital_77",
        "metadata_type": "recital"
    },
    {
        "question": "What is the primary reason for providing a derogation concerning the conformity assessment procedure for high-risk AI systems that are also classified as important and critical products with digital elements?",
        "ground_truth_answer": "A derogation is provided to ensure that the necessary level of assurance for critical products with digital elements is not reduced, even when they are also high-risk AI systems covered by the conformity assessment procedure under this Regulation.",
        "context_id": "recital_78_part_1",
        "metadata_type": "recital"
    },
    {
        "question": "For important and critical products with digital elements, how is the conformity assessment procedure divided between horizontal cybersecurity requirements and the aspects covered by this Regulation?",
        "ground_truth_answer": "For important and critical products with digital elements, the conformity assessment provisions of a regulation on horizontal cybersecurity requirements apply insofar as the essential cybersecurity requirements of that regulation are concerned. For all other aspects covered by this Regulation, the respective provisions on conformity assessment based on internal control set out in an annex to this Regulation should apply.",
        "context_id": "recital_78_part_2",
        "metadata_type": "recital"
    },
    {
        "question": "What is the intended outcome of applying universal design principles to new technologies and services, particularly concerning AI technologies?",
        "ground_truth_answer": "The application of universal design principles to all new technologies and services should ensure full and equal access for everyone potentially affected by or using AI technologies, including persons with disabilities, in a way that takes full account of their inherent dignity and diversity.",
        "context_id": "recital_80",
        "metadata_type": "recital"
    },
    {
        "question": "What is the intended approach for providers of high-risk AI systems who already have quality management systems under relevant sectoral Union law, regarding the quality management system elements required by the EU AI Act?",
        "ground_truth_answer": "Providers of high-risk AI systems that are subject to obligations regarding quality management systems under relevant sectoral Union law should have the possibility to include the elements of the quality management system provided for in this Regulation as part of their existing quality management system provided for in that other sectoral Union law.",
        "context_id": "recital_81",
        "metadata_type": "recital"
    },
    {
        "question": "Under what specific conditions might a distributor, importer, or deployer be considered a provider of a high-risk AI system and assume the relevant obligations?",
        "ground_truth_answer": "A distributor, importer, or deployer may be considered a provider of a high-risk AI system if that party puts its name or trademark on a high-risk AI system already placed on the market or put into service (without prejudice to contractual arrangements), if that party makes a substantial modification to a high-risk AI system that has already been placed on the market or put into service in a way that it remains a high-risk AI system, or if it modifies the intended purpose of an AI system, including a general-purpose AI system, which has not been classified.",
        "context_id": "recital_84_part_1",
        "metadata_type": "recital"
    },
    {
        "question": "How do the provisions of the EU AI Act relate to more specific Union harmonisation legislation based on the New Legislative Framework?",
        "ground_truth_answer": "The provisions of the EU AI Act should apply without prejudice to more specific provisions established in certain Union harmonisation legislation based on the New Legislative Framework. This means the AI Act should apply together with such legislation. For instance, Article 16(2) of Regulation (EU) 2017/745, which states that certain changes are not considered modifications affecting device compliance, will continue to apply to high-risk AI systems that are medical devices under that Regulation.",
        "context_id": "recital_84_part_2",
        "metadata_type": "recital"
    },
    {
        "question": "What is the intended collaborative responsibility of parties supplying AI systems, tools, or components integrated into a high-risk AI system towards the provider of that system, and what is the underlying reason for this requirement?",
        "ground_truth_answer": "Parties supplying AI systems, tools, services, components or processes that are incorporated into a high-risk AI system should, by written agreement, provide the provider with the necessary information, capabilities, technical access and other assistance based on the generally acknowledged state of the art. This is intended to enable the provider to fully comply with the obligations set out in the Regulation, without compromising the suppliers' own intellectual property rights or trade secrets.",
        "context_id": "recital_88",
        "metadata_type": "recital"
    },
    {
        "question": "What is the rationale for establishing specific responsibilities for deployers of AI systems?",
        "ground_truth_answer": "Specific responsibilities are established for deployers of AI systems due to the nature of AI systems and the potential risks to safety and fundamental rights associated with their use, including the need to ensure proper monitoring of an AI system's performance in a real-life setting.",
        "context_id": "recital_91",
        "metadata_type": "recital"
    },
    {
        "question": "What is the primary reason deployers of high-risk AI systems are considered to play a critical role in ensuring the protection of fundamental rights?",
        "ground_truth_answer": "Deployers of high-risk AI systems are critical because risks can stem not only from the design but also from how such systems are used. They are best positioned to understand the concrete context of use, the persons or groups likely to be affected (including vulnerable groups), and can therefore identify potential significant risks that were not foreseen during the development phase.",
        "context_id": "recital_93",
        "metadata_type": "recital"
    },
    {
        "question": "What legal framework and principles govern the processing of biometric data by AI systems for biometric identification in the context of law enforcement?",
        "ground_truth_answer": "The processing of biometric data by AI systems for biometric identification for law enforcement must comply with Article 10 of Directive (EU) 2016/680. This allows such processing only where strictly necessary, subject to appropriate safeguards for data subjects' rights and freedoms, and where authorised by Union or Member State law. When authorised, it also needs to respect the principles laid down in Article 4(1) of Directive (EU) 2016/680, including lawfulness, fairness, transparency, purpose limitation, accuracy, and storage limitation.",
        "context_id": "recital_94",
        "metadata_type": "recital"
    },
    {
        "question": "What is the primary reason why post-remote biometric identification systems should be subject to safeguards?",
        "ground_truth_answer": "Post-remote biometric identification systems should be subject to safeguards due to their intrusive nature.",
        "context_id": "recital_95",
        "metadata_type": "recital"
    },
    {
        "question": "Which entities are expected to carry out a fundamental rights impact assessment prior to putting certain high-risk AI systems into use?",
        "ground_truth_answer": "Deployers of high-risk AI systems that are bodies governed by public law, private entities providing public services, and deployers of certain high-risk AI systems listed in an annex to this Regulation, such as banking or insurance entities.",
        "context_id": "recital_96_part_1",
        "metadata_type": "recital"
    },
    {
        "question": "What is the fundamental purpose of an impact assessment concerning high-risk AI systems?",
        "ground_truth_answer": "The fundamental purpose of an impact assessment is to identify the specific risks to the rights of individuals or groups of individuals likely to be affected, to identify measures to be taken in the case of a materialisation of those risks, and to identify specific risks of harm likely to have an impact on the fundamental rights.",
        "context_id": "recital_96_part_2",
        "metadata_type": "recital"
    },
    {
        "question": "What actions should deployers take once risks are identified, and what types of measures are mentioned as examples to mitigate fundamental rights risks?",
        "ground_truth_answer": "Once risks are identified, deployers should determine measures to be taken if those risks materialize. Examples of such measures, instrumental in mitigating fundamental rights risks in concrete use-cases, include governance arrangements like human oversight according to the instructions of use, or complaint handling and redress procedures.",
        "context_id": "recital_96_part_3",
        "metadata_type": "recital"
    },
    {
        "question": "What is the primary reason for clearly defining general-purpose AI models and differentiating them from AI systems?",
        "ground_truth_answer": "The primary reason for clearly defining general-purpose AI models and distinguishing them from AI systems is to enable legal certainty.",
        "context_id": "recital_97_part_1",
        "metadata_type": "recital"
    },
    {
        "question": "When do the obligations for providers of general-purpose AI models apply, and how are these models considered 'placed on the market' if integrated into an AI system by the same provider?",
        "ground_truth_answer": "The obligations for providers of general-purpose AI models apply once the models are placed on the market. If a provider integrates an own general-purpose AI model into its own AI system that is then made available on the market or put into service, that model is considered to be placed on the market.",
        "context_id": "recital_97_part_2",
        "metadata_type": "recital"
    },
    {
        "question": "Under what specific conditions do the obligations laid down for models not apply when an own model is used for internal processes?",
        "ground_truth_answer": "The obligations laid down for models do not apply when an own model is used for purely internal processes that are not essential for providing a product or a service to third parties, and the rights of natural persons are not affected.",
        "context_id": "recital_97_part_3",
        "metadata_type": "recital"
    },
    {
        "question": "Which specific characteristics should a model possess to be considered as displaying significant generality and being able to competently perform a wide range of distinctive tasks?",
        "ground_truth_answer": "Models with at least a billion parameters, trained with a large amount of data, and utilizing self-supervision at scale should be considered to display significant generality and competently perform a wide range of distinctive tasks.",
        "context_id": "recital_98",
        "metadata_type": "recital"
    },
    {
        "question": "What is the primary reason for laying down proportionate transparency measures for providers of general-purpose AI models?",
        "ground_truth_answer": "Proportionate transparency measures should be laid down because providers of general-purpose AI models have a particular role and responsibility along the AI value chain, as the models they provide may form the basis for a range of downstream systems. This necessitates a good understanding of the models and their capabilities by downstream providers, both for integration into their products and to fulfill their obligations under this or other regulations.",
        "context_id": "recital_101",
        "metadata_type": "recital"
    },
    {
        "question": "Under what conditions do free and open-source AI components, as defined by this Regulation, not benefit from the exceptions provided to them?",
        "ground_truth_answer": "Free and open-source AI components do not benefit from the exceptions if they are provided against a price or otherwise monetised, including through the provision of technical support or other services related to the AI component, or the use of personal data for reasons other than exclusively for improving the security, compatibility, or interoperability of the software. However, transactions between microenterprises are an exception to these conditions.",
        "context_id": "recital_103",
        "metadata_type": "recital"
    },
    {
        "question": "What transparency-related requirements are not subject to exceptions for providers of general-purpose AI models released under a free and open-source licence, regardless of other conditions?",
        "ground_truth_answer": "Even when generally subject to exceptions, providers of general-purpose AI models released under a free and open-source licence remain obligated to produce a summary about the content used for model training and to implement a policy to comply with Union copyright law, particularly identifying and complying with the reservation of rights pursuant to Article 4(3) of Directive (EU) 2019/790.",
        "context_id": "recital_104",
        "metadata_type": "recital"
    },
    {
        "question": "Under what conditions must providers of general-purpose AI models obtain authorization from rightsholders for text and data mining activities?",
        "ground_truth_answer": "Providers of general-purpose AI models must obtain authorization from rightsholders if the right to opt out has been expressly reserved in an appropriate manner over the works or other subject matter, unless the text and data mining is done for the purposes of scientific research.",
        "context_id": "recital_105",
        "metadata_type": "recital"
    },
    {
        "question": "What is the primary objective behind requiring providers of general-purpose AI models to comply with Union copyright standards, regardless of the jurisdiction where training acts take place?",
        "ground_truth_answer": "The primary objective is to ensure a level playing field among providers of general-purpose AI models, preventing any provider from gaining a competitive advantage in the Union market by applying lower copyright standards than those provided in the Union.",
        "context_id": "recital_106",
        "metadata_type": "recital"
    },
    {
        "question": "What is the intended purpose for requiring providers of general-purpose AI models to make publicly available a sufficiently detailed summary of the content used for training these models?",
        "ground_truth_answer": "The intended purpose is to increase transparency on the data used in the pre-training and training of general-purpose AI models, including text and data protected by copyright law, and to facilitate parties with legitimate interests, such as copyright holders, to exercise and enforce their rights under Union law.",
        "context_id": "recital_107",
        "metadata_type": "recital"
    },
    {
        "question": "What types of negative effects are identified as systemic risks that general-purpose AI models could pose, and what factors are understood to influence these risks?",
        "ground_truth_answer": "Systemic risks include any actual or reasonably foreseeable negative effects relating to major accidents, disruptions of critical sectors, serious consequences to public health and safety, negative effects on democratic processes, public and economic security, and the dissemination of illegal, false, or discriminatory content. These risks are understood to increase with model capabilities and reach, and are influenced by conditions of misuse, model reliability, model fairness and model security, the level of autonomy of the model, its access to tools, novel or combined modalities, release and distribution strategies, the potential to remove guardrails, and other factors.",
        "context_id": "recital_110_part_1",
        "metadata_type": "recital"
    },
    {
        "question": "What specific categories of risks, according to international approaches, require attention regarding AI?",
        "ground_truth_answer": "International approaches have identified a need to pay attention to risks from potential intentional misuse or unintended issues of control relating to alignment with human intent; chemical, biological, radiological, and nuclear risks, such as the ways in which barriers to entry can be lowered for weapons development; offensive cyber capabilities; the effects of interaction and tool use, including the capacity to control physical systems and interfere with critical infrastructure; risks from models making copies of themselves or ‘self-replicating’ or training other models; the ways in which models can give rise to harmful bias and discrimination; and the facilitation of disinformation or harming privacy with threats to democratic values and human rights.",
        "context_id": "recital_110_part_2",
        "metadata_type": "recital"
    },
    {
        "question": "Under what conditions should a general-purpose AI model be considered to present systemic risks?",
        "ground_truth_answer": "A general-purpose AI model should be considered to present systemic risks if it has high-impact capabilities, evaluated on the basis of appropriate technical tools and methodologies, or significant impact on the internal market due to its reach.",
        "context_id": "recital_111_part_1",
        "metadata_type": "recital"
    },
    {
        "question": "What is the intended consequence for a general-purpose AI model if it meets the initial threshold of floating point operations?",
        "ground_truth_answer": "If a general-purpose AI model meets this initial threshold of floating point operations, it leads to a presumption that the model is a general-purpose AI model with systemic risks.",
        "context_id": "recital_111_part_2",
        "metadata_type": "recital"
    },
    {
        "question": "Under what conditions can the Commission individually designate a general-purpose AI model as having systemic risk?",
        "ground_truth_answer": "The Commission can take individual decisions to designate a general-purpose AI model as having systemic risk if it is found that such model has capabilities or an impact equivalent to those captured by the set threshold.",
        "context_id": "recital_111_part_3",
        "metadata_type": "recital"
    },
    {
        "question": "Under what conditions might a general-purpose AI model, previously designated as presenting systemic risk, be reassessed by the Commission?",
        "ground_truth_answer": "A general-purpose AI model, previously designated as presenting systemic risk, might be reassessed by the Commission upon a reasoned request from its provider. The Commission should take this request into account and may decide to reassess whether the general-purpose AI model can still be considered to present systemic risks.",
        "context_id": "recital_111_part_4",
        "metadata_type": "recital"
    },
    {
        "question": "Why are providers of general-purpose AI models expected to know if their model will meet systemic risk thresholds before training is completed?",
        "ground_truth_answer": "Providers are able to know if their model would meet the threshold before training is completed because the training of general-purpose AI models requires considerable planning, which includes the upfront allocation of compute resources and, specifically, the threshold of floating point operations.",
        "context_id": "recital_112_part_1",
        "metadata_type": "recital"
    },
    {
        "question": "Why is information concerning general-purpose AI models planned to be released as open-source considered especially important?",
        "ground_truth_answer": "Information concerning general-purpose AI models planned to be released as open-source is considered especially important because, after the open-source model release, necessary measures to ensure compliance with the obligations under this Regulation may be more difficult to implement.",
        "context_id": "recital_112_part_2",
        "metadata_type": "recital"
    },
    {
        "question": "What is the primary purpose of subjecting providers of general-purpose AI models presenting systemic risks to additional obligations?",
        "ground_truth_answer": "The primary purpose is to identify and mitigate those systemic risks and to ensure an adequate level of cybersecurity protection, regardless of whether the model is provided as a standalone model or embedded in an AI system or a product.",
        "context_id": "recital_114",
        "metadata_type": "recital"
    },
    {
        "question": "What are the primary responsibilities and considerations for providers of general-purpose AI models with systemic risks, particularly concerning risk mitigation, incident management, and cybersecurity?",
        "ground_truth_answer": "Providers of general-purpose AI models with systemic risks should assess and mitigate possible systemic risks. If a serious incident occurs despite their efforts, they should without undue delay keep track of the incident and report relevant information and possible corrective measures to the Commission and national competent authorities. Furthermore, providers should ensure an adequate level of cybersecurity protection for the model and its physical infrastructure along the entire model lifecycle, specifically considering systemic risks from malicious use or attacks, such as accidental model leakage, unauthorised releases, circumvention of safety measures, and defence against cyberattacks, unauthorised access or model theft.",
        "context_id": "recital_115",
        "metadata_type": "recital"
    },
    {
        "question": "What is the intended purpose of the AI Office collaborating with various national authorities, civil society organisations, stakeholders, and experts when developing codes of practice?",
        "ground_truth_answer": "The AI Office should collaborate with relevant national competent authorities and could consult with civil society organisations and other relevant stakeholders and experts, including the Scientific Panel, to ensure that the codes of practice reflect the state of the art and duly take into account a diverse set of perspectives.",
        "context_id": "recital_116",
        "metadata_type": "recital"
    },
    {
        "question": "What is the intended primary role of codes of practice for providers of general-purpose AI models under this Regulation?",
        "ground_truth_answer": "Codes of practice are intended to represent a central tool for proper compliance with the obligations provided for under this Regulation for providers of general-purpose AI models, allowing them to demonstrate compliance by relying on these codes.",
        "context_id": "recital_117",
        "metadata_type": "recital"
    },
    {
        "question": "How do the obligations of the EU AI Act relate to AI systems or models embedded in designated very large online platforms or very large online search engines already regulated by Regulation (EU) 2022/2065?",
        "ground_truth_answer": "For AI systems or models embedded into designated very large online platforms or very large online search engines, they are subject to the risk-management framework provided for in Regulation (EU) 2022/2065. Consequently, the corresponding obligations of the EU AI Act are presumed to be fulfilled, unless significant systemic risks not covered by Regulation (EU) 2022/2065 emerge and are identified in such models.",
        "context_id": "recital_118",
        "metadata_type": "recital"
    },
    {
        "question": "What is the primary reason that obligations on providers and deployers of certain AI systems to detect and disclose artificially generated or manipulated outputs are relevant for Regulation (EU) 2022/2065?",
        "ground_truth_answer": "These obligations are particularly relevant to facilitate the effective implementation of Regulation (EU) 2022/2065. This applies especially to the duties of providers of very large online platforms or very large online search engines to identify and mitigate systemic risks stemming from the dissemination of artificially generated or manipulated content, particularly risks affecting democratic processes, civic discourse, and electoral processes, including through disinformation.",
        "context_id": "recital_120",
        "metadata_type": "recital"
    },
    {
        "question": "What is the intended purpose and benefit of standardisation in the context of the EU AI Act?",
        "ground_truth_answer": "Standardisation is intended to play a key role by providing technical solutions to providers to ensure compliance with the Regulation, in line with the state of the art, and to promote innovation, competitiveness, and growth in the single market.",
        "context_id": "recital_121_part_1",
        "metadata_type": "recital"
    },
    {
        "question": "Under what circumstances can the European Commission establish common specifications for requirements under this Regulation as an exceptional fall back solution?",
        "ground_truth_answer": "The Commission can establish common specifications as an exceptional fall back solution, via implementing acts and after consultation of the advisory forum, when the standardisation request has not been accepted by any of the European standardisation organisations, or when the relevant harmonised standards insufficiently address fundamental rights concerns, or when the harmonised standards do not comply with the request, or when there are delays.",
        "context_id": "recital_121_part_2",
        "metadata_type": "recital"
    },
    {
        "question": "Under what conditions is a high-risk AI system presumed to comply with the data governance requirement of the EU AI Act?",
        "ground_truth_answer": "Providers of a high-risk AI system are presumed to comply with the relevant measure under the data governance requirement if the system has been trained and tested on data reflecting the specific geographical, behavioural, contextual or functional setting within which it is intended to be used.",
        "context_id": "recital_122",
        "metadata_type": "recital"
    },
    {
        "question": "What is the rationale for limiting third-party conformity assessment for certain high-risk AI systems, and what is the general rule for their conformity assessment?",
        "ground_truth_answer": "Third-party conformity assessment for high-risk AI systems, other than those related to products, is limited in an initial phase due to the current experience of professional pre-market certifiers in product safety and the different nature of risks involved. As a general rule, the conformity assessment for such systems should be carried out by the provider under its own responsibility, with the sole exception of AI systems intended for biometrics.",
        "context_id": "recital_125",
        "metadata_type": "recital"
    },
    {
        "question": "What is the purpose of notifying bodies under this Regulation, and what fundamental requirements must these bodies meet?",
        "ground_truth_answer": "Notified bodies should be notified under this Regulation by national competent authorities to carry out third-party conformity assessments when required. They must comply with a set of requirements, in particular on independence, competence, absence of conflicts of interests, and suitable cybersecurity requirements.",
        "context_id": "recital_126",
        "metadata_type": "recital"
    },
    {
        "question": "Under what circumstances should a high-risk AI system be considered a new AI system requiring a new conformity assessment, and when are changes to such a system not considered a substantial modification?",
        "ground_truth_answer": "A high-risk AI system should be considered a new AI system requiring a new conformity assessment when a change occurs that may affect its compliance with the Regulation (e.g., a change of operating system or software architecture), or when the intended purpose of the system changes. However, changes to the algorithm and performance of AI systems that continue to 'learn' after being placed on the market or put into service are not considered a substantial modification, provided those changes have been pre-determined by the provider and assessed at the moment of the conformity assessment.",
        "context_id": "recital_128",
        "metadata_type": "recital"
    },
    {
        "question": "Under what exceptional conditions might market surveillance authorities approve AI systems that have not undergone a conformity assessment?",
        "ground_truth_answer": "Market surveillance authorities could authorise the placing on the market or the putting into service of AI systems which have not undergone a conformity assessment under exceptional reasons of public security or protection of life and health of natural persons, environmental protection, and the protection of key industrial and infrastructural assets.",
        "context_id": "recital_130",
        "metadata_type": "recital"
    },
    {
        "question": "What is the intended purpose of establishing an EU database for AI systems, according to Recital 131?",
        "ground_truth_answer": "The intended purpose of establishing an EU database for AI systems is to facilitate the work of the Commission and the Member States in the AI field and to increase transparency towards the public.",
        "context_id": "recital_131_part_1",
        "metadata_type": "recital"
    },
    {
        "question": "What characteristics should the publicly accessible section of the EU database for high-risk AI systems possess?",
        "ground_truth_answer": "The publicly accessible section of the EU database should be free of charge, easily navigable, understandable, machine-readable, and user-friendly, offering search functionalities, including through keywords.",
        "context_id": "recital_131_part_2",
        "metadata_type": "recital"
    },
    {
        "question": "What measures are intended to ensure the full functionality of the EU database for high-risk AI systems upon its deployment?",
        "ground_truth_answer": "To ensure its full functionality, the procedure for setting the database should include the development of functional specifications by the Commission and an independent audit report.",
        "context_id": "recital_131_part_3",
        "metadata_type": "recital"
    },
    {
        "question": "What is the primary reason for subjecting certain AI systems, intended to interact with natural persons or generate content, to specific transparency obligations?",
        "ground_truth_answer": "The primary reason is that these AI systems may pose specific risks of impersonation or deception, irrespective of whether they qualify as high-risk or not.",
        "context_id": "recital_132",
        "metadata_type": "recital"
    },
    {
        "question": "What are the primary concerns that necessitate requiring providers of AI systems to embed technical solutions for marking and detecting AI-generated content?",
        "ground_truth_answer": "The primary concerns are the significant impact on the integrity and trust in the information ecosystem, raising new risks of misinformation and manipulation at scale, fraud, impersonation, and consumer deception, because AI-generated synthetic content is becoming increasingly hard for humans to distinguish from human-generated and authentic content.",
        "context_id": "recital_133_part_1",
        "metadata_type": "recital"
    },
    {
        "question": "What characteristics should techniques and methods used for content generated by AI possess, and what considerations should providers take into account when implementing them?",
        "ground_truth_answer": "Techniques and methods for content generated by AI should be sufficiently reliable, interoperable, effective, and robust, as far as technically feasible. When implementing this obligation, providers should also consider the specificities and limitations of different content types and relevant technological and market developments, as reflected in the generally acknowledged state of the art.",
        "context_id": "recital_133_part_2",
        "metadata_type": "recital"
    },
    {
        "question": "What clarification is provided regarding the transparency obligation for deep fakes concerning the rights to freedom of expression and the arts and sciences?",
        "ground_truth_answer": "Compliance with the transparency obligation, which requires deployers to clearly and distinguishably disclose that deep fake content has been artificially created or manipulated, should not be interpreted as indicating that the use of the AI system or its output impedes the right to freedom of expression and the right to freedom of the arts and sciences. This is particularly relevant where the content is part of an evidently creative, satirical, artistic, fictional or analogous work or programme, subject to appropriate safeguards for the rights and freedoms.",
        "context_id": "recital_134_part_1",
        "metadata_type": "recital"
    },
    {
        "question": "What is the intended scope of the transparency obligation for deep fakes when the content is part of an evidently creative, satirical, artistic, fictional, or analogous work or programme?",
        "ground_truth_answer": "In such cases, the transparency obligation for deep fakes is limited to the disclosure of the existence of such generated or manipulated content. This disclosure must be made in an appropriate manner that does not hamper the display or enjoyment of the work, including its normal exploitation and use, while maintaining the utility and quality of the work, and is subject to appropriate safeguards for the rights and freedoms of third parties.",
        "context_id": "recital_134_part_2",
        "metadata_type": "recital"
    },
    {
        "question": "What is the primary relevance of the EU AI Act's obligations regarding the detection and disclosure of artificially generated or manipulated AI system outputs to Regulation (EU) 2022/2065?",
        "ground_truth_answer": "The obligations in the EU AI Act to enable the detection and disclosure of artificially generated or manipulated AI system outputs are particularly relevant to facilitate the effective implementation of Regulation (EU) 2022/2065. This is especially true as regards the obligations of providers of very large online platforms or very large online search engines to identify and mitigate systemic risks that may arise from the dissemination of such content, including risks to democratic processes, civic discourse, and electoral processes.",
        "context_id": "recital_136",
        "metadata_type": "recital"
    },
    {
        "question": "What is the intended purpose of AI regulatory sandboxes established by Member States' national competent authorities?",
        "ground_truth_answer": "The intended purpose of AI regulatory sandboxes is to facilitate the development and testing of innovative AI systems under strict regulatory oversight before these systems are placed on the market or otherwise put into service, thereby ensuring a legal framework that promotes innovation, is future-proof and resilient to disruption.",
        "context_id": "recital_138",
        "metadata_type": "recital"
    },
    {
        "question": "What specific consideration should be given to the accessibility of AI regulatory sandboxes throughout the Union?",
        "ground_truth_answer": "Particular attention should be given to their accessibility for SMEs, including start-ups.",
        "context_id": "recital_139_part_2",
        "metadata_type": "recital"
    },
    {
        "question": "What is the intended outcome of establishing common rules for the implementation of AI regulatory sandboxes and a framework for cooperation between the relevant supervising authorities?",
        "ground_truth_answer": "The intended outcome is to ensure uniform implementation across the Union and achieve economies of scale.",
        "context_id": "recital_139_part_3",
        "metadata_type": "recital"
    },
    {
        "question": "What is the intended role of the EU AI Act regarding the use of personal data by providers in the AI regulatory sandbox?",
        "ground_truth_answer": "The EU AI Act is intended to provide the legal basis for providers and prospective providers in the AI regulatory sandbox to use personal data, originally collected for other purposes, for developing certain AI systems in the public interest within the sandbox, only under specified conditions and in accordance with Regulations (EU) 2016/679 and (EU) 2018/1725.",
        "context_id": "recital_140",
        "metadata_type": "recital"
    },
    {
        "question": "What is a key guarantee that should be introduced for providers or prospective providers testing high-risk AI systems in real-world conditions, taking into account the possible consequences for individuals, and what is a specific exception to this guarantee?",
        "ground_truth_answer": "Appropriate and sufficient guarantees and conditions should be introduced, which should include, among other things, requesting informed consent of natural persons to participate in testing in real-world conditions. An exception applies to law enforcement, where the seeking of informed consent would prevent the AI system from being tested.",
        "context_id": "recital_141_part_1",
        "metadata_type": "recital"
    },
    {
        "question": "What are the primary objectives for imposing specific requirements on prospective providers conducting real-world testing of AI systems, as outlined in this Regulation?",
        "ground_truth_answer": "The primary objectives are to minimise the risks associated with such testing and to enable effective oversight by competent authorities.",
        "context_id": "recital_141_part_2",
        "metadata_type": "recital"
    },
    {
        "question": "What additional safeguards are considered appropriate concerning the protection of personal data and its transfer to third countries during the real-world testing of AI systems?",
        "ground_truth_answer": "It is appropriate to envisage additional safeguards to ensure that personal data is protected and is deleted when subjects have withdrawn their consent to participate in the testing, without prejudice to their rights as data subjects under Union data protection law. Furthermore, data collected and processed for real-world testing should only be transferred to third countries where appropriate and applicable safeguards under Union law are implemented, in particular in accordance with bases for transfer of personal data under Union law on data protection.",
        "context_id": "recital_141_part_3",
        "metadata_type": "recital"
    },
    {
        "question": "What measures are proposed to support SMEs, including start-ups, in their engagement with the EU AI Act and to address their compliance costs?",
        "ground_truth_answer": "To support SMEs, dedicated communication channels should be established to provide guidance and respond to queries about the Regulation's implementation. These channels should work together to create synergies and ensure homogeneity in guidance. Member States should facilitate the participation of SMEs and other relevant stakeholders in standardisation development processes. Notified bodies should consider the specific interests and needs of providers that are SMEs when setting conformity assessment fees. Furthermore, the Commission should regularly assess and work with Member States to lower certification and compliance costs for SMEs, such as translation costs.",
        "context_id": "recital_143_part_2",
        "metadata_type": "recital"
    },
    {
        "question": "What is the rationale for allowing microenterprises to establish a quality management system in a simplified manner under the EU AI Act?",
        "ground_truth_answer": "The rationale is to ensure proportionality regarding costs of innovation and to reduce the administrative burden and costs for microenterprises, given their very small size. This simplification aims to achieve these goals without affecting the level of protection or the need for compliance with the requirements for high-risk AI systems.",
        "context_id": "recital_146",
        "metadata_type": "recital"
    },
    {
        "question": "What are the main components and supporting mechanisms of the governance framework established by this Regulation to coordinate and support the application of the EU AI Act at Union and national levels?",
        "ground_truth_answer": "The governance framework includes the AI Office, a Board composed of representatives of the Member States, a scientific panel to integrate the scientific community, and an advisory forum to contribute stakeholder input. Additionally, it involves making use of existing resources and expertise through synergies with structures such as the EuroHPC Joint Undertaking and the AI testing and experimentation facilities under the Digital Europe Programme.",
        "context_id": "recital_148",
        "metadata_type": "recital"
    },
    {
        "question": "What is the primary purpose for establishing a Board in the context of the EU AI Act?",
        "ground_truth_answer": "A Board should be established in order to facilitate a smooth, effective and harmonised implementation of this Regulation.",
        "context_id": "recital_149_part_1",
        "metadata_type": "recital"
    },
    {
        "question": "What is the primary function of the two standing sub-groups that the Board is required to establish?",
        "ground_truth_answer": "The two standing sub-groups are intended to provide a platform for cooperation and exchange among market surveillance authorities and notifying authorities on issues related, respectively, to market surveillance and notified bodies.",
        "context_id": "recital_149_part_2",
        "metadata_type": "recital"
    },
    {
        "question": "What kind of stakeholders should be represented in the advisory forum to ensure varied and balanced representation, balancing commercial and non-commercial interests?",
        "ground_truth_answer": "To ensure varied and balanced stakeholder representation, the advisory forum should comprise inter alia industry, start-ups, SMEs, academia, civil society (including the social partners), as well as the Fundamental Rights Agency, ENISA, the European Committee for Standardization (CEN), the European Committee for Electrotechnical Standardization (CENELEC) and the European Telecommunications Standards Institute (ETSI). This composition aims to balance commercial and non-commercial interests, and within commercial interests, distinguish between SMEs and other undertakings.",
        "context_id": "recital_150",
        "metadata_type": "recital"
    },
    {
        "question": "What is the intended purpose for establishing a scientific panel of independent experts under the EU AI Act?",
        "ground_truth_answer": "The scientific panel of independent experts is intended to support the implementation and enforcement of the Regulation, particularly the monitoring activities of the AI Office as regards general-purpose AI models. Furthermore, Member States should be able to request support from this pool of experts for their enforcement activities to reinforce national capacities.",
        "context_id": "recital_151",
        "metadata_type": "recital"
    },
    {
        "question": "What is the intended purpose of designating a market surveillance authority as a single point of contact within each Member State?",
        "ground_truth_answer": "The intended purpose is to increase organisational efficiency on the side of Member States and to establish a single point of contact vis-à-vis the public and other counterparts at Member State and Union levels.",
        "context_id": "recital_153",
        "metadata_type": "recital"
    },
    {
        "question": "What are the key objectives for requiring providers of high-risk AI systems to establish a post-market monitoring system?",
        "ground_truth_answer": "The key objectives are to ensure that providers can take into account the experience on the use of high-risk AI systems for improving their systems and the design and development process, take any possible corrective action in a timely manner, and more efficiently and timely address possible risks emerging from AI systems which continue to ‘learn’ after being placed on the market or put into service.",
        "context_id": "recital_155",
        "metadata_type": "recital"
    },
    {
        "question": "Why is the European Data Protection Supervisor designated as a competent market surveillance authority for Union institutions, agencies, and bodies?",
        "ground_truth_answer": "The European Data Protection Supervisor is designated as a competent market surveillance authority for Union institutions, agencies, and bodies due to the specific nature of these entities falling within the scope of this Regulation.",
        "context_id": "recital_156",
        "metadata_type": "recital"
    },
    {
        "question": "What is the intended purpose of the specific safeguard procedure for AI systems presenting a risk to health, safety, and fundamental rights?",
        "ground_truth_answer": "The specific safeguard procedure is intended for ensuring adequate and timely enforcement against AI systems presenting a risk to health, safety and fundamental rights.",
        "context_id": "recital_157",
        "metadata_type": "recital"
    },
    {
        "question": "What is the primary objective for designating competent authorities under Union financial services law to supervise the implementation of the EU AI Act?",
        "ground_truth_answer": "The primary objective is to ensure the coherent application and enforcement of the obligations under the EU AI Act and relevant rules and requirements of the Union financial services legal acts. These authorities are also intended to supervise the implementation of the EU AI Act, including market surveillance activities, regarding AI systems provided or used.",
        "context_id": "recital_158_part_1",
        "metadata_type": "recital"
    },
    {
        "question": "What entities are primarily designated as competent authorities for supervising the implementation of this Regulation, including market surveillance, regarding AI systems in regulated financial institutions, and what option do Member States have for designating these tasks?",
        "ground_truth_answer": "Competent authorities, within their respective competences, are designated for supervising the implementation of this Regulation, including for market surveillance activities, concerning AI systems provided or used by regulated and supervised financial institutions. However, Member States may decide to designate another authority to fulfill these market surveillance tasks.",
        "context_id": "recital_158_part_2",
        "metadata_type": "recital"
    },
    {
        "question": "What is the primary objective of integrating some of the providers' procedural obligations regarding risk management, post-marketing monitoring, and documentation into the existing obligations and procedures under Directive 2013/36/EU?",
        "ground_truth_answer": "The primary objective is to further enhance the consistency between the EU AI Act (this Regulation) and the rules applicable to credit institutions regulated under Directive 2013/36/EU.",
        "context_id": "recital_158_part_3",
        "metadata_type": "recital"
    },
    {
        "question": "What kind of powers should market surveillance authorities possess for high-risk AI systems used in areas such as biometrics for law enforcement, and how should these authorities operate?",
        "ground_truth_answer": "Market surveillance authorities for high-risk AI systems in areas like biometrics used for law enforcement, migration, asylum, border control management, or the administration of justice and democratic processes, should have effective investigative and corrective powers, including at least the power to obtain access to all personal data being processed and all information necessary for the performance of their tasks. These authorities should be able to exercise their powers by acting with complete independence.",
        "context_id": "recital_159",
        "metadata_type": "recital"
    },
    {
        "question": "What is the intended division of supervisory responsibilities for AI systems built on general-purpose AI models, particularly when the model and system share a provider?",
        "ground_truth_answer": "To avoid overlapping competences, where an AI system is based on a general-purpose AI model and both the model and system are provided by the same provider, supervision is intended to take place at Union level through the AI Office. In all other cases, national market surveillance authorities are intended to remain responsible for the supervision of AI systems.",
        "context_id": "recital_161",
        "metadata_type": "recital"
    },
    {
        "question": "What is the intended benefit of centralising the supervision and enforcement powers for obligations on providers of general-purpose AI models with the Commission?",
        "ground_truth_answer": "The intended benefit is to make the best use of centralised Union expertise and synergies at Union level.",
        "context_id": "recital_162",
        "metadata_type": "recital"
    },
    {
        "question": "Under what circumstances may the scientific panel provide qualified alerts to the AI Office?",
        "ground_truth_answer": "The scientific panel may provide qualified alerts to the AI Office if it has reason to suspect that a general-purpose AI model poses a concrete and identifiable risk at Union level, or if it has reason to suspect that a general-purpose AI model meets the criteria that would lead to its classification as a general-purpose AI model with systemic risk.",
        "context_id": "recital_163",
        "metadata_type": "recital"
    },
    {
        "question": "What is the overarching purpose of the AI Office's involvement with providers of general-purpose AI models?",
        "ground_truth_answer": "The AI Office's overarching purpose is to monitor the effective implementation of and compliance with the obligations for providers of general-purpose AI models laid down in this Regulation, and to ensure that compliance with these obligations is enforceable.",
        "context_id": "recital_164",
        "metadata_type": "recital"
    },
    {
        "question": "What principles are Member States expected to adhere to when establishing penalties for infringements of this Regulation?",
        "ground_truth_answer": "Member States should lay down effective, proportionate and dissuasive penalties for infringements and are also expected to respect the 'ne bis in idem' principle.",
        "context_id": "recital_168",
        "metadata_type": "recital"
    },
    {
        "question": "Under what circumstances do affected persons have the right to obtain an explanation when a deployer's decision is based on an AI system's output?",
        "ground_truth_answer": "Affected persons have the right to obtain an explanation where a deployer’s decision is based mainly upon the output from certain high-risk AI systems that fall within the scope of this Regulation, and where that decision produces legal effects or similarly significantly affects those persons in a way that they consider to have an adverse impact on their health, safety or fundamental rights.",
        "context_id": "recital_171",
        "metadata_type": "recital"
    },
    {
        "question": "For what primary purpose are powers delegated to the Commission in accordance with Article 290 TFEU regarding the EU AI Act's regulatory framework?",
        "ground_truth_answer": "The primary purpose for delegating powers to the Commission in accordance with Article 290 TFEU is to ensure that the regulatory framework can be adapted where necessary. This delegation allows the Commission to amend aspects such as the conditions under which an AI system is not considered high-risk, the list of high-risk AI systems, provisions for technical documentation, the content of the EU declaration of conformity, conformity assessment procedures, specific conformity assessment procedures for certain high-risk AI systems, thresholds, benchmarks and indicators for classifying general-purpose AI models with systemic risk, the criteria for designating such models, and technical documentation for their providers.",
        "context_id": "recital_173_part_1",
        "metadata_type": "recital"
    },
    {
        "question": "What is the primary rationale behind the Commission's obligation to regularly evaluate and review the EU AI Act, as outlined in Recital 174?",
        "ground_truth_answer": "The primary rationale for the Commission's regular evaluation and review is to ensure the effective application of the Regulation, given the rapid technological developments and the technical expertise required. These evaluations also assess the need to amend various lists and evaluate the effectiveness of supervision and governance systems.",
        "context_id": "recital_174",
        "metadata_type": "recital"
    },
    {
        "question": "What are the primary objectives for the EU AI Act applying to high-risk AI systems placed on the market or put into service before the general date of application only if they undergo significant changes?",
        "ground_truth_answer": "The primary objectives are to ensure legal certainty, ensure an appropriate adaptation period for operators, and avoid disruption to the market, including by ensuring continuity of the use of AI systems.",
        "context_id": "recital_177",
        "metadata_type": "recital"
    },
    {
        "question": "Why do the prohibitions and general provisions of the EU AI Act apply earlier than the overall regulation's application date?",
        "ground_truth_answer": "The prohibitions and general provisions of the Regulation should apply earlier, from 2 February 2025, to address the unacceptable risk associated with the use of AI in certain ways, account for these risks, and to have an effect on other procedures, such as in civil law.",
        "context_id": "recital_179",
        "metadata_type": "recital"
    },
    {
        "question": "To which providers does this Regulation apply, regarding AI systems and general-purpose AI models?",
        "ground_truth_answer": "This Regulation applies to providers placing on the market or putting into service AI systems or placing on the market general-purpose AI models in the Union, irrespective of whether those providers are established or located within the Union or in a third country.",
        "context_id": "article_2_paragraph_1_point_a",
        "metadata_type": "article"
    },
    {
        "question": "Under what condition does the EU AI Act apply to providers and deployers of AI systems located in a third country?",
        "ground_truth_answer": "The EU AI Act applies to providers and deployers of AI systems that have their place of establishment or are located in a third country when the output produced by the AI system is used in the Union.",
        "context_id": "article_2_paragraph_1_point_c",
        "metadata_type": "article"
    },
    {
        "question": "Under what specific conditions does Article 57 apply to high-risk AI systems related to products covered by the Union harmonisation legislation listed in Section B of Annex I?",
        "ground_truth_answer": "Article 57 applies to such high-risk AI systems only insofar as the requirements for high-risk AI systems under this Regulation have been integrated into that Union harmonisation legislation.",
        "context_id": "article_2_paragraph_2",
        "metadata_type": "article"
    },
    {
        "question": "Under what specific conditions does the Regulation not apply to AI systems used for military, defence, or national security purposes?",
        "ground_truth_answer": "The Regulation does not apply to AI systems if they are placed on the market, put into service, or used with or without modification exclusively for military, defence, or national security purposes. It also does not apply to AI systems not placed on the market or put into service in the Union, where their output is used in the Union exclusively for military, defence, or national security purposes.",
        "context_id": "article_2_paragraph_3",
        "metadata_type": "article"
    },
    {
        "question": "Under what conditions does the EU AI Act not apply to public authorities in a third country or international organisations using AI systems?",
        "ground_truth_answer": "The EU AI Act does not apply to public authorities in a third country or international organisations when they use AI systems in the framework of international cooperation or agreements for law enforcement and judicial cooperation with the Union or with one or more Member States, provided that such a third country or international organisation provides adequate safeguards with respect to the protection of fundamental rights and freedoms of individuals.",
        "context_id": "article_2_paragraph_4",
        "metadata_type": "article"
    },
    {
        "question": "Under what specific condition do AI systems or AI models not fall under the scope of this Regulation?",
        "ground_truth_answer": "This Regulation does not apply to AI systems or AI models, including their output, that are specifically developed and put into service for the sole purpose of scientific research and development.",
        "context_id": "article_2_paragraph_6",
        "metadata_type": "article"
    },
    {
        "question": "Which specific Union laws related to personal data, privacy, and communication confidentiality are explicitly stated as not being affected by this Regulation, and what is the general rule regarding the application of such laws?",
        "ground_truth_answer": "Union law on the protection of personal data, privacy and the confidentiality of communications applies to personal data processed in connection with the rights and obligations laid down in this Regulation. Specifically, this Regulation shall not affect Regulation (EU) 2016/679 or (EU) 2018/1725, or Directive 2002/58/EC or (EU) 2016/680, without prejudice to Article 10(5) and Article 59 of this Regulation.",
        "context_id": "article_2_paragraph_7",
        "metadata_type": "article"
    },
    {
        "question": "What types of activities related to AI systems or AI models are generally excluded from the EU AI Act's scope, and what specific type of testing is expressly *not* covered by this exclusion?",
        "ground_truth_answer": "The EU AI Act does not apply to any research, testing, or development activity regarding AI systems or AI models prior to their being placed on the market or put into service. However, testing in real world conditions is expressly not covered by that exclusion.",
        "context_id": "article_2_paragraph_8",
        "metadata_type": "article"
    },
    {
        "question": "What actions can the Union or Member States take to protect workers' rights concerning AI systems used by employers, according to this Regulation?",
        "ground_truth_answer": "The Union or Member States can maintain or introduce laws, regulations, or administrative provisions that are more favorable to workers in terms of protecting their rights regarding the use of AI systems by employers. They can also encourage or allow the application of collective agreements that are more favorable to workers.",
        "context_id": "article_2_paragraph_11",
        "metadata_type": "article"
    },
    {
        "question": "Under what circumstances does the EU AI Act apply to AI systems released under free and open-source licences?",
        "ground_truth_answer": "The EU AI Act applies to AI systems released under free and open-source licences if they are placed on the market or put into service as high-risk AI systems, or as an AI system that falls under Article 5 or 50.",
        "context_id": "article_2_paragraph_12",
        "metadata_type": "article"
    },
    {
        "question": "What essential characteristics define an 'AI system' according to Article 3 (1) of the EU AI Act?",
        "ground_truth_answer": "An 'AI system' is defined as a machine-based system designed to operate with varying levels of autonomy, which may exhibit adaptiveness after deployment. It infers, for explicit or implicit objectives and from its input, how to generate outputs such as predictions, content, recommendations, or decisions that can influence physical or virtual environments.",
        "context_id": "article_3_definition_1",
        "metadata_type": "article"
    },
    {
        "question": "What criteria define a 'provider' according to Article 3 (3) of the EU AI Act?",
        "ground_truth_answer": "A 'provider' is defined as a natural or legal person, public authority, agency, or other body that develops an AI system or a general-purpose AI model, or that has an AI system or a general-purpose AI model developed and places it on the market or puts the AI system into service under its own name or trademark, whether for payment or free of charge.",
        "context_id": "article_3_definition_3",
        "metadata_type": "article"
    },
    {
        "question": "Who is considered a 'deployer' under Article 3 (4) of the EU AI Act?",
        "ground_truth_answer": "A 'deployer' is defined as a natural or legal person, public authority, agency, or other body using an AI system under its authority, excluding cases where the AI system is used in the course of a personal non-professional activity.",
        "context_id": "article_3_definition_4",
        "metadata_type": "article"
    },
    {
        "question": "What defines an 'authorised representative' under the Regulation?",
        "ground_truth_answer": "An 'authorised representative' is a natural or legal person located or established in the Union who has received and accepted a written mandate from a provider of an AI system or a general-purpose AI model to perform and carry out on its behalf the obligations and procedures established by this Regulation.",
        "context_id": "article_3_definition_5",
        "metadata_type": "article"
    },
    {
        "question": "What constitutes an 'importer' under Article 3(6) of the EU AI Act?",
        "ground_truth_answer": "An 'importer' is defined as a natural or legal person located or established in the Union that places on the market an AI system which bears the name or trademark of a natural or legal person established in a third country.",
        "context_id": "article_3_definition_6",
        "metadata_type": "article"
    },
    {
        "question": "What does ‘making available on the market’ signify for an AI system or a general-purpose AI model under the EU AI Act?",
        "ground_truth_answer": "‘Making available on the market’ means the supply of an AI system or a general-purpose AI model for distribution or use on the Union market in the course of a commercial activity, regardless of whether it is provided for payment or free of charge.",
        "context_id": "article_3_definition_10",
        "metadata_type": "article"
    },
    {
        "question": "What is the definition of 'putting into service' under Article 3 (11) of the EU AI Act?",
        "ground_truth_answer": "Under Article 3 (11), 'putting into service' means the supply of an AI system for first use directly to the deployer or for own use in the Union for its intended purpose.",
        "context_id": "article_3_definition_11",
        "metadata_type": "article"
    },
    {
        "question": "What constitutes the 'intended purpose' of an AI system according to Article 3(12) of the EU AI Act?",
        "ground_truth_answer": "The 'intended purpose' of an AI system is defined as the use for which it is intended by the provider. This includes the specific context and conditions of use, which are specified in the information supplied by the provider in the instructions for use, promotional or sales materials and statements, and in the technical documentation.",
        "context_id": "article_3_definition_12",
        "metadata_type": "article"
    },
    {
        "question": "What constitutes ‘reasonably foreseeable misuse’ of an AI system as defined in Article 3(13) of the EU AI Act?",
        "ground_truth_answer": "‘Reasonably foreseeable misuse’ means the use of an AI system in a way that is not in accordance with its intended purpose, but which may result from reasonably foreseeable human behaviour or interaction with other systems, including other AI systems.",
        "context_id": "article_3_definition_13",
        "metadata_type": "article"
    },
    {
        "question": "What criteria define a 'safety component' according to the EU AI Act?",
        "ground_truth_answer": "A 'safety component' is defined as a component of a product or of an AI system that either fulfils a safety function for that product or AI system, or one whose failure or malfunctioning endangers the health and safety of persons or property.",
        "context_id": "article_3_definition_14",
        "metadata_type": "article"
    },
    {
        "question": "What is meant by the 'recall of an AI system' according to Article 3 (16) of the EU AI Act?",
        "ground_truth_answer": "The 'recall of an AI system' means any measure aiming to achieve the return of an AI system to the provider, or taking it out of service, or disabling its use, after it has been made available to deployers.",
        "context_id": "article_3_definition_16",
        "metadata_type": "article"
    },
    {
        "question": "What constitutes 'withdrawal of an AI system' according to Article 3 (17) of the EU AI Act?",
        "ground_truth_answer": "According to Article 3 (17), 'withdrawal of an AI system' means any measure aiming to prevent an AI system in the supply chain being made available on the market.",
        "context_id": "article_3_definition_17",
        "metadata_type": "article"
    },
    {
        "question": "What is a 'notifying authority' responsible for under Article 3 (19) of the EU AI Act?",
        "ground_truth_answer": "A 'notifying authority' is responsible for setting up and carrying out the necessary procedures for the assessment, designation and notification of conformity assessment bodies, and for their monitoring.",
        "context_id": "article_3_definition_19",
        "metadata_type": "article"
    },
    {
        "question": "What is the definition of 'conformity assessment' according to Article 3 (20) of the EU AI Act?",
        "ground_truth_answer": "'Conformity assessment' means the process of demonstrating whether the requirements set out in Chapter III, Section 2 relating to a high-risk AI system have been fulfilled.",
        "context_id": "article_3_definition_20",
        "metadata_type": "article"
    },
    {
        "question": "According to Article 3 (23) of the EU AI Act, what constitutes a 'substantial modification' to an AI system?",
        "ground_truth_answer": "A 'substantial modification' is defined as a change to an AI system after its placing on the market or putting into service which was not foreseen or planned in the initial conformity assessment carried out by the provider, and as a result of which either the compliance of the AI system with the requirements set out in Chapter III, Section 2 is affected or there is a modification to the intended purpose for which the AI system has been assessed.",
        "context_id": "article_3_definition_23",
        "metadata_type": "article"
    },
    {
        "question": "What does the CE marking indicate regarding an AI system?",
        "ground_truth_answer": "The CE marking indicates that an AI system is in conformity with the requirements set out in Chapter III, Section 2 of the EU AI Act and other applicable Union harmonisation legislation providing for its affixing.",
        "context_id": "article_3_definition_24",
        "metadata_type": "article"
    },
    {
        "question": "What are the specific purposes for which 'validation data' is used, according to Article 3 (30) of the EU AI Act?",
        "ground_truth_answer": "According to Article 3 (30) of the EU AI Act, 'validation data' is data used for providing an evaluation of the trained AI system, for tuning its non-learnable parameters and its learning process, and to prevent underfitting or overfitting.",
        "context_id": "article_3_definition_30",
        "metadata_type": "article"
    },
    {
        "question": "What constitutes ‘biometric identification’ according to Article 3 (35) of the EU AI Act?",
        "ground_truth_answer": "According to Article 3 (35), ‘biometric identification’ means the automated recognition of physical, physiological, behavioural, or psychological human features for the purpose of establishing the identity of a natural person by comparing biometric data of that individual to biometric data of individuals stored in a database.",
        "context_id": "article_3_definition_35",
        "metadata_type": "article"
    },
    {
        "question": "What is the definition of ‘biometric verification’ according to Article 3 (36) of the EU AI Act?",
        "ground_truth_answer": "‘Biometric verification’ means the automated, one-to-one verification, including authentication, of the identity of natural persons by comparing their biometric data to previously provided biometric data.",
        "context_id": "article_3_definition_36",
        "metadata_type": "article"
    },
    {
        "question": "What constitutes a 'biometric categorisation system' under Article 3 (40) of the EU AI Act?",
        "ground_truth_answer": "A 'biometric categorisation system' is an AI system designed to assign natural persons to specific categories based on their biometric data, unless it is ancillary to another commercial service and strictly necessary for objective technical reasons.",
        "context_id": "article_3_definition_40",
        "metadata_type": "article"
    },
    {
        "question": "What defines a 'remote biometric identification system' under Article 3 (41) of the EU AI Act?",
        "ground_truth_answer": "A 'remote biometric identification system' is an AI system designed to identify natural persons without their active involvement, typically from a distance, by comparing a person’s biometric data with biometric data stored in a reference database.",
        "context_id": "article_3_definition_41",
        "metadata_type": "article"
    },
    {
        "question": "What is the definition of a ‘real-time remote biometric identification system’?",
        "ground_truth_answer": "A ‘real-time remote biometric identification system’ is a remote biometric identification system where the capturing of biometric data, the comparison, and the identification all occur without a significant delay. This includes instant identification as well as limited short delays that are incorporated to avoid circumvention.",
        "context_id": "article_3_definition_42",
        "metadata_type": "article"
    },
    {
        "question": "What constitutes a 'publicly accessible space' as defined in Article 3 (44) of the EU AI Act?",
        "ground_truth_answer": "A 'publicly accessible space' is defined as any publicly or privately owned physical place accessible to an undetermined number of natural persons, irrespective of whether certain conditions for access or potential capacity restrictions may apply.",
        "context_id": "article_3_definition_44",
        "metadata_type": "article"
    },
    {
        "question": "What is the function of the 'AI Office' according to Article 3 (47) of the EU AI Act?",
        "ground_truth_answer": "The 'AI Office' is defined as the Commission’s function of contributing to the implementation, monitoring, and supervision of AI systems and general-purpose AI models, as well as AI governance, as provided for in the Commission Decision of 24 January 2024.",
        "context_id": "article_3_definition_47",
        "metadata_type": "article"
    },
    {
        "question": "How is a ‘national competent authority’ defined according to Article 3 (48) of the EU AI Act, and who acts as this authority for AI systems utilized by Union institutions, agencies, offices, and bodies?",
        "ground_truth_answer": "A ‘national competent authority’ is defined as a notifying authority or a market surveillance authority. For AI systems put into service or used by Union institutions, agencies, offices and bodies, the European Data Protection Supervisor is construed as the national competent authority or market surveillance authority.",
        "context_id": "article_3_definition_48",
        "metadata_type": "article"
    },
    {
        "question": "What is an AI regulatory sandbox?",
        "ground_truth_answer": "An AI regulatory sandbox is a controlled framework established by a competent authority, which enables providers or prospective providers of AI systems to develop, train, validate, and test an innovative AI system, potentially in real-world conditions, for a limited time and under regulatory supervision, in accordance with a sandbox plan.",
        "context_id": "article_3_definition_55",
        "metadata_type": "article"
    },
    {
        "question": "What constitutes 'AI literacy' as defined in Article 3 (56) of the EU AI Act?",
        "ground_truth_answer": "'AI literacy' means skills, knowledge and understanding that allow providers, deployers and affected persons, taking into account their respective rights and obligations in the context of this Regulation, to make an informed deployment of AI systems, as well as to gain awareness about the opportunities and risks of AI and possible harm it can cause.",
        "context_id": "article_3_definition_56",
        "metadata_type": "article"
    },
    {
        "question": "What is the purpose of 'testing in real-world conditions' for an AI system under the EU AI Act?",
        "ground_truth_answer": "The purpose of 'testing in real-world conditions' is to gather reliable and robust data and to assess and verify the conformity of the AI system with the requirements of this Regulation.",
        "context_id": "article_3_definition_57",
        "metadata_type": "article"
    },
    {
        "question": "What constitutes ‘informed consent’ as defined in Article 3 (59) of the EU AI Act?",
        "ground_truth_answer": "‘Informed consent’ means a subject’s freely given, specific, unambiguous and voluntary expression of his or her willingness to participate in a particular testing in real-world conditions, after having been informed of all aspects of the testing that are relevant to the subject’s decision to participate.",
        "context_id": "article_3_definition_59",
        "metadata_type": "article"
    },
    {
        "question": "What is the definition of a 'deep fake' under Article 3 (60) of the EU AI Act?",
        "ground_truth_answer": "A 'deep fake' means AI-generated or manipulated image, audio or video content that resembles existing persons, objects, places, entities or events and would falsely appear to a person to be authentic or truthful.",
        "context_id": "article_3_definition_60",
        "metadata_type": "article"
    },
    {
        "question": "According to Article 3 (61) (a) (i), what constitutes a 'widespread infringement'?",
        "ground_truth_answer": "A 'widespread infringement' is defined as any act or omission contrary to Union law protecting the interest of individuals, which has harmed or is likely to harm the collective interests of individuals residing in at least two Member States other than the Member State in which the act or omission originated or took place.",
        "context_id": "article_3_definition_61_point_a_point_i",
        "metadata_type": "article"
    },
    {
        "question": "What is the definition of a 'widespread infringement' as outlined in Article 3 (61) (a) (ii) of the EU AI Act?",
        "ground_truth_answer": "A 'widespread infringement' is any act or omission contrary to Union law protecting the interest of individuals, which has harmed or is likely to harm the collective interests of individuals residing in at least two Member States other than the Member State in which the concerned provider or its authorised representative is located or established.",
        "context_id": "article_3_definition_61_point_a_point_ii",
        "metadata_type": "article"
    },
    {
        "question": "What defines a 'widespread infringement' when committed by a deployer?",
        "ground_truth_answer": "When committed by a deployer, a 'widespread infringement' is defined as any act or omission contrary to Union law protecting the interest of individuals, which has harmed or is likely to harm the collective interests of individuals residing in at least two Member States other than the Member State in which the deployer is established.",
        "context_id": "article_3_definition_61_point_a_point_iii",
        "metadata_type": "article"
    },
    {
        "question": "What criteria define a 'widespread infringement' as per Article 3 (61) (b) of the EU AI Act?",
        "ground_truth_answer": "A 'widespread infringement' means any act or omission contrary to Union law protecting the interest of individuals, which has caused, causes or is likely to cause harm to the collective interests of individuals and has common features, including the same unlawful practice or the same interest being infringed, and is occurring concurrently, committed by the same operator, in at least three Member States.",
        "context_id": "article_3_definition_61_point_b",
        "metadata_type": "article"
    },
    {
        "question": "What defines a 'general-purpose AI model' under Article 3 (63) of the EU AI Act?",
        "ground_truth_answer": "A 'general-purpose AI model' is an AI model that displays significant generality and is capable of competently performing a wide range of distinct tasks, regardless of how it is placed on the market. It can be integrated into a variety of downstream systems or applications. This definition includes models trained with a large amount of data using self-supervision at scale, but excludes AI models used for research, development, or prototyping activities before they are placed on the market.",
        "context_id": "article_3_definition_63",
        "metadata_type": "article"
    },
    {
        "question": "What types of AI practices are prohibited under Article 5(1)(a) of the EU AI Act, based on the deployment of subliminal or manipulative techniques?",
        "ground_truth_answer": "Article 5(1)(a) prohibits the placing on the market, the putting into service, or the use of an AI system that deploys subliminal techniques beyond a person’s consciousness or purposefully manipulative or deceptive techniques. These practices are prohibited if they have the objective or effect of materially distorting the behaviour of a person or a group of persons by appreciably impairing their ability to make an informed decision, thereby causing them to take a decision they would not have otherwise taken, in a manner that causes or is reasonably likely to cause that person, another person, or group of persons significant harm.",
        "context_id": "article_5_paragraph_1_point_a",
        "metadata_type": "article"
    },
    {
        "question": "Under what conditions is an AI system that exploits the vulnerabilities of natural persons or specific groups prohibited by Article 5(1)(b) of the EU AI Act?",
        "ground_truth_answer": "An AI system is prohibited by Article 5(1)(b) if it exploits the vulnerabilities of a natural person or a specific group of persons due to their age, disability, or a specific social or economic situation, with the objective or effect of materially distorting their behaviour in a manner that causes or is reasonably likely to cause that person or another person significant harm. This prohibition applies to the placing on the market, the putting into service, or the use of such an AI system.",
        "context_id": "article_5_paragraph_1_point_b",
        "metadata_type": "article"
    },
    {
        "question": "Under Article 5(1)(c), what specific types of AI practices for evaluating or classifying natural persons or groups are prohibited?",
        "ground_truth_answer": "Article 5(1)(c) prohibits the placing on the market, the putting into service, or the use of AI systems for the evaluation or classification of natural persons or groups of persons over a certain period of time based on their social behaviour or known, inferred, or predicted personal or personality characteristics, with the social score leading to certain specified outcomes.",
        "context_id": "article_5_paragraph_1_point_c",
        "metadata_type": "article"
    },
    {
        "question": "Which AI practices are prohibited concerning the assessment or prediction of a natural person's risk of committing a criminal offence, according to Article 5 1 (d)?",
        "ground_truth_answer": "According to Article 5 1 (d), the placing on the market, the putting into service for this specific purpose, or the use of an AI system for making risk assessments of natural persons to assess or predict the risk of a natural person committing a criminal offence is prohibited. This prohibition applies when the assessment is based solely on the profiling of a natural person or on assessing their personality traits and characteristics.",
        "context_id": "article_5_paragraph_1_point_d",
        "metadata_type": "article"
    },
    {
        "question": "Which AI practices are prohibited under Article 5, paragraph 1, point (e) of the EU AI Act?",
        "ground_truth_answer": "The placing on the market, the putting into service for this specific purpose, or the use of AI systems that create or expand facial recognition databases through the untargeted scraping of facial images from the internet or CCTV footage are prohibited.",
        "context_id": "article_5_paragraph_1_point_e",
        "metadata_type": "article"
    },
    {
        "question": "Which AI practices are prohibited concerning the inference of emotions of natural persons in workplaces and education institutions, and what is the exception to this prohibition?",
        "ground_truth_answer": "The placing on the market, the putting into service for this specific purpose, or the use of AI systems to infer emotions of a natural person in the areas of workplace and education institutions is prohibited. However, this prohibition does not apply where the use of the AI system is intended to be put in place or into the market for medical or safety reasons.",
        "context_id": "article_5_paragraph_1_point_f",
        "metadata_type": "article"
    },
    {
        "question": "Which specific AI practices involving biometric categorisation systems are prohibited under Article 5(1)(g) of the EU AI Act, and what does this prohibition exclude?",
        "ground_truth_answer": "Article 5(1)(g) prohibits the placing on the market, the putting into service for this specific purpose, or the use of biometric categorisation systems that categorise individually natural persons based on their biometric data to deduce or infer their race, political opinions, trade union membership, religious or philosophical beliefs, sex life or sexual orientation. This prohibition does not cover any labelling or filtering of lawfully acquired biometric datasets, such as images, based on biometric data, or categorizing of biometric data in the area of law enforcement.",
        "context_id": "article_5_paragraph_1_point_g",
        "metadata_type": "article"
    },
    {
        "question": "Which AI practices are prohibited under Article 5 1 (h) concerning law enforcement?",
        "ground_truth_answer": "The use of ‘real-time’ remote biometric identification systems in publicly accessible spaces for the purposes of law enforcement is prohibited, unless and in so far as such use is strictly necessary for one of the specified objectives.",
        "context_id": "article_5_paragraph_1_point_h",
        "metadata_type": "article"
    },
    {
        "question": "What is the minimum maximum period for a custodial sentence or a detention order that a criminal offence must be punishable by in the Member State concerned to permit the use of 'real-time' remote biometric identification systems by law enforcement for the purpose of localising or identifying a suspected person?",
        "ground_truth_answer": "The criminal offence must be punishable by a custodial sentence or a detention order for a maximum period of at least four years.",
        "context_id": "article_5_paragraph_1_point_h_point_iii",
        "metadata_type": "article"
    },
    {
        "question": "For what specific purpose shall 'real-time' remote biometric identification systems be deployed by law enforcement in publicly accessible spaces, according to Article 5(2)?",
        "ground_truth_answer": "According to Article 5(2), such systems, when used for the objectives referred to in paragraph 1, first subparagraph, point (h), shall be deployed only to confirm the identity of the specifically targeted individual, and their use must also take into account certain elements.",
        "context_id": "article_5_paragraph_2",
        "metadata_type": "article"
    },
    {
        "question": "What are the prerequisites for a law enforcement authority to be authorised to use a ‘real-time’ remote biometric identification system in publicly accessible spaces?",
        "ground_truth_answer": "The law enforcement authority must have completed a fundamental rights impact assessment as provided for in Article 27 and registered the system in the EU database according to Article 49.",
        "context_id": "article_5_paragraph_2_paragraph_21",
        "metadata_type": "article"
    },
    {
        "question": "Under what specific conditions can the use of a 'real-time' remote biometric identification system in publicly accessible spaces for law enforcement commence without prior authorisation, and what are the immediate consequences if authorisation is subsequently rejected?",
        "ground_truth_answer": "In a duly justified situation of urgency, the use of such a system may be commenced without an authorisation, provided that such authorisation is requested without undue delay, at the latest within 24 hours. If such authorisation is rejected, the use must be stopped with immediate effect.",
        "context_id": "article_5_paragraph_3_part_1",
        "metadata_type": "article"
    },
    {
        "question": "Under what conditions can a competent judicial or independent administrative authority authorize the use of a 'real-time' remote biometric identification system?",
        "ground_truth_answer": "A competent judicial or an independent administrative authority, whose decision is binding, can grant authorisation only if it is satisfied, based on objective evidence or clear indications, that the use of the system is necessary for and proportionate to achieving one of the objectives specified in paragraph 1, first subparagraph, point (h), as identified in the request. Additionally, the use must remain strictly necessary concerning the period of time, geographic, and personal scope. When deciding, the authority must also consider the elements referred to in paragraph 2.",
        "context_id": "article_5_paragraph_3_part_2",
        "metadata_type": "article"
    },
    {
        "question": "What restriction applies to decisions that result in an adverse legal effect on a person when based on the output of a 'real-time' remote biometric identification system?",
        "ground_truth_answer": "No decision that produces an adverse legal effect on a person may be taken based solely on the output of the ‘real-time’ remote biometric identification system.",
        "context_id": "article_5_paragraph_3_part_3",
        "metadata_type": "article"
    },
    {
        "question": "To which authorities must each use of a 'real-time' remote biometric identification system in publicly accessible spaces for law enforcement purposes be notified?",
        "ground_truth_answer": "Each use must be notified to the relevant market surveillance authority and the national data protection authority.",
        "context_id": "article_5_paragraph_4",
        "metadata_type": "article"
    },
    {
        "question": "What are the obligations of Member States regarding the establishment of detailed national rules for the authorisation, supervision, and reporting of 'real-time' remote biometric identification systems used for law enforcement, and their subsequent notification?",
        "ground_truth_answer": "Member States concerned shall lay down in their national law the necessary detailed rules for the request, issuance, exercise, supervision, and reporting relating to the authorisations referred to in paragraph 3. These rules must specify for which objectives listed in paragraph 1, first subparagraph, point (h), including specific criminal offences referred to in point (h)(iii), competent authorities may be authorised to use these systems for law enforcement. Additionally, Member States shall notify these rules to the Commission at the latest 30 days following their adoption.",
        "context_id": "article_5_paragraph_5",
        "metadata_type": "article"
    },
    {
        "question": "Which authorities are required to submit annual reports to the Commission on the use of 'real-time' remote biometric identification systems in publicly accessible spaces for law enforcement purposes, and what specific information must be included in these reports?",
        "ground_truth_answer": "National market surveillance authorities and the national data protection authorities of Member States that have been notified of the use of 'real-time' remote biometric identification systems in publicly accessible spaces for law enforcement purposes pursuant to paragraph 4 are required to submit annual reports to the Commission on such use. These reports must include information on the number and the result of decisions taken by competent judicial authorities or an independent administrative authority whose decision is binding upon requests for authorisations in accordance with paragraph 3.",
        "context_id": "article_5_paragraph_6",
        "metadata_type": "article"
    },
    {
        "question": "According to Article 6, paragraph 1 of the EU AI Act, what makes an AI system be considered high-risk?",
        "ground_truth_answer": "An AI system shall be considered high-risk where both of the specified conditions are fulfilled, regardless of whether it is placed on the market or put into service independently of the products referred to in points (a) and (b).",
        "context_id": "article_6_paragraph_1",
        "metadata_type": "article"
    },
    {
        "question": "Under what specific conditions, and by derogation from paragraph 2, might an AI system referred to in Annex III not be considered high-risk?",
        "ground_truth_answer": "An AI system referred to in Annex III shall not be considered high-risk where it does not pose a significant risk of harm to the health, safety, or fundamental rights of natural persons, including by not materially influencing the outcome of decision-making. This derogation applies if any of certain specified conditions are fulfilled.",
        "context_id": "article_6_paragraph_3",
        "metadata_type": "article"
    },
    {
        "question": "Under what specific conditions can an AI system, referred to in Annex III and intended to detect decision-making patterns or deviations from prior patterns, not be considered high-risk?",
        "ground_truth_answer": "An AI system referred to in Annex III shall not be considered high-risk if it does not pose a significant risk of harm to the health, safety or fundamental rights of natural persons, including by not materially influencing the outcome of decision making, and if it is not meant to replace or influence the previously completed human assessment without proper human review.",
        "context_id": "article_6_paragraph_3_point_c",
        "metadata_type": "article"
    },
    {
        "question": "Under what specific condition is an AI system referred to in Annex III always considered to be high-risk, notwithstanding other potential derogations?",
        "ground_truth_answer": "An AI system referred to in Annex III is always considered to be high-risk where the AI system performs profiling of natural persons.",
        "context_id": "article_6_paragraph_3_paragraph_31",
        "metadata_type": "article"
    },
    {
        "question": "What obligations does a provider have if they determine that an AI system referred to in Annex III is not high-risk?",
        "ground_truth_answer": "If a provider considers an AI system referred to in Annex III to not be high-risk, they must document their assessment before the system is placed on the market or put into service. Such a provider is also subject to the registration obligation set out in Article 49(2) and must provide the documentation of the assessment upon request of national competent authorities.",
        "context_id": "article_6_paragraph_4",
        "metadata_type": "article"
    },
    {
        "question": "Under what specific conditions is the Commission empowered to amend paragraph 3, second subparagraph, of Article 6?",
        "ground_truth_answer": "The Commission is empowered to amend paragraph 3, second subparagraph, of Article 6 by adding new conditions or modifying existing ones, through delegated acts in accordance with Article 97, when there is concrete and reliable evidence of the existence of AI systems that fall under the scope of Annex III but do not pose a significant risk of harm to the health, safety or fundamental rights of natural persons.",
        "context_id": "article_6_paragraph_6",
        "metadata_type": "article"
    },
    {
        "question": "Under what circumstances can the Commission amend paragraph 3, second subparagraph, of Article 6 by deleting any of the conditions laid down therein through delegated acts?",
        "ground_truth_answer": "The Commission can amend paragraph 3, second subparagraph, of Article 6 by deleting any of the conditions laid down therein when there is concrete and reliable evidence that this is necessary to maintain the level of protection of health, safety and fundamental rights provided for by this Regulation.",
        "context_id": "article_6_paragraph_7",
        "metadata_type": "article"
    },
    {
        "question": "What conditions must any amendment to the conditions laid down in paragraph 3, second subparagraph, of Article 6, adopted in accordance with paragraphs 6 and 7 of the same Article, fulfill?",
        "ground_truth_answer": "Any such amendment must not decrease the overall level of protection of health, safety and fundamental rights provided for by this Regulation. It must also ensure consistency with the delegated acts adopted pursuant to Article 7(1), and take account of market and technological developments.",
        "context_id": "article_6_paragraph_8",
        "metadata_type": "article"
    },
    {
        "question": "What power does the Commission hold under Article 7 paragraph 1 regarding Annex III?",
        "ground_truth_answer": "Under Article 7 paragraph 1, the Commission is empowered to adopt delegated acts in accordance with Article 97 to amend Annex III by adding or modifying use-cases of high-risk AI systems, provided that both of the specified conditions are fulfilled.",
        "context_id": "article_7_paragraph_1",
        "metadata_type": "article"
    },
    {
        "question": "What two conditions must be fulfilled for the Commission to amend Annex III by adding or modifying use-cases of high-risk AI systems?",
        "ground_truth_answer": "For the Commission to amend Annex III by adding or modifying use-cases of high-risk AI systems, two conditions must be fulfilled: the AI systems must pose a risk of harm to health and safety, or an adverse impact on fundamental rights, and that risk must be equivalent to, or greater than, the risk of harm or of adverse impact posed by the high-risk AI systems already referred to in Annex III.",
        "context_id": "article_7_paragraph_1_point_b",
        "metadata_type": "article"
    },
    {
        "question": "What criteria shall the Commission consider when assessing the condition under paragraph 1, point (b), regarding the use of an AI system?",
        "ground_truth_answer": "When assessing the condition under paragraph 1, point (b), the Commission shall take into account the extent to which the use of an AI system has already caused harm to health and safety, has had an adverse impact on fundamental rights or has given rise to significant concerns in relation to the likelihood of such harm or adverse impact. This can be demonstrated, for example, by reports or documented allegations submitted to national competent authorities or by other reports, as appropriate.",
        "context_id": "article_7_paragraph_2_point_e",
        "metadata_type": "article"
    },
    {
        "question": "What specific criteria concerning vulnerability does the Commission take into account when assessing the condition under Article 7(1)(b)?",
        "ground_truth_answer": "When assessing the condition under Article 7(1)(b), the Commission shall take into account the extent to which there is an imbalance of power, or the persons who are potentially harmed or suffer an adverse impact are in a vulnerable position in relation to the deployer of an AI system, particularly due to status, authority, knowledge, economic or social circumstances, or age.",
        "context_id": "article_7_paragraph_2_point_h",
        "metadata_type": "article"
    },
    {
        "question": "What criteria does the Commission consider when assessing whether an outcome produced involving an AI system is easily corrigible or reversible, and which outcomes are explicitly excluded from being considered easily corrigible or reversible?",
        "ground_truth_answer": "When assessing the corrigibility or reversibility of an AI system's outcome, the Commission considers the extent to which the outcome is easily corrigible or reversible, taking into account available technical solutions. However, outcomes that have an adverse impact on health, safety, or fundamental rights are explicitly not considered to be easily corrigible or reversible.",
        "context_id": "article_7_paragraph_2_point_i",
        "metadata_type": "article"
    },
    {
        "question": "When a product contains an AI system subject to both the EU AI Act and other Union harmonisation legislation, what flexibility do providers have regarding documentation and procedures for high-risk AI systems?",
        "ground_truth_answer": "Providers have the choice to integrate, as appropriate, the necessary testing and reporting processes, information, and documentation they provide with regard to their product into documentation and procedures that already exist and are required under the Union harmonisation legislation listed in Section A of Annex I. This flexibility is intended to ensure consistency, avoid duplication, and minimise additional burdens.",
        "context_id": "article_8_paragraph_2",
        "metadata_type": "article"
    },
    {
        "question": "What steps must a risk management system comprise regarding the evaluation of risks for a high-risk AI system?",
        "ground_truth_answer": "The risk management system must comprise the estimation and evaluation of the risks that may emerge when the high-risk AI system is used in accordance with its intended purpose, and under conditions of reasonably foreseeable misuse.",
        "context_id": "article_9_paragraph_2_point_b",
        "metadata_type": "article"
    },
    {
        "question": "What types of risks are addressed by Article 9 of the EU AI Act?",
        "ground_truth_answer": "Article 9 addresses only those risks that can be reasonably mitigated or eliminated through the development or design of the high-risk AI system, or by providing adequate technical information.",
        "context_id": "article_9_paragraph_3",
        "metadata_type": "article"
    },
    {
        "question": "What must be ensured regarding the elimination or reduction of risks when identifying the most appropriate risk management measures for high-risk AI systems?",
        "ground_truth_answer": "It must be ensured that risks identified and evaluated pursuant to paragraph 2 are eliminated or reduced as far as technically feasible through adequate design and development of the high-risk AI system.",
        "context_id": "article_9_paragraph_5_point_a",
        "metadata_type": "article"
    },
    {
        "question": "What factors must be given due consideration when identifying risk management measures to eliminate or reduce risks related to the use of a high-risk AI system?",
        "ground_truth_answer": "When identifying risk management measures to eliminate or reduce risks related to the use of a high-risk AI system, due consideration must be given to the technical knowledge, experience, education, the training to be expected by the deployer, and the presumable context in which the system is intended to be used.",
        "context_id": "article_9_paragraph_5_paragraph_51",
        "metadata_type": "article"
    },
    {
        "question": "When must the testing of high-risk AI systems be performed according to the EU AI Act?",
        "ground_truth_answer": "The testing of high-risk AI systems shall be performed, as appropriate, at any time throughout the development process, and, in any event, prior to their being placed on the market or put into service.",
        "context_id": "article_9_paragraph_8",
        "metadata_type": "article"
    },
    {
        "question": "What type of data sets must be used for high-risk AI systems that train AI models with data, and what criteria must these data sets meet?",
        "ground_truth_answer": "High-risk AI systems which make use of techniques involving the training of AI models with data shall be developed on the basis of training, validation and testing data sets that meet the quality criteria referred to in paragraphs 2 to 5 whenever such data sets are used.",
        "context_id": "article_10_paragraph_1",
        "metadata_type": "article"
    },
    {
        "question": "What specific data-preparation processing operations are particularly relevant for data governance and management practices concerning training, validation, and testing data sets of high-risk AI systems?",
        "ground_truth_answer": "Data governance and management practices for training, validation, and testing data sets of high-risk AI systems shall particularly concern relevant data-preparation processing operations such as annotation, labelling, cleaning, updating, enrichment, and aggregation.",
        "context_id": "article_10_paragraph_2_point_c",
        "metadata_type": "article"
    },
    {
        "question": "What specific issues must data governance and management practices address regarding training, validation, and testing data sets for high-risk AI systems?",
        "ground_truth_answer": "Data governance and management practices for training, validation, and testing data sets for high-risk AI systems must particularly concern the examination for possible biases that are likely to affect the health and safety of persons, have a negative impact on fundamental rights, or lead to discrimination prohibited under Union law, especially where data outputs influence inputs for future operations.",
        "context_id": "article_10_paragraph_2_point_f",
        "metadata_type": "article"
    },
    {
        "question": "What characteristics must training, validation, and testing data sets possess according to Article 10, paragraph 3 of the EU AI Act?",
        "ground_truth_answer": "According to Article 10, paragraph 3, training, validation, and testing data sets shall be relevant, sufficiently representative, and to the best extent possible, free of errors and complete in view of the intended purpose. They must also have appropriate statistical properties, including, where applicable, as regards the persons or groups of persons in relation to whom the high-risk AI system is intended to be used. These characteristics can be met at the level of individual data sets or at the level of a combination thereof.",
        "context_id": "article_10_paragraph_3",
        "metadata_type": "article"
    },
    {
        "question": "Under what conditions may providers of high-risk AI systems exceptionally process special categories of personal data?",
        "ground_truth_answer": "Providers of high-risk AI systems may exceptionally process special categories of personal data to the extent that it is strictly necessary for the purpose of ensuring bias detection and correction in relation to those systems, in accordance with paragraph (2), points (f) and (g) of Article 10. This is subject to appropriate safeguards for the fundamental rights and freedoms of natural persons, and in addition to the provisions set out in Regulations (EU) 2016/679 and (EU) 2018/1725 and Directive (EU) 2016/680, all other specified conditions must be met.",
        "context_id": "article_10_paragraph_5",
        "metadata_type": "article"
    },
    {
        "question": "What is one specific condition that must be met for providers of high-risk AI systems to exceptionally process special categories of personal data for bias detection and correction?",
        "ground_truth_answer": "One specific condition that must be met is that the bias detection and correction cannot be effectively fulfilled by processing other data, including synthetic or anonymised data.",
        "context_id": "article_10_paragraph_5_point_a",
        "metadata_type": "article"
    },
    {
        "question": "What specific additional conditions, beyond those in Regulations (EU) 2016/679 and (EU) 2018/1725 and Directive (EU) 2016/680, must be met for providers of high-risk AI systems to process special categories of personal data for bias detection and correction?",
        "ground_truth_answer": "In addition to existing data protection provisions, the special categories of personal data must be subject to technical limitations on re-use and state-of-the-art security and privacy-preserving measures, including pseudonymisation.",
        "context_id": "article_10_paragraph_5_point_b",
        "metadata_type": "article"
    },
    {
        "question": "What specific conditions must be met regarding special categories of personal data when processed by providers of high-risk AI systems for bias detection and correction, according to Article 10, paragraph 5, point (c) of the EU AI Act?",
        "ground_truth_answer": "The special categories of personal data must be subject to measures to ensure that they are secured, protected, and subject to suitable safeguards, including strict controls and documentation of access. These measures are designed to avoid misuse and to ensure that only authorised persons with appropriate confidentiality obligations have access to those personal data.",
        "context_id": "article_10_paragraph_5_point_c",
        "metadata_type": "article"
    },
    {
        "question": "What is a specific condition for the processing of special categories of personal data by providers of high-risk AI systems for bias detection and correction, concerning the transmission or access by other parties?",
        "ground_truth_answer": "When providers of high-risk AI systems process special categories of personal data for bias detection and correction, those data are not to be transmitted, transferred, or otherwise accessed by other parties.",
        "context_id": "article_10_paragraph_5_point_d",
        "metadata_type": "article"
    },
    {
        "question": "What is the specific deletion requirement for special categories of personal data processed for bias detection and correction in high-risk AI systems?",
        "ground_truth_answer": "The special categories of personal data must be deleted once the bias has been corrected or the personal data has reached the end of its retention period, whichever comes first.",
        "context_id": "article_10_paragraph_5_point_e",
        "metadata_type": "article"
    },
    {
        "question": "What specific information must be included in the records of processing activities when providers of high-risk AI systems exceptionally process special categories of personal data for bias detection and correction?",
        "ground_truth_answer": "When providers of high-risk AI systems exceptionally process special categories of personal data for bias detection and correction, their records of processing activities must include the reasons why the processing of special categories of personal data was strictly necessary to detect and correct biases, and why that objective could not be achieved by processing other data.",
        "context_id": "article_10_paragraph_5_point_f",
        "metadata_type": "article"
    },
    {
        "question": "When must the technical documentation for a high-risk AI system be drawn up, and what is its maintenance requirement?",
        "ground_truth_answer": "The technical documentation of a high-risk AI system must be drawn up before that system is placed on the market or put into service, and it shall be kept up-to date.",
        "context_id": "article_11_paragraph_1",
        "metadata_type": "article"
    },
    {
        "question": "What is a technical requirement for high-risk AI systems concerning event recording?",
        "ground_truth_answer": "High-risk AI systems shall technically allow for the automatic recording of events (logs) over the lifetime of the system.",
        "context_id": "article_12_paragraph_1",
        "metadata_type": "article"
    },
    {
        "question": "What minimum logging capability is required for high-risk AI systems referred to in point 1 (a) of Annex III concerning the period of use?",
        "ground_truth_answer": "For high-risk AI systems referred to in point 1 (a) of Annex III, the logging capabilities shall provide, at a minimum, the recording of the period of each use of the system, including its start date and time and end date and time.",
        "context_id": "article_12_paragraph_3_point_a",
        "metadata_type": "article"
    },
    {
        "question": "What specific information must the logging capabilities of high-risk AI systems referred to in point 1 (a) of Annex III provide at a minimum concerning input data checks?",
        "ground_truth_answer": "The logging capabilities must provide the reference database against which input data has been checked by the system.",
        "context_id": "article_12_paragraph_3_point_b",
        "metadata_type": "article"
    },
    {
        "question": "What minimum information must the logging capabilities of high-risk AI systems referred to in point 1 (a) of Annex III provide concerning search matches?",
        "ground_truth_answer": "The logging capabilities shall provide, at a minimum, the input data for which the search has led to a match.",
        "context_id": "article_12_paragraph_3_point_c",
        "metadata_type": "article"
    },
    {
        "question": "For high-risk AI systems referred to in point 1 (a) of Annex III, what specific identification must the logging capabilities provide concerning the verification of results?",
        "ground_truth_answer": "The logging capabilities must provide, at a minimum, the identification of the natural persons involved in the verification of the results, as referred to in Article 14(5).",
        "context_id": "article_12_paragraph_3_point_d",
        "metadata_type": "article"
    },
    {
        "question": "What is the transparency requirement for the design and development of high-risk AI systems?",
        "ground_truth_answer": "High-risk AI systems must be designed and developed to ensure their operation is sufficiently transparent, enabling deployers to interpret a system’s output and use it appropriately. An appropriate type and degree of transparency shall be ensured to achieve compliance with the relevant obligations of the provider and deployer set out in Section 3.",
        "context_id": "article_13_paragraph_1",
        "metadata_type": "article"
    },
    {
        "question": "What characteristics must the instructions for use accompanying high-risk AI systems possess to be considered adequate?",
        "ground_truth_answer": "The instructions for use must be in an appropriate digital format or otherwise, and include information that is concise, complete, correct, clear, relevant, accessible, and comprehensible to deployers.",
        "context_id": "article_13_paragraph_2",
        "metadata_type": "article"
    },
    {
        "question": "What specific details regarding the accuracy, robustness, and cybersecurity of a high-risk AI system must be included in its instructions for use?",
        "ground_truth_answer": "The instructions for use of a high-risk AI system must include the level of accuracy (including its metrics), robustness, and cybersecurity against which the system has been tested and validated and which can be expected. Furthermore, any known and foreseeable circumstances that may impact that expected level of accuracy, robustness, and cybersecurity must also be contained within the instructions.",
        "context_id": "article_13_paragraph_3_point_b_point_ii",
        "metadata_type": "article"
    },
    {
        "question": "What specific information concerning circumstances that could lead to risks must be included in the instructions for use of a high-risk AI system?",
        "ground_truth_answer": "The instructions for use of a high-risk AI system must include information on any known or foreseeable circumstance, related to its use in accordance with its intended purpose or under conditions of reasonably foreseeable misuse, which may lead to risks to health and safety or fundamental rights as referred to in Article 9(2).",
        "context_id": "article_13_paragraph_3_point_b_point_iii",
        "metadata_type": "article"
    },
    {
        "question": "What specific information, when appropriate, must the instructions for use for high-risk AI systems contain regarding their performance concerning specific persons or groups of persons?",
        "ground_truth_answer": "When appropriate, the instructions for use for high-risk AI systems must contain information on the system's performance regarding specific persons or groups of persons on which the system is intended to be used.",
        "context_id": "article_13_paragraph_3_point_b_point_v",
        "metadata_type": "article"
    },
    {
        "question": "What information regarding input data or data sets must be included in the instructions for use of a high-risk AI system, taking into account its intended purpose?",
        "ground_truth_answer": "The instructions for use of a high-risk AI system must include, when appropriate, specifications for the input data or any other relevant information in terms of the training, validation, and testing data sets used, taking into account the intended purpose of the high-risk AI system.",
        "context_id": "article_13_paragraph_3_point_b_point_vi",
        "metadata_type": "article"
    },
    {
        "question": "What specific information must the instructions for use contain regarding human oversight measures?",
        "ground_truth_answer": "The instructions for use must contain information about the human oversight measures referred to in Article 14, including the technical measures put in place to facilitate the interpretation of the outputs of high-risk AI systems by the deployers.",
        "context_id": "article_13_paragraph_3_point_d",
        "metadata_type": "article"
    },
    {
        "question": "What information regarding resources, lifetime, and maintenance must be included in the instructions for use for a high-risk AI system?",
        "ground_truth_answer": "The instructions for use for a high-risk AI system must contain information about the computational and hardware resources needed, the expected lifetime of the system, and any necessary maintenance and care measures, including their frequency, to ensure its proper functioning, including as regards software updates.",
        "context_id": "article_13_paragraph_3_point_e",
        "metadata_type": "article"
    },
    {
        "question": "What specific information must be included in the instructions for use concerning log management mechanisms within a high-risk AI system?",
        "ground_truth_answer": "The instructions for use shall contain, where relevant, a description of the mechanisms included within the high-risk AI system that allows deployers to properly collect, store, and interpret the logs in accordance with Article 12.",
        "context_id": "article_13_paragraph_3_point_f",
        "metadata_type": "article"
    },
    {
        "question": "How must high-risk AI systems be designed and developed to ensure human oversight?",
        "ground_truth_answer": "High-risk AI systems shall be designed and developed, including with appropriate human-machine interface tools, in such a way that they can be effectively overseen by natural persons during the period in which they are in use.",
        "context_id": "article_14_paragraph_1",
        "metadata_type": "article"
    },
    {
        "question": "What is the aim of human oversight concerning high-risk AI systems, according to Article 14, paragraph 2 of the EU AI Act?",
        "ground_truth_answer": "Human oversight shall aim to prevent or minimise the risks to health, safety or fundamental rights that may emerge when a high-risk AI system is used in accordance with its intended purpose or under conditions of reasonably foreseeable misuse, in particular where such risks persist despite the application of other requirements set out in that Section.",
        "context_id": "article_14_paragraph_2",
        "metadata_type": "article"
    },
    {
        "question": "What is one type of oversight measure that providers of high-risk AI systems must ensure before placing them on the market or putting them into service?",
        "ground_truth_answer": "One type of oversight measure involves measures identified and built, when technically feasible, into the high-risk AI system by the provider before it is placed on the market or put into service.",
        "context_id": "article_14_paragraph_3_point_a",
        "metadata_type": "article"
    },
    {
        "question": "What type of oversight measures can be implemented by the deployer for a high-risk AI system, as identified by the provider?",
        "ground_truth_answer": "Oversight measures that can be implemented by the deployer are those identified by the provider before placing the high-risk AI system on the market or putting it into service, provided they are appropriate for the deployer to implement.",
        "context_id": "article_14_paragraph_3_point_b",
        "metadata_type": "article"
    },
    {
        "question": "How must a high-risk AI system be provided to the deployer to facilitate human oversight, as appropriate and proportionate?",
        "ground_truth_answer": "The high-risk AI system must be provided to the deployer in a way that enables natural persons assigned human oversight to properly understand the relevant capacities and limitations of the system, and to duly monitor its operation, including for detecting and addressing anomalies, dysfunctions, and unexpected performance.",
        "context_id": "article_14_paragraph_4_point_a",
        "metadata_type": "article"
    },
    {
        "question": "Regarding high-risk AI systems, what specific awareness are natural persons assigned human oversight enabled to maintain?",
        "ground_truth_answer": "Natural persons assigned human oversight are enabled to remain aware of the possible tendency of automatically relying or over-relying on the output produced by a high-risk AI system (automation bias), especially for systems providing information or recommendations for decisions taken by natural persons.",
        "context_id": "article_14_paragraph_4_point_b",
        "metadata_type": "article"
    },
    {
        "question": "What must the provision of a high-risk AI system to a deployer enable for natural persons assigned human oversight, in order to implement Article 14, paragraphs 1, 2, and 3?",
        "ground_truth_answer": "The provision of a high-risk AI system to a deployer must enable natural persons assigned human oversight, as appropriate and proportionate, to correctly interpret the high-risk AI system’s output, taking into account, for example, the interpretation tools and methods available.",
        "context_id": "article_14_paragraph_4_point_c",
        "metadata_type": "article"
    },
    {
        "question": "What actions must natural persons assigned human oversight be enabled to take concerning a high-risk AI system's output or use?",
        "ground_truth_answer": "Natural persons assigned human oversight must be enabled, as appropriate and proportionate, to decide, in any particular situation, not to use the high-risk AI system or to otherwise disregard, override, or reverse the output of the high-risk AI system.",
        "context_id": "article_14_paragraph_4_point_d",
        "metadata_type": "article"
    },
    {
        "question": "What functionalities must a high-risk AI system provide to natural persons assigned human oversight regarding its operation and interruption?",
        "ground_truth_answer": "A high-risk AI system must be provided in such a way that natural persons to whom human oversight is assigned are enabled, as appropriate and proportionate, to intervene in its operation or interrupt the system through a 'stop' button or a similar procedure that allows it to come to a halt in a safe state.",
        "context_id": "article_14_paragraph_4_point_e",
        "metadata_type": "article"
    },
    {
        "question": "What additional measures are required for high-risk AI systems referred to in point 1(a) of Annex III concerning actions or decisions based on system identification, and under what conditions does the separate verification requirement not apply?",
        "ground_truth_answer": "For high-risk AI systems referred to in point 1(a) of Annex III, no action or decision is to be taken by the deployer based on the identification resulting from the system unless that identification has been separately verified and confirmed by at least two natural persons with the necessary competence, training, and authority. This requirement for separate verification by at least two natural persons does not apply to high-risk AI systems used for law enforcement, migration, border control, or asylum, if Union or national law considers its application disproportionate.",
        "context_id": "article_14_paragraph_5",
        "metadata_type": "article"
    },
    {
        "question": "What design and development requirements apply to high-risk AI systems?",
        "ground_truth_answer": "High-risk AI systems must be designed and developed to achieve an appropriate level of accuracy, robustness, and cybersecurity, and to perform consistently in these respects throughout their lifecycle.",
        "context_id": "article_15_paragraph_1",
        "metadata_type": "article"
    },
    {
        "question": "What actions shall the Commission take to address the technical aspects of measuring appropriate levels of accuracy and robustness for AI systems, and other relevant performance metrics?",
        "ground_truth_answer": "The Commission shall, in cooperation with relevant stakeholders and organisations such as metrology and benchmarking authorities, encourage, as appropriate, the development of benchmarks and measurement methodologies.",
        "context_id": "article_15_paragraph_2",
        "metadata_type": "article"
    },
    {
        "question": "What resilience standard must high-risk AI systems meet concerning errors, faults, or inconsistencies?",
        "ground_truth_answer": "High-risk AI systems shall be as resilient as possible regarding errors, faults or inconsistencies that may occur within the system or the environment in which the system operates, particularly due to their interaction with natural persons or other systems.",
        "context_id": "article_15_paragraph_4",
        "metadata_type": "article"
    },
    {
        "question": "What types of attacks and flaws should technical solutions for high-risk AI systems address to handle AI-specific vulnerabilities?",
        "ground_truth_answer": "Technical solutions designed to address AI-specific vulnerabilities in high-risk AI systems should include measures to prevent, detect, respond to, resolve, and control for attacks such as manipulating the training data set (data poisoning), manipulating pre-trained components used in training (model poisoning), inputs designed to cause the AI model to make a mistake (adversarial examples or model evasion), confidentiality attacks, or model flaws.",
        "context_id": "article_15_paragraph_5",
        "metadata_type": "article"
    },
    {
        "question": "What contact information must providers of high-risk AI systems include, and where should this information be indicated?",
        "ground_truth_answer": "Providers of high-risk AI systems must indicate their name, registered trade name or registered trade mark, and the address at which they can be contacted. This information should be placed on the high-risk AI system itself or, if that is not possible, on its packaging or its accompanying documentation.",
        "context_id": "article_16_paragraph_1_point_b",
        "metadata_type": "article"
    },
    {
        "question": "What obligation do providers of high-risk AI systems have concerning conformity assessment procedures?",
        "ground_truth_answer": "Providers of high-risk AI systems shall ensure that the high-risk AI system undergoes the relevant conformity assessment procedure as referred to in Article 43, prior to its being placed on the market or put into service.",
        "context_id": "article_16_paragraph_1_point_f",
        "metadata_type": "article"
    },
    {
        "question": "Where must providers of high-risk AI systems affix the CE marking to demonstrate conformity with the EU AI Act?",
        "ground_truth_answer": "Providers of high-risk AI systems must affix the CE marking to the high-risk AI system itself or, if that is not possible, on its packaging or its accompanying documentation, to indicate conformity with the Regulation, in accordance with Article 48.",
        "context_id": "article_16_paragraph_1_point_h",
        "metadata_type": "article"
    },
    {
        "question": "Under what specific condition must providers of high-risk AI systems demonstrate the conformity of their systems with the requirements set out in Section 2?",
        "ground_truth_answer": "Providers of high-risk AI systems must demonstrate the conformity of their systems with the requirements set out in Section 2 upon a reasoned request of a national competent authority.",
        "context_id": "article_16_paragraph_1_point_k",
        "metadata_type": "article"
    },
    {
        "question": "What accessibility requirements must providers of high-risk AI systems ensure their systems comply with?",
        "ground_truth_answer": "Providers of high-risk AI systems must ensure their systems comply with accessibility requirements in accordance with Directives (EU) 2016/2102 and (EU) 2019/882.",
        "context_id": "article_16_paragraph_1_point_l",
        "metadata_type": "article"
    },
    {
        "question": "What specific aspects must a quality management system, implemented by providers of high-risk AI systems, include to ensure regulatory compliance?",
        "ground_truth_answer": "The quality management system must include at least a strategy for regulatory compliance, encompassing compliance with conformity assessment procedures and procedures for the management of modifications to the high-risk AI system.",
        "context_id": "article_17_paragraph_1_point_a",
        "metadata_type": "article"
    },
    {
        "question": "What specific aspects related to design must be included in the quality management system for providers of high-risk AI systems?",
        "ground_truth_answer": "The quality management system must include techniques, procedures, and systematic actions to be used for the design, design control, and design verification of the high-risk AI system.",
        "context_id": "article_17_paragraph_1_point_b",
        "metadata_type": "article"
    },
    {
        "question": "What specific aspects must the quality management system for high-risk AI systems include regarding development, quality control, and quality assurance?",
        "ground_truth_answer": "The quality management system for high-risk AI systems must include techniques, procedures, and systematic actions to be used for the development, quality control, and quality assurance of the high-risk AI system.",
        "context_id": "article_17_paragraph_1_point_c",
        "metadata_type": "article"
    },
    {
        "question": "What specific procedures must be included in the quality management system of providers of high-risk AI systems concerning the development stages?",
        "ground_truth_answer": "The quality management system must include examination, test, and validation procedures to be carried out before, during, and after the development of the high-risk AI system, and specify the frequency with which these procedures have to be carried out.",
        "context_id": "article_17_paragraph_1_point_d",
        "metadata_type": "article"
    },
    {
        "question": "What must the quality management system implemented by providers of high-risk AI systems include regarding technical specifications and standards?",
        "ground_truth_answer": "The quality management system must include technical specifications, including standards to be applied. Additionally, where relevant harmonised standards are not applied in full or do not cover all of the relevant requirements set out in Section 2, the system must detail the means to be used to ensure that the high-risk AI system complies with those requirements.",
        "context_id": "article_17_paragraph_1_point_e",
        "metadata_type": "article"
    },
    {
        "question": "What specific data management aspects must be included in the quality management system that providers of high-risk AI systems are required to put in place?",
        "ground_truth_answer": "The quality management system for providers of high-risk AI systems must include systems and procedures for data management, encompassing data acquisition, data collection, data analysis, data labelling, data storage, data filtration, data mining, data aggregation, data retention, and any other operation regarding the data that is performed before and for the purpose of the placing on the market or the putting into service of high-risk AI systems.",
        "context_id": "article_17_paragraph_1_point_f",
        "metadata_type": "article"
    },
    {
        "question": "What specific communication aspects must be included in the quality management system of providers of high-risk AI systems?",
        "ground_truth_answer": "The quality management system of providers of high-risk AI systems must include the handling of communication with national competent authorities, other relevant authorities (including those providing or supporting the access to data), notified bodies, other operators, customers, or other interested parties.",
        "context_id": "article_17_paragraph_1_point_j",
        "metadata_type": "article"
    },
    {
        "question": "How should providers implement the aspects mentioned in paragraph 1 and what level of protection must they maintain for their high-risk AI systems?",
        "ground_truth_answer": "The implementation of the aspects referred to in paragraph 1 shall be proportionate to the size of the provider’s organisation. Additionally, providers shall, in any event, respect the degree of rigour and the level of protection required to ensure the compliance of their high-risk AI systems with this Regulation.",
        "context_id": "article_17_paragraph_2",
        "metadata_type": "article"
    },
    {
        "question": "What may providers of high-risk AI systems do if they are already subject to obligations concerning quality management systems or an equivalent function under relevant sectoral Union law?",
        "ground_truth_answer": "Providers of high-risk AI systems who are subject to obligations regarding quality management systems or an equivalent function under relevant sectoral Union law may include the aspects listed in paragraph 1 as part of their quality management systems pursuant to that law.",
        "context_id": "article_17_paragraph_3",
        "metadata_type": "article"
    },
    {
        "question": "How can financial institutions, subject to Union financial services law requirements concerning internal governance, arrangements, or processes, fulfill their obligation to implement a quality management system under the EU AI Act?",
        "ground_truth_answer": "Such financial institutions can fulfill their obligation to implement a quality management system, with the exception of Article 17, paragraph 1, points (g) and (h), by complying with the rules on internal governance arrangements or processes pursuant to the relevant Union financial services law. Additionally, any harmonised standards referred to in Article 40 must be taken into account.",
        "context_id": "article_17_paragraph_4_point_i",
        "metadata_type": "article"
    },
    {
        "question": "What specific documentation must a provider of a high-risk AI system keep at the disposal of national competent authorities for a period ending 10 years after the system is placed on the market or put into service?",
        "ground_truth_answer": "A provider must keep the documentation concerning the changes approved by notified bodies, where applicable, at the disposal of the national competent authorities.",
        "context_id": "article_18_paragraph_1_point_c",
        "metadata_type": "article"
    },
    {
        "question": "What specific documents must a provider keep at the disposal of national competent authorities for a period of 10 years after a high-risk AI system has been placed on the market or put into service?",
        "ground_truth_answer": "The provider must keep the decisions and other documents issued by the notified bodies, where applicable, at the disposal of national competent authorities.",
        "context_id": "article_18_paragraph_1_point_d",
        "metadata_type": "article"
    },
    {
        "question": "What is the responsibility of each Member State concerning documentation when an AI provider or its authorised representative goes bankrupt or ceases activity?",
        "ground_truth_answer": "Each Member State shall determine the conditions under which the documentation referred to in paragraph 1 remains at the disposal of the national competent authorities for the period indicated in that paragraph, specifically when a provider or its authorised representative established on its territory goes bankrupt or ceases its activity prior to the end of that period.",
        "context_id": "article_18_paragraph_2",
        "metadata_type": "article"
    },
    {
        "question": "What specific requirement applies to providers that are financial institutions regarding the maintenance of technical documentation?",
        "ground_truth_answer": "Providers that are financial institutions subject to requirements regarding their internal governance, arrangements or processes under Union financial services law shall maintain the technical documentation as part of the documentation kept under the relevant Union financial services law.",
        "context_id": "article_18_paragraph_3",
        "metadata_type": "article"
    },
    {
        "question": "For what duration are providers of high-risk AI systems required to keep the automatically generated logs mentioned in Article 12(1)?",
        "ground_truth_answer": "Providers of high-risk AI systems must keep the automatically generated logs for a period appropriate to the intended purpose of the high-risk AI system, of at least six months. This duration applies unless otherwise specified in applicable Union or national law, particularly Union law on the protection of personal data.",
        "context_id": "article_19_paragraph_1",
        "metadata_type": "article"
    },
    {
        "question": "What specific obligation applies to financial institutions that are providers of high-risk AI systems, according to Article 19, paragraph 2?",
        "ground_truth_answer": "Providers that are financial institutions subject to requirements regarding their internal governance, arrangements or processes under Union financial services law shall maintain the logs automatically generated by their high-risk AI systems as part of the documentation kept under the relevant financial services law.",
        "context_id": "article_19_paragraph_2",
        "metadata_type": "article"
    },
    {
        "question": "What actions must providers of high-risk AI systems take if they determine that a high-risk AI system they have placed on the market or put into service is not in conformity with the EU AI Act?",
        "ground_truth_answer": "Providers of high-risk AI systems must immediately take the necessary corrective actions to bring that system into conformity, to withdraw it, to disable it, or to recall it, as appropriate. They must also inform the distributors of the high-risk AI system concerned and, where applicable, the deployers, the authorised representative, and importers.",
        "context_id": "article_20_paragraph_1",
        "metadata_type": "article"
    },
    {
        "question": "What actions must a provider of a high-risk AI system take if the system presents a risk within the meaning of Article 79(1) and the provider becomes aware of that risk?",
        "ground_truth_answer": "Upon becoming aware that a high-risk AI system presents a risk within the meaning of Article 79(1), the provider must immediately investigate the causes, in collaboration with the reporting deployer where applicable. The provider must also inform the market surveillance authorities competent for the high-risk AI system concerned and, where applicable, the notified body that issued a certificate for that system in accordance with Article 44, specifically detailing the nature of the non-compliance and any relevant corrective action taken.",
        "context_id": "article_20_paragraph_2",
        "metadata_type": "article"
    },
    {
        "question": "What is the obligation of providers of high-risk AI systems when a competent authority makes a reasoned request regarding their systems?",
        "ground_truth_answer": "Upon a reasoned request from a competent authority, providers of high-risk AI systems must provide all necessary information and documentation to demonstrate conformity with the requirements outlined in Section 2. This information must be presented in a language easily understood by the authority, chosen from one of the official languages of the Union institutions as indicated by the Member State concerned.",
        "context_id": "article_21_paragraph_1",
        "metadata_type": "article"
    },
    {
        "question": "What must providers give a competent authority access to upon a reasoned request, regarding high-risk AI systems?",
        "ground_truth_answer": "Upon a reasoned request by a competent authority, providers must give the requesting competent authority access to the automatically generated logs of the high-risk AI system referred to in Article 12(1), to the extent such logs are under their control.",
        "context_id": "article_21_paragraph_2",
        "metadata_type": "article"
    },
    {
        "question": "How must information obtained by a competent authority pursuant to Article 21 be treated?",
        "ground_truth_answer": "Any information obtained by a competent authority pursuant to Article 21 shall be treated in accordance with the confidentiality obligations set out in Article 78.",
        "context_id": "article_21_paragraph_3",
        "metadata_type": "article"
    },
    {
        "question": "What must providers established in third countries do before making their high-risk AI systems available on the Union market?",
        "ground_truth_answer": "Prior to making their high-risk AI systems available on the Union market, providers established in third countries shall, by written mandate, appoint an authorised representative which is established in the Union.",
        "context_id": "article_22_paragraph_1",
        "metadata_type": "article"
    },
    {
        "question": "What specific tasks does the mandate empower the authorised representative to carry out regarding the EU declaration of conformity, technical documentation, and conformity assessment procedures?",
        "ground_truth_answer": "For the purposes of this Regulation, the mandate shall empower the authorised representative to verify that the EU declaration of conformity referred to in Article 47 and the technical documentation referred to in Article 11 have been drawn up and that an appropriate conformity assessment procedure has been carried out by the provider.",
        "context_id": "article_22_paragraph_3_point_a",
        "metadata_type": "article"
    },
    {
        "question": "What specific documents and contact details must an authorized representative keep at the disposal of the competent authorities and national authorities or bodies for a period of 10 years after a high-risk AI system has been placed on the market or put into service?",
        "ground_truth_answer": "The authorized representative must keep at the disposal of the competent authorities and national authorities or bodies the contact details of the provider that appointed the authorized representative, a copy of the EU declaration of conformity referred to in Article 47, the technical documentation, and, if applicable, the certificate issued by the notified body.",
        "context_id": "article_22_paragraph_3_point_b",
        "metadata_type": "article"
    },
    {
        "question": "What information and access must an authorised representative provide to a competent authority to demonstrate the conformity of a high-risk AI system?",
        "ground_truth_answer": "Upon a reasoned request, the authorised representative must provide a competent authority with all the information and documentation necessary to demonstrate the conformity of a high-risk AI system with the requirements set out in Section 2. This includes access to the logs, as referred to in Article 12(1), automatically generated by the high-risk AI system, to the extent such logs are under the control of the provider.",
        "context_id": "article_22_paragraph_3_point_c",
        "metadata_type": "article"
    },
    {
        "question": "What task must the mandate empower the authorised representative to carry out regarding cooperation with competent authorities for high-risk AI systems?",
        "ground_truth_answer": "The mandate must empower the authorised representative to cooperate with competent authorities, upon a reasoned request, in any action the latter take in relation to the high-risk AI system, particularly to reduce and mitigate the risks posed by the high-risk AI system.",
        "context_id": "article_22_paragraph_3_point_d",
        "metadata_type": "article"
    },
    {
        "question": "What specific tasks related to registration is an authorised representative empowered to carry out under their mandate, as per Article 22(3)(e) of the EU AI Act?",
        "ground_truth_answer": "The authorised representative is empowered to, where applicable, comply with the registration obligations referred to in Article 49(1), or, if the registration is carried out by the provider itself, ensure that the information referred to in point 3 of Section A of Annex VIII is correct.",
        "context_id": "article_22_paragraph_3_point_e",
        "metadata_type": "article"
    },
    {
        "question": "What power does the mandate grant the authorised representative regarding communication with competent authorities for compliance with the EU AI Act?",
        "ground_truth_answer": "The mandate shall empower the authorised representative to be addressed, in addition to or instead of the provider, by the competent authorities, on all issues related to ensuring compliance with this Regulation.",
        "context_id": "article_22_paragraph_3_paragraph_31",
        "metadata_type": "article"
    },
    {
        "question": "What actions must an authorised representative take immediately after terminating a mandate because they consider the provider to be acting contrary to its obligations under the EU AI Act?",
        "ground_truth_answer": "Upon terminating the mandate, the authorised representative shall immediately inform the relevant market surveillance authority, and where applicable, the relevant notified body, about the termination of the mandate and the reasons for it.",
        "context_id": "article_22_paragraph_4",
        "metadata_type": "article"
    },
    {
        "question": "What specific action must importers verify concerning a high-risk AI system's conformity before placing it on the market?",
        "ground_truth_answer": "Importers shall ensure the system is in conformity with this Regulation by verifying that the relevant conformity assessment procedure referred to in Article 43 has been carried out by the provider of the high-risk AI system.",
        "context_id": "article_23_paragraph_1_point_a",
        "metadata_type": "article"
    },
    {
        "question": "What must importers verify regarding the technical documentation for a high-risk AI system before placing it on the market?",
        "ground_truth_answer": "Importers must verify that the provider has drawn up the technical documentation in accordance with Article 11 and Annex IV.",
        "context_id": "article_23_paragraph_1_point_b",
        "metadata_type": "article"
    },
    {
        "question": "What specific verifications must importers perform regarding documentation and marking for a high-risk AI system before placing it on the market, as per Article 23 1 (c)?",
        "ground_truth_answer": "Before placing a high-risk AI system on the market, importers must verify that the system bears the required CE marking, and is accompanied by both the EU declaration of conformity referred to in Article 47 and instructions for use.",
        "context_id": "article_23_paragraph_1_point_c",
        "metadata_type": "article"
    },
    {
        "question": "What must importers verify concerning an authorised representative before placing a high-risk AI system on the market?",
        "ground_truth_answer": "Before placing a high-risk AI system on the market, importers must verify that the provider has appointed an authorised representative in accordance with Article 22(1).",
        "context_id": "article_23_paragraph_1_point_d",
        "metadata_type": "article"
    },
    {
        "question": "What is an importer's obligation if they have sufficient reason to believe a high-risk AI system is not in conformity with the Regulation, is falsified, or is accompanied by falsified documentation?",
        "ground_truth_answer": "The importer shall not place the system on the market until it has been brought into conformity.",
        "context_id": "article_23_paragraph_2",
        "metadata_type": "article"
    },
    {
        "question": "What information must importers provide on a high-risk AI system, its packaging, or accompanying documentation?",
        "ground_truth_answer": "Importers must indicate their name, registered trade name or registered trade mark, and the address at which they can be contacted.",
        "context_id": "article_23_paragraph_3",
        "metadata_type": "article"
    },
    {
        "question": "What documentation are importers of high-risk AI systems required to retain and for what duration?",
        "ground_truth_answer": "Importers are required to keep, for a period of 10 years after a high-risk AI system has been placed on the market or put into service, a copy of the certificate issued by the notified body (where applicable), of the instructions for use, and of the EU declaration of conformity referred to in Article 47.",
        "context_id": "article_23_paragraph_5",
        "metadata_type": "article"
    },
    {
        "question": "What obligations do importers have regarding providing information and documentation for high-risk AI systems to competent authorities?",
        "ground_truth_answer": "Importers must provide the relevant competent authorities, upon a reasoned request, with all the necessary information and documentation, including that referred to in paragraph 5 of Article 23, to demonstrate the conformity of a high-risk AI system with the requirements set out in Section 2. This information must be in a language which can be easily understood by the authorities, and importers must also ensure that the technical documentation can be made available to those authorities.",
        "context_id": "article_23_paragraph_6",
        "metadata_type": "article"
    },
    {
        "question": "What obligation do importers have regarding high-risk AI systems they place on the market?",
        "ground_truth_answer": "Importers shall cooperate with the relevant competent authorities in any action those authorities take in relation to a high-risk AI system placed on the market by the importers, especially to reduce and mitigate the risks posed by it.",
        "context_id": "article_23_paragraph_7",
        "metadata_type": "article"
    },
    {
        "question": "What verifications must distributors complete before making a high-risk AI system available on the market?",
        "ground_truth_answer": "Before making a high-risk AI system available on the market, distributors must verify that it bears the required CE marking, is accompanied by a copy of the EU declaration of conformity referred to in Article 47 and instructions for use, and that the provider and the importer of that system, as applicable, have complied with their respective obligations as laid down in Article 16, points (b) and (c) and Article 23(3).",
        "context_id": "article_24_paragraph_1",
        "metadata_type": "article"
    },
    {
        "question": "What is a distributor's obligation if they consider or have reason to consider that a high-risk AI system is not in conformity with the requirements set out in Section 2?",
        "ground_truth_answer": "If a distributor considers or has reason to consider that a high-risk AI system is not in conformity with the requirements set out in Section 2, they shall not make the system available on the market until it has been brought into conformity with those requirements.",
        "context_id": "article_24_paragraph_2",
        "metadata_type": "article"
    },
    {
        "question": "What actions must a distributor take if they consider a high-risk AI system they have made available on the market not to be in conformity with the requirements set out in Section 2, and what additional steps are required if the system presents a risk?",
        "ground_truth_answer": "If a distributor considers a high-risk AI system they have made available on the market not to be in conformity with the requirements set out in Section 2, they shall take the corrective actions necessary to bring that system into conformity, to withdraw it, or recall it, or ensure that the provider, the importer, or any relevant operator takes those corrective actions. If the high-risk AI system presents a risk within the meaning of Article 79(1), the distributor must immediately inform the provider or importer of the system and the authorities competent for the high-risk AI system concerned, providing details of the non-compliance and any corrective actions taken.",
        "context_id": "article_24_paragraph_4",
        "metadata_type": "article"
    },
    {
        "question": "What information and documentation are distributors of a high-risk AI system obliged to provide to a relevant competent authority upon a reasoned request?",
        "ground_truth_answer": "Upon a reasoned request from a relevant competent authority, distributors of a high-risk AI system shall provide that authority with all the information and documentation regarding their actions pursuant to paragraphs 1 to 4 necessary to demonstrate the conformity of that system with the requirements set out in Section 2.",
        "context_id": "article_24_paragraph_5",
        "metadata_type": "article"
    },
    {
        "question": "What is the obligation of distributors regarding high-risk AI systems they make available on the market?",
        "ground_truth_answer": "Distributors shall cooperate with the relevant competent authorities in any action those authorities take in relation to a high-risk AI system made available on the market by the distributors, particularly to reduce or mitigate the risk posed by it.",
        "context_id": "article_24_paragraph_6",
        "metadata_type": "article"
    },
    {
        "question": "Which entities, under specific circumstances, are considered providers of a high-risk AI system for the purposes of this Regulation and are subject to the obligations of a provider under Article 16?",
        "ground_truth_answer": "Any distributor, importer, deployer, or other third-party shall be considered a provider of a high-risk AI system and shall be subject to the obligations of the provider under Article 16, in circumstances that follow this statement.",
        "context_id": "article_25_paragraph_1",
        "metadata_type": "article"
    },
    {
        "question": "Under what circumstance does a distributor, importer, deployer, or other third-party become considered a provider of a high-risk AI system and subject to the obligations of a provider under Article 16?",
        "ground_truth_answer": "A distributor, importer, deployer, or other third-party is considered a provider of a high-risk AI system and subject to the obligations of a provider under Article 16 if they put their name or trademark on a high-risk AI system already placed on the market or put into service, unless contractual arrangements stipulate that the obligations are otherwise allocated.",
        "context_id": "article_25_paragraph_1_point_a",
        "metadata_type": "article"
    },
    {
        "question": "Under what condition is a distributor, importer, deployer, or other third-party considered a provider of a high-risk AI system, thereby becoming subject to the obligations of a provider under Article 16?",
        "ground_truth_answer": "A distributor, importer, deployer, or other third-party is considered a provider of a high-risk AI system and subject to the obligations of a provider under Article 16 if they make a substantial modification to a high-risk AI system that has already been placed on the market or has already been put into service, in such a way that it remains a high-risk AI system pursuant to Article 6.",
        "context_id": "article_25_paragraph_1_point_b",
        "metadata_type": "article"
    },
    {
        "question": "Under what specific condition related to modification does a distributor, importer, deployer, or other third-party become considered a provider of a high-risk AI system for the purposes of the EU AI Act?",
        "ground_truth_answer": "A distributor, importer, deployer, or other third-party becomes considered a provider of a high-risk AI system if they modify the intended purpose of an AI system, including a general-purpose AI system, which has not been classified as high-risk and has already been placed on the market or put into service, in such a way that the AI system concerned becomes a high-risk AI system in accordance with Article 6.",
        "context_id": "article_25_paragraph_1_point_c",
        "metadata_type": "article"
    },
    {
        "question": "When an initial provider is no longer considered the provider of a specific AI system, what obligations do they have towards new providers under the EU AI Act, and what is a key exception to these obligations?",
        "ground_truth_answer": "The initial provider shall closely cooperate with new providers, make available the necessary information, and provide the reasonably expected technical access and other assistance required for the fulfilment of obligations set out in this Regulation, particularly concerning the conformity assessment of high-risk AI systems. However, this paragraph does not apply if the initial provider has clearly specified that its AI system is not to be changed into a high-risk AI system and therefore does not fall under the obligation to hand over the documentation.",
        "context_id": "article_25_paragraph_2",
        "metadata_type": "article"
    },
    {
        "question": "When a high-risk AI system functions as a safety component of products covered by the Union harmonisation legislation listed in Section A of Annex I, who is considered the provider of that high-risk AI system, and what obligations are they subject to?",
        "ground_truth_answer": "In such cases, the product manufacturer shall be considered to be the provider of the high-risk AI system and shall be subject to the obligations under Article 16.",
        "context_id": "article_25_paragraph_3",
        "metadata_type": "article"
    },
    {
        "question": "Under what specific circumstance, related to its branding, is a product manufacturer considered the provider of a high-risk AI system that is a safety component of products covered by Union harmonisation legislation listed in Section A of Annex I, and subject to the obligations under Article 16?",
        "ground_truth_answer": "A product manufacturer is considered the provider of such a high-risk AI system when the high-risk AI system is placed on the market together with the product under the name or trademark of the product manufacturer.",
        "context_id": "article_25_paragraph_3_point_a",
        "metadata_type": "article"
    },
    {
        "question": "What is one specific circumstance under which a product manufacturer is considered the provider of a high-risk AI system, and consequently subject to the obligations under Article 16, if the AI system is a safety component of products covered by the Union harmonisation legislation listed in Section A of Annex I?",
        "ground_truth_answer": "The product manufacturer is considered the provider and is subject to the obligations under Article 16 if the high-risk AI system is put into service under the name or trademark of the product manufacturer after the product has been placed on the market.",
        "context_id": "article_25_paragraph_3_point_b",
        "metadata_type": "article"
    },
    {
        "question": "What must be specified by written agreement between the provider of a high-risk AI system and a third party supplying tools, services, components, or processes used or integrated into it?",
        "ground_truth_answer": "By written agreement, the provider of a high-risk AI system and the third party supplying components must specify the necessary information, capabilities, technical access, and other assistance based on the generally acknowledged state of the art, to enable the provider of the high-risk AI system to fully comply with the obligations set out in the Regulation.",
        "context_id": "article_25_paragraph_4",
        "metadata_type": "article"
    },
    {
        "question": "What measures must deployers of high-risk AI systems take regarding the use of these systems?",
        "ground_truth_answer": "Deployers of high-risk AI systems shall take appropriate technical and organisational measures to ensure they use such systems in accordance with the instructions for use accompanying the systems, pursuant to paragraphs 3 and 6.",
        "context_id": "article_26_paragraph_1",
        "metadata_type": "article"
    },
    {
        "question": "What qualifications and provisions must natural persons possess to be assigned human oversight by deployers according to Article 26, paragraph 2?",
        "ground_truth_answer": "Natural persons assigned human oversight by deployers must have the necessary competence, training, authority, and support.",
        "context_id": "article_26_paragraph_2",
        "metadata_type": "article"
    },
    {
        "question": "What obligation does a deployer have regarding input data if they exercise control over it for a high-risk AI system?",
        "ground_truth_answer": "If a deployer exercises control over the input data for a high-risk AI system, they shall ensure that the input data is relevant and sufficiently representative in view of the intended purpose of the system.",
        "context_id": "article_26_paragraph_4",
        "metadata_type": "article"
    },
    {
        "question": "What actions are required of deployers of high-risk AI systems if they have reason to believe that the system's use, even in accordance with its instructions, may present a risk as defined in Article 79(1)?",
        "ground_truth_answer": "If deployers have reason to consider that the use of a high-risk AI system in accordance with the instructions may result in that AI system presenting a risk within the meaning of Article 79(1), they shall, without undue delay, inform the provider or distributor and the relevant market surveillance authority, and shall suspend the use of that system.",
        "context_id": "article_26_paragraph_5",
        "metadata_type": "article"
    },
    {
        "question": "What is the obligation for deployers of high-risk AI systems regarding automatically generated logs, and for what minimum period must these logs be retained?",
        "ground_truth_answer": "Deployers of high-risk AI systems must keep the logs automatically generated by the system, to the extent such logs are under their control. These logs must be retained for a period appropriate to the intended purpose of the high-risk AI system, of at least six months, unless specified otherwise by applicable Union or national law, particularly Union law on the protection of personal data.",
        "context_id": "article_26_paragraph_6",
        "metadata_type": "article"
    },
    {
        "question": "What obligation do deployers who are employers have before implementing a high-risk AI system at the workplace?",
        "ground_truth_answer": "Before putting into service or using a high-risk AI system at the workplace, deployers who are employers must inform workers’ representatives and the affected workers that they will be subject to the use of the high-risk AI system. This information shall be provided, where applicable, in accordance with the rules and procedures laid down in Union and national law and practice on information of workers and their representatives.",
        "context_id": "article_26_paragraph_7",
        "metadata_type": "article"
    },
    {
        "question": "What specific actions must deployers of high-risk AI systems, such as public authorities or Union institutions, take if they discover that the system they intend to use has not been registered in the EU database?",
        "ground_truth_answer": "If such deployers find that the high-risk AI system they envisage using has not been registered in the EU database referred to in Article 71, they shall not use that system and shall inform the provider or the distributor.",
        "context_id": "article_26_paragraph_8",
        "metadata_type": "article"
    },
    {
        "question": "What information should deployers of high-risk AI systems use to comply with their obligation to carry out a data protection impact assessment?",
        "ground_truth_answer": "Deployers of high-risk AI systems shall use the information provided under Article 13 of this Regulation to comply with their obligation to carry out a data protection impact assessment.",
        "context_id": "article_26_paragraph_9",
        "metadata_type": "article"
    },
    {
        "question": "When must the deployer of a high-risk AI system for post-remote biometric identification request authorisation for its use in an investigation for the targeted search of a person suspected or convicted of a criminal offence?",
        "ground_truth_answer": "The deployer must request authorisation ex ante, or without undue delay and no later than 48 hours, from a judicial authority or an administrative authority whose decision is binding and subject to judicial review. This authorisation is required except when the system is used for the initial identification of a potential suspect based on objective and verifiable facts directly linked to the offence.",
        "context_id": "article_26_paragraph_10_part_1",
        "metadata_type": "article"
    },
    {
        "question": "What restriction is placed on law enforcement authorities regarding decisions made solely based on the output of post-remote biometric identification systems?",
        "ground_truth_answer": "Law enforcement authorities shall ensure that no decision that produces an adverse legal effect on a person may be taken based solely on the output of such post-remote biometric identification systems.",
        "context_id": "article_26_paragraph_10_part_2",
        "metadata_type": "article"
    },
    {
        "question": "What documentation is required for each use of high-risk AI systems, such as post-remote biometric identification systems, regardless of purpose or deployer?",
        "ground_truth_answer": "Each use of such high-risk AI systems shall be documented in the relevant police file and shall be made available to the relevant market surveillance authority and the national data protection authority upon request, excluding the disclosure of sensitive operational data related to law enforcement.",
        "context_id": "article_26_paragraph_10_part_3",
        "metadata_type": "article"
    },
    {
        "question": "Which authorities are to receive annual reports concerning the use of post-remote biometric identification systems, and what information is explicitly excluded from being disclosed in these reports?",
        "ground_truth_answer": "Annual reports on the use of post-remote biometric identification systems must be submitted to the relevant market surveillance and national data protection authorities. The disclosure of sensitive operational data related to law enforcement is excluded from these reports.",
        "context_id": "article_26_paragraph_10_part_4",
        "metadata_type": "article"
    },
    {
        "question": "What information must deployers of high-risk AI systems, as referred to in Annex III, provide to natural persons when the systems make or assist in making decisions related to them?",
        "ground_truth_answer": "Deployers of high-risk AI systems referred to in Annex III that make decisions or assist in making decisions related to natural persons shall inform the natural persons that they are subject to the use of the high-risk AI system.",
        "context_id": "article_26_paragraph_11",
        "metadata_type": "article"
    },
    {
        "question": "Under Article 27(1) of the EU AI Act, which deployers are obligated to perform an assessment of the impact on fundamental rights prior to deploying certain high-risk AI systems, and what is the purpose of this assessment?",
        "ground_truth_answer": "Deployers that are bodies governed by public law, private entities providing public services, and deployers of high-risk AI systems referred to in points 5 (b) and (c) of Annex III are obligated to perform an assessment of the impact on fundamental rights that the use of such system may produce. This assessment must be performed prior to deploying a high-risk AI system referred to in Article 6(2), with the exception of high-risk AI systems intended to be used in the area listed in point 2 of Annex III.",
        "context_id": "article_27_paragraph_1",
        "metadata_type": "article"
    },
    {
        "question": "What is the first required component of the fundamental rights impact assessment that specific deployers must perform prior to deploying certain high-risk AI systems?",
        "ground_truth_answer": "The assessment must include a description of the deployer’s processes in which the high-risk AI system will be used in line with its intended purpose.",
        "context_id": "article_27_paragraph_1_point_a",
        "metadata_type": "article"
    },
    {
        "question": "What specific information must deployers include in their fundamental rights impact assessment concerning the intended use of each high-risk AI system?",
        "ground_truth_answer": "Deployers must include a description of the period of time within which, and the frequency with which, each high-risk AI system is intended to be used.",
        "context_id": "article_27_paragraph_1_point_b",
        "metadata_type": "article"
    },
    {
        "question": "What must an assessment of the impact on fundamental rights, performed by certain deployers prior to deploying a high-risk AI system, consist of?",
        "ground_truth_answer": "For the purpose of assessing the impact on fundamental rights, the assessment must consist of the categories of natural persons and groups likely to be affected by the use of the high-risk AI system in the specific context.",
        "context_id": "article_27_paragraph_1_point_c",
        "metadata_type": "article"
    },
    {
        "question": "What must the assessment of the impact on fundamental rights performed by certain deployers consist of, according to Article 27(1)(d) of the EU AI Act?",
        "ground_truth_answer": "The assessment must consist of the specific risks of harm likely to have an impact on the categories of natural persons or groups of persons identified pursuant to point (c) of this paragraph, taking into account the information given by the provider pursuant to Article 13.",
        "context_id": "article_27_paragraph_1_point_d",
        "metadata_type": "article"
    },
    {
        "question": "What must the fundamental rights impact assessment for certain high-risk AI systems, performed by specified deployers, include regarding human oversight measures?",
        "ground_truth_answer": "The assessment must include a description of the implementation of human oversight measures, according to the instructions for use.",
        "context_id": "article_27_paragraph_1_point_e",
        "metadata_type": "article"
    },
    {
        "question": "What specific elements must be included in the fundamental rights impact assessment that certain deployers are required to perform under Article 27(1)(f)?",
        "ground_truth_answer": "The assessment must consist of the measures to be taken in the case of the materialisation of fundamental rights risks, including the arrangements for internal governance and complaint mechanisms.",
        "context_id": "article_27_paragraph_1_point_f",
        "metadata_type": "article"
    },
    {
        "question": "When does a deployer need to update information regarding a high-risk AI system during its use?",
        "ground_truth_answer": "A deployer must update information regarding a high-risk AI system during its use if they consider that any of the elements listed in paragraph 1 has changed or is no longer up to date.",
        "context_id": "article_27_paragraph_2",
        "metadata_type": "article"
    },
    {
        "question": "What is the deployer's obligation after completing the assessment referred to in Article 27, paragraph 1, and what document must be submitted as part of this action, including any potential exemptions?",
        "ground_truth_answer": "After performing the assessment referred to in Article 27, paragraph 1, the deployer shall notify the market surveillance authority of its results. This notification must include the filled-out template referred to in Article 27, paragraph 5. Deployers may be exempt from this notification obligation in the case referred to in Article 46(1).",
        "context_id": "article_27_paragraph_3",
        "metadata_type": "article"
    },
    {
        "question": "What is the requirement for the fundamental rights impact assessment if obligations from Article 27 are already satisfied through a data protection impact assessment conducted under Article 35 of Regulation (EU) 2016/679 or Article 27 of Directive (EU) 2016/680?",
        "ground_truth_answer": "If any of the obligations laid down in Article 27 are already met through a data protection impact assessment, the fundamental rights impact assessment referred to in paragraph 1 of Article 27 shall complement that data protection impact assessment.",
        "context_id": "article_27_paragraph_4",
        "metadata_type": "article"
    },
    {
        "question": "What are the responsibilities of the notifying authority that each Member State is required to designate or establish under Article 28(1) of the EU AI Act?",
        "ground_truth_answer": "Each notifying authority is responsible for setting up and carrying out the necessary procedures for the assessment, designation, and notification of conformity assessment bodies, as well as for their monitoring. These procedures must be developed in cooperation between the notifying authorities of all Member States.",
        "context_id": "article_28_paragraph_1",
        "metadata_type": "article"
    },
    {
        "question": "What types of activities or services are notifying authorities prohibited from offering or providing?",
        "ground_truth_answer": "Notifying authorities are prohibited from offering or providing any activities that conformity assessment bodies perform, and they are also forbidden from providing any consultancy services on a commercial or competitive basis.",
        "context_id": "article_28_paragraph_5",
        "metadata_type": "article"
    },
    {
        "question": "What specific areas of expertise are required for competent personnel within notifying authorities?",
        "ground_truth_answer": "Competent personnel shall have the necessary expertise, where applicable, for their function, in fields such as information technologies, AI and law, including the supervision of fundamental rights.",
        "context_id": "article_28_paragraph_7",
        "metadata_type": "article"
    },
    {
        "question": "What information and documents must be included with an application for notification by a conformity assessment body?",
        "ground_truth_answer": "The application for notification must be accompanied by a description of the conformity assessment activities, the conformity assessment module or modules, and the types of AI systems for which the conformity assessment body claims to be competent. It must also include an accreditation certificate, if one exists, issued by a national accreditation body attesting that the conformity assessment body fulfills the requirements laid down in Article 31. Additionally, any valid document related to existing designations of the applicant notified body under any other Union harmonisation legislation must be added.",
        "context_id": "article_29_paragraph_2",
        "metadata_type": "article"
    },
    {
        "question": "What is the obligation of a conformity assessment body if it cannot provide an accreditation certificate?",
        "ground_truth_answer": "If a conformity assessment body cannot provide an accreditation certificate, it shall provide the notifying authority with all the documentary evidence necessary for the verification, recognition, and regular monitoring of its compliance with the requirements laid down in Article 31.",
        "context_id": "article_29_paragraph_3",
        "metadata_type": "article"
    },
    {
        "question": "What is the obligation of a notified body regarding the updating of its documentation, and what is the reason for this requirement?",
        "ground_truth_answer": "A notified body shall update the documentation referred to in paragraphs 2 and 3 of Article 29 whenever relevant changes occur, in order to enable the authority responsible for notified bodies to monitor and verify continuous compliance with all the requirements laid down in Article 31.",
        "context_id": "article_29_paragraph_4",
        "metadata_type": "article"
    },
    {
        "question": "What must the notifying authority provide to the Commission and other Member States if a notification is not based on an accreditation certificate as referred to in Article 29(2)?",
        "ground_truth_answer": "The notifying authority shall provide the Commission and the other Member States with documentary evidence which attests to the competence of the conformity assessment body and to the arrangements in place to ensure that that body will be monitored regularly and will continue to satisfy the requirements laid down in Article 31.",
        "context_id": "article_30_paragraph_3",
        "metadata_type": "article"
    },
    {
        "question": "Under what conditions and timelines can a conformity assessment body perform the activities of a notified body?",
        "ground_truth_answer": "A conformity assessment body may perform the activities of a notified body only if no objections are raised by the Commission or the other Member States. This no-objection period is two weeks if the notification by a notifying authority includes an accreditation certificate referred to in Article 29(2), or two months if the notification includes documentary evidence referred to in Article 29(3).",
        "context_id": "article_30_paragraph_4",
        "metadata_type": "article"
    },
    {
        "question": "What actions must the Commission take when objections are raised regarding an authorization?",
        "ground_truth_answer": "When objections are raised, the Commission must, without delay, enter into consultations with the relevant Member States and the conformity assessment body. Following this, the Commission must decide whether the authorisation is justified and address its decision to the Member State concerned and to the relevant conformity assessment body.",
        "context_id": "article_30_paragraph_5",
        "metadata_type": "article"
    },
    {
        "question": "What independence requirements apply to notified bodies performing conformity assessment activities for high-risk AI systems?",
        "ground_truth_answer": "Notified bodies must be independent of the provider of a high-risk AI system for which they perform conformity assessment activities. They must also be independent of any other operator having an economic interest in high-risk AI systems assessed, as well as of any competitors of the provider.",
        "context_id": "article_31_paragraph_4",
        "metadata_type": "article"
    },
    {
        "question": "What activities are prohibited for a conformity assessment body, its top-level management, and personnel responsible for conformity assessment tasks regarding high-risk AI systems and their independence?",
        "ground_truth_answer": "A conformity assessment body, its top-level management, and the personnel responsible for carrying out its conformity assessment tasks are prohibited from being directly involved in the design, development, marketing, or use of high-risk AI systems. They are also forbidden from representing parties engaged in these activities. Furthermore, they shall not engage in any activity that might conflict with their independence of judgement or integrity concerning conformity assessment activities for which they are notified, with consultancy services specifically highlighted as an example.",
        "context_id": "article_31_paragraph_5",
        "metadata_type": "article"
    },
    {
        "question": "What are the obligations of notified bodies and their staff regarding the confidentiality of information obtained during conformity assessment activities under the EU AI Act?",
        "ground_truth_answer": "Notified bodies must have documented procedures in place ensuring that their personnel, committees, subsidiaries, subcontractors, and any associated body or personnel of external bodies maintain the confidentiality of information which comes into their possession during the performance of conformity assessment activities, except when its disclosure is required by law. Furthermore, the staff of notified bodies shall be bound to observe professional secrecy with regard to all information obtained in carrying out their tasks, with the exception of disclosure to the notifying authorities of the Member State in which their activities are carried out.",
        "context_id": "article_31_paragraph_7",
        "metadata_type": "article"
    },
    {
        "question": "Under what circumstances are notified bodies exempt from taking out liability insurance for their conformity assessment activities?",
        "ground_truth_answer": "Notified bodies are exempt from taking out appropriate liability insurance if liability is assumed by the Member State in which they are established in accordance with national law, or if that Member State is itself directly responsible for the conformity assessment.",
        "context_id": "article_31_paragraph_9",
        "metadata_type": "article"
    },
    {
        "question": "What kind of personnel, possessing specific experience and knowledge, must notified bodies permanently have available?",
        "ground_truth_answer": "Notified bodies must have permanent availability of sufficient administrative, technical, legal and scientific personnel who possess experience and knowledge relating to the relevant types of AI systems, data and data computing, and relating to the requirements set out in Section 2.",
        "context_id": "article_31_paragraph_11",
        "metadata_type": "article"
    },
    {
        "question": "Under what conditions is a conformity assessment body presumed to comply with the requirements set out in Article 31?",
        "ground_truth_answer": "A conformity assessment body is presumed to comply with the requirements set out in Article 31 if it demonstrates its conformity with the criteria laid down in the relevant harmonised standards (or parts thereof) whose references have been published in the Official Journal of the European Union, in so far as the applicable harmonised standards cover those requirements.",
        "context_id": "article_32_paragraph_1",
        "metadata_type": "article"
    },
    {
        "question": "What two key actions must a notified body take if it subcontracts specific tasks related to conformity assessment or utilizes a subsidiary for such tasks?",
        "ground_truth_answer": "When a notified body subcontracts specific tasks connected with the conformity assessment or has recourse to a subsidiary, it must ensure that the subcontractor or the subsidiary meets the requirements laid down in Article 31, and it must inform the notifying authority accordingly.",
        "context_id": "article_33_paragraph_1",
        "metadata_type": "article"
    },
    {
        "question": "Under what condition can activities be subcontracted or carried out by a subsidiary?",
        "ground_truth_answer": "Activities may be subcontracted or carried out by a subsidiary only with the agreement of the provider.",
        "context_id": "article_33_paragraph_3",
        "metadata_type": "article"
    },
    {
        "question": "For how long must documents related to the qualifications and work of a subcontractor or subsidiary be kept at the disposal of the notifying authority?",
        "ground_truth_answer": "The relevant documents concerning the assessment of the qualifications of the subcontractor or the subsidiary and the work carried out by them under this Regulation shall be kept at the disposal of the notifying authority for a period of five years from the termination date of the subcontracting.",
        "context_id": "article_33_paragraph_4",
        "metadata_type": "article"
    },
    {
        "question": "What considerations must notified bodies take into account when performing their activities to avoid unnecessary burdens for providers, particularly micro- and small enterprises, while ensuring compliance with the EU AI Act?",
        "ground_truth_answer": "Notified bodies must avoid unnecessary burdens for providers by taking due account of the provider's size, the sector in which it operates, its structure, and the degree of complexity of the high-risk AI system concerned. This includes specifically minimising administrative burdens and compliance costs for micro- and small enterprises as defined by Recommendation 2003/361/EC. However, notified bodies must always respect the degree of rigour and the level of protection required for the high-risk AI system's compliance with the requirements of this Regulation.",
        "context_id": "article_34_paragraph_2",
        "metadata_type": "article"
    },
    {
        "question": "Which procedures apply when there are extensions to the scope of the notification?",
        "ground_truth_answer": "The procedures laid down in Articles 29 and 30 shall apply to extensions of the scope of the notification.",
        "context_id": "article_36_paragraph_2",
        "metadata_type": "article"
    },
    {
        "question": "Under what conditions can a notified body's certificates remain valid after it ceases its conformity assessment activities?",
        "ground_truth_answer": "The certificates of the notified body may remain valid for a period of nine months after cessation of its activities, on condition that another notified body has confirmed in writing that it will assume responsibilities for the high-risk AI systems covered by those certificates. The latter notified body shall complete a full assessment of the high-risk AI systems affected by the end of that nine-month-period before issuing new certificates for those systems.",
        "context_id": "article_36_paragraph_3",
        "metadata_type": "article"
    },
    {
        "question": "What actions must a notifying authority take if it has sufficient reason to believe a notified body no longer meets the requirements of Article 31 or is failing to fulfill its obligations?",
        "ground_truth_answer": "If a notifying authority has sufficient reason to consider that a notified body no longer meets the requirements laid down in Article 31 or is failing to fulfill its obligations, it must without delay investigate the matter with the utmost diligence. During this process, it must inform the notified body concerned about the objections raised and give it the opportunity to make its views known. If the authority concludes that the notified body is non-compliant, it shall restrict, suspend, or withdraw the designation as appropriate, depending on the seriousness of the failure. Subsequently, it must immediately inform the Commission and the other Member States of its decision.",
        "context_id": "article_36_paragraph_4",
        "metadata_type": "article"
    },
    {
        "question": "What is the timeframe within which a notified body must inform concerned providers if its designation has been suspended, restricted, or fully or partially withdrawn?",
        "ground_truth_answer": "The notified body shall inform the providers concerned within 10 days.",
        "context_id": "article_36_paragraph_5",
        "metadata_type": "article"
    },
    {
        "question": "What action must a notifying authority take regarding unduly issued certificates when a designation is restricted, suspended, or withdrawn?",
        "ground_truth_answer": "The notifying authority shall require the notified body to suspend or withdraw, within a reasonable period of time determined by the authority, any certificates which were unduly issued, in order to ensure the continuing conformity of high-risk AI systems on the market.",
        "context_id": "article_36_paragraph_7_point_c",
        "metadata_type": "article"
    },
    {
        "question": "What measures must the national competent authorities take upon receiving information regarding the suspension or withdrawal of certificates, when a designation has been restricted, suspended, or withdrawn?",
        "ground_truth_answer": "Upon receiving such information, the national competent authorities shall take the appropriate measures, where necessary, to avoid a potential risk to health, safety or fundamental rights.",
        "context_id": "article_36_paragraph_7_point_e",
        "metadata_type": "article"
    },
    {
        "question": "Under what specific conditions can certificates remain valid despite a designation being suspended or restricted, excluding unduly issued certificates?",
        "ground_truth_answer": "Certificates remain valid if the notifying authority confirms, within one month of the suspension or restriction, that there is no risk to health, safety, or fundamental rights related to the affected certificates, and the notifying authority outlines a timeline for actions to remedy the suspension or restriction.",
        "context_id": "article_36_paragraph_8_point_a",
        "metadata_type": "article"
    },
    {
        "question": "When a notifying authority determines that a notified body, whose designation has been suspended or restricted, lacks the capability to support existing certificates, what action must the provider of the system covered by the certificate take?",
        "ground_truth_answer": "The provider of the system covered by the certificate shall confirm in writing to the national competent authorities of the Member State in which it has its registered place of business, within three months of the suspension or restriction, that another qualified notified body is temporarily assuming the functions of the notified body to monitor and remain responsible for the certificates during the period of suspension or restriction.",
        "context_id": "article_36_paragraph_8_point_b",
        "metadata_type": "article"
    },
    {
        "question": "What specific condition must be met for certificates, excluding unduly issued ones, to remain valid for nine months after a designation withdrawal?",
        "ground_truth_answer": "Certificates remain valid for a period of nine months after a designation withdrawal (excluding unduly issued certificates) if the national competent authority of the Member State where the provider of the high-risk AI system covered by the certificate has its registered place of business has confirmed that there is no risk to health, safety, or fundamental rights associated with the high-risk AI systems concerned.",
        "context_id": "article_36_paragraph_9_point_a",
        "metadata_type": "article"
    },
    {
        "question": "Under what specific circumstances, excluding unduly issued certificates, can certificates remain valid for a period of nine months after a designation has been withdrawn?",
        "ground_truth_answer": "Certificates can remain valid for a period of nine months if another notified body has confirmed in writing that it will assume immediate responsibility for those AI systems and completes its assessment within 12 months of the withdrawal of the designation.",
        "context_id": "article_36_paragraph_9_point_b",
        "metadata_type": "article"
    },
    {
        "question": "Who is authorized to extend the provisional validity of certificates when a designation has been withdrawn, and what is the maximum total duration for such extensions?",
        "ground_truth_answer": "The national competent authority of the Member State where the provider of the system covered by the certificate has its place of business may extend the provisional validity for additional periods of three months, which shall not exceed 12 months in total.",
        "context_id": "article_36_paragraph_9_paragraph_91",
        "metadata_type": "article"
    },
    {
        "question": "What measures can the Commission take if a notifying Member State fails to implement corrective actions for a non-compliant notified body, and how is such a measure adopted?",
        "ground_truth_answer": "If a notifying Member State fails to take the necessary corrective measures after being informed by the Commission that a notified body does not meet or no longer meets the requirements, the Commission may, by means of an implementing act, suspend, restrict, or withdraw the designation. This implementing act shall be adopted in accordance with the examination procedure referred to in Article 98(2).",
        "context_id": "article_37_paragraph_4",
        "metadata_type": "article"
    },
    {
        "question": "What conditions must conformity assessment bodies established under the law of a third country meet to be authorized to carry out activities as notified bodies under the EU AI Act?",
        "ground_truth_answer": "Conformity assessment bodies from a third country with which the Union has concluded an agreement may be authorised to carry out the activities of notified bodies, provided that they meet the requirements laid down in Article 31 or they ensure an equivalent level of compliance.",
        "context_id": "article_39_paragraph_1",
        "metadata_type": "article"
    },
    {
        "question": "When are high-risk AI systems or general-purpose AI models presumed to be in conformity with certain requirements or obligations of the EU AI Act?",
        "ground_truth_answer": "High-risk AI systems or general-purpose AI models are presumed to be in conformity with the requirements set out in Section 2 of this Chapter or with the obligations set out in Chapter V, Sections 2 and 3, of this Regulation when they conform to harmonised standards, or parts thereof, whose references have been published in the Official Journal of the European Union in accordance with Regulation (EU) No 1025/2012, to the extent that those standards cover those requirements or obligations.",
        "context_id": "article_40_paragraph_1",
        "metadata_type": "article"
    },
    {
        "question": "What specific criteria must be met by standards when the Commission issues a standardisation request to European standardisation organisations?",
        "ground_truth_answer": "When the Commission issues a standardisation request to European standardisation organisations, the standards must be clear, consistent (including with standards developed in various sectors for products covered by existing Union harmonisation legislation listed in Annex I), and aim to ensure that high-risk AI systems or general-purpose AI models placed on the market or put into service in the Union meet the relevant requirements or obligations laid down in this Regulation.",
        "context_id": "article_40_paragraph_2",
        "metadata_type": "article"
    },
    {
        "question": "What are the key responsibilities of participants in the standardisation process concerning AI investment, innovation, and international cooperation, as outlined in Article 40(3) of the EU AI Act?",
        "ground_truth_answer": "Participants in the standardisation process shall seek to promote investment and innovation in AI, including through increasing legal certainty, as well as the competitiveness and growth of the Union market. They are also required to contribute to strengthening global cooperation on standardisation and to take into account existing international standards in the field of AI that are consistent with Union values, fundamental rights, and interests. Furthermore, they must enhance multi-stakeholder governance, ensuring balanced representation of interests and effective participation of relevant stakeholders in accordance with Articles 5, 6, and 7 of Regulation (EU) No 1025/2012.",
        "context_id": "article_40_paragraph_3",
        "metadata_type": "article"
    },
    {
        "question": "Under what specific circumstance concerning harmonised standards and fundamental rights can the Commission adopt implementing acts to establish common specifications, provided it has previously requested standardisation organizations to draft a harmonised standard?",
        "ground_truth_answer": "The Commission may adopt implementing acts establishing common specifications if the relevant harmonised standards insufficiently address fundamental rights concerns, provided the Commission has previously requested one or more European standardisation organisations to draft a harmonised standard for the requirements or obligations.",
        "context_id": "article_41_paragraph_1_point_a_point_iii",
        "metadata_type": "article"
    },
    {
        "question": "What is the Commission required to do when drafting common specifications?",
        "ground_truth_answer": "When drafting the common specifications, the Commission shall consult the advisory forum referred to in Article 67.",
        "context_id": "article_41_paragraph_1_paragraph_11",
        "metadata_type": "article"
    },
    {
        "question": "What is the presumption for high-risk AI systems or general-purpose AI models that conform to common specifications mentioned in paragraph 1?",
        "ground_truth_answer": "High-risk AI systems or general-purpose AI models which are in conformity with the common specifications referred to in paragraph 1, or parts of those specifications, shall be presumed to be in conformity with the requirements set out in Section 2 of this Chapter or, as applicable, to comply with the obligations referred to in Sections 2 and 3 of Chapter V, to the extent those common specifications cover those requirements or those obligations.",
        "context_id": "article_41_paragraph_3",
        "metadata_type": "article"
    },
    {
        "question": "What action must the Commission take when a harmonised standard's reference is published in the Official Journal of the European Union?",
        "ground_truth_answer": "When reference to a harmonised standard is published in the Official Journal of the European Union, the Commission shall repeal the implementing acts referred to in paragraph 1, or parts thereof which cover the same requirements set out in Section 2 of this Chapter or, as applicable, the same obligations set out in Sections 2 and 3 of Chapter V.",
        "context_id": "article_41_paragraph_4",
        "metadata_type": "article"
    },
    {
        "question": "What obligation do providers of high-risk AI systems or general-purpose AI models have if they do not comply with the common specifications?",
        "ground_truth_answer": "If providers of high-risk AI systems or general-purpose AI models do not comply with the common specifications, they shall duly justify that they have adopted technical solutions that meet the requirements referred to in Section 2 of this Chapter or, as applicable, comply with the obligations set out in Sections 2 and 3 of Chapter V to a level at least equivalent thereto.",
        "context_id": "article_41_paragraph_5",
        "metadata_type": "article"
    },
    {
        "question": "What action must a Member State take if it considers that a common specification does not entirely meet the requirements set out in Section 2 or comply with obligations set out in Sections 2 and 3 of Chapter V?",
        "ground_truth_answer": "It shall inform the Commission thereof with a detailed explanation.",
        "context_id": "article_41_paragraph_6",
        "metadata_type": "article"
    },
    {
        "question": "Under what conditions are high-risk AI systems presumed to comply with the relevant requirements laid down in Article 10(4)?",
        "ground_truth_answer": "High-risk AI systems that have been trained and tested on data reflecting the specific geographical, behavioural, contextual or functional setting within which they are intended to be used shall be presumed to comply with the relevant requirements laid down in Article 10(4).",
        "context_id": "article_42_paragraph_1",
        "metadata_type": "article"
    },
    {
        "question": "Under what conditions are high-risk AI systems presumed to comply with the cybersecurity requirements set out in Article 15 of this Regulation?",
        "ground_truth_answer": "High-risk AI systems are presumed to comply with the cybersecurity requirements set out in Article 15 of this Regulation if they have been certified or for which a statement of conformity has been issued under a cybersecurity scheme pursuant to Regulation (EU) 2019/881 and the references of which have been published in the Official Journal of the European Union, in so far as the cybersecurity certificate or statement of conformity or parts thereof cover those requirements.",
        "context_id": "article_42_paragraph_2",
        "metadata_type": "article"
    },
    {
        "question": "Which conformity assessment procedure must a provider opt for when demonstrating compliance of a high-risk AI system listed in point 1 of Annex III, if they have applied harmonised standards or common specifications?",
        "ground_truth_answer": "The provider must opt for a conformity assessment procedure based on the assessment of the quality management system and the assessment of the technical documentation, with the involvement of a notified body, referred to in Annex VII.",
        "context_id": "article_43_paragraph_1_point_b",
        "metadata_type": "article"
    },
    {
        "question": "What conformity assessment procedure must a provider follow for a high-risk AI system listed in point 1 of Annex III when harmonised standards referred to in Article 40 do not exist and common specifications referred to in Article 41 are not available?",
        "ground_truth_answer": "In such a scenario, the provider shall follow the conformity assessment procedure set out in Annex VII.",
        "context_id": "article_43_paragraph_1_paragraph_11_point_a",
        "metadata_type": "article"
    },
    {
        "question": "When demonstrating compliance for a high-risk AI system listed in point 1 of Annex III, what conformity assessment procedure must a provider follow if they have not applied, or have applied only part of, the harmonised standard?",
        "ground_truth_answer": "If the provider has not applied, or has applied only part of, the harmonised standard, they shall follow the conformity assessment procedure set out in Annex VII.",
        "context_id": "article_43_paragraph_1_paragraph_11_point_b",
        "metadata_type": "article"
    },
    {
        "question": "When is a provider of a high-risk AI system required to follow the conformity assessment procedure specified in Annex VII for demonstrating compliance?",
        "ground_truth_answer": "A provider of a high-risk AI system must follow the conformity assessment procedure set out in Annex VII when common specifications referred to in point (a) exist, but the provider has not applied them.",
        "context_id": "article_43_paragraph_1_paragraph_11_point_c",
        "metadata_type": "article"
    },
    {
        "question": "What specific circumstance requires a provider to follow the conformity assessment procedure set out in Annex VII when demonstrating compliance for a high-risk AI system, as stated in Article 43 1 1.1 (d)?",
        "ground_truth_answer": "A provider is required to follow the conformity assessment procedure set out in Annex VII when one or more of the harmonised standards referred to in point (a) has been published with a restriction, and only on the part of the standard that was restricted.",
        "context_id": "article_43_paragraph_1_paragraph_11_point_d",
        "metadata_type": "article"
    },
    {
        "question": "Which entity acts as the notified body for the conformity assessment procedure when a high-risk AI system is intended to be put into service by law enforcement, immigration or asylum authorities, or by Union institutions, bodies, offices or agencies?",
        "ground_truth_answer": "When a high-risk AI system is intended to be put into service by law enforcement, immigration or asylum authorities or by Union institutions, bodies, offices or agencies, the market surveillance authority referred to in Article 74(8) or (9), as applicable, shall act as a notified body.",
        "context_id": "article_43_paragraph_1_paragraph_12",
        "metadata_type": "article"
    },
    {
        "question": "For high-risk AI systems referred to in points 2 to 8 of Annex III, what conformity assessment procedure must providers follow, and what is a key characteristic of this procedure concerning external involvement?",
        "ground_truth_answer": "Providers must follow the conformity assessment procedure based on internal control as referred to in Annex VI. A key characteristic of this procedure is that it does not provide for the involvement of a notified body.",
        "context_id": "article_43_paragraph_2",
        "metadata_type": "article"
    },
    {
        "question": "Under what conditions can a product manufacturer, whose high-risk AI system is covered by Union harmonisation legislation listed in Section A of Annex I, utilize the option to opt out from a third-party conformity assessment?",
        "ground_truth_answer": "A product manufacturer, if enabled by a legal act listed in Section A of Annex I to opt out from a third-party conformity assessment (provided that the manufacturer has applied all harmonised standards covering all the relevant requirements), may use that option only if it has also applied harmonised standards or, where applicable, common specifications referred to in Article 41, covering all requirements set out in Section 2 of this Chapter.",
        "context_id": "article_43_paragraph_3",
        "metadata_type": "article"
    },
    {
        "question": "Under what circumstances do changes to high-risk AI systems that continue to learn after being placed on the market or put into service not constitute a substantial modification, thereby not requiring a new conformity assessment?",
        "ground_truth_answer": "Changes to high-risk AI systems and their performance that have been pre-determined by the provider at the moment of the initial conformity assessment and are part of the information contained in the technical documentation referred to in point 2(f) of Annex IV do not constitute a substantial modification.",
        "context_id": "article_43_paragraph_4",
        "metadata_type": "article"
    },
    {
        "question": "What are the maximum initial validity periods for certificates for AI systems covered by Annex I and Annex III, and what are the maximum durations for which these certificates can be extended?",
        "ground_truth_answer": "Certificates for AI systems covered by Annex I shall not exceed five years, and for AI systems covered by Annex III, they shall not exceed four years. When extended, each further period shall also not exceed five years for AI systems covered by Annex I, and four years for AI systems covered by Annex III.",
        "context_id": "article_44_paragraph_2",
        "metadata_type": "article"
    },
    {
        "question": "What measures must a notified body implement when an AI system no longer fulfills the requirements specified in Section 2, and what is the exception to these measures?",
        "ground_truth_answer": "When a notified body determines that an AI system no longer meets the requirements outlined in Section 2, it must, while considering the principle of proportionality, either suspend or withdraw the issued certificate, or impose restrictions on it. This action is not required if the provider of the system ensures compliance with those requirements through appropriate corrective action taken within a deadline set by the notified body.",
        "context_id": "article_44_paragraph_3",
        "metadata_type": "article"
    },
    {
        "question": "What are the requirements for a market surveillance authority to authorise the placing on the market or putting into service of specific high-risk AI systems as a derogation from Article 43?",
        "ground_truth_answer": "A market surveillance authority may grant such authorisation upon a duly justified request for exceptional reasons, specifically public security, the protection of life and health of persons, environmental protection, or the protection of key industrial and infrastructural assets. The authorisation itself shall be for a limited period while necessary conformity assessment procedures are carried out, with the completion of those procedures undertaken without undue delay.",
        "context_id": "article_46_paragraph_1",
        "metadata_type": "article"
    },
    {
        "question": "Under what specific conditions can law-enforcement or civil protection authorities deploy a high-risk AI system without prior authorization, and what are the consequences if authorization is subsequently refused?",
        "ground_truth_answer": "Law-enforcement authorities or civil protection authorities may put a specific high-risk AI system into service without prior authorisation in a duly justified situation of urgency for exceptional reasons of public security or in the case of a specific, substantial and imminent threat to the life or physical safety of natural persons. This is provided that such authorisation is requested during or after the use without undue delay. If the authorisation is refused, the use of the high-risk AI system must be stopped with immediate effect, and all the results and outputs of such use must be immediately discarded.",
        "context_id": "article_46_paragraph_2",
        "metadata_type": "article"
    },
    {
        "question": "Under what condition shall the authorisation for a high-risk AI system be issued, according to Article 46 paragraph 3?",
        "ground_truth_answer": "The authorisation shall be issued only if the market surveillance authority concludes that the high-risk AI system complies with the requirements of Section 2.",
        "context_id": "article_46_paragraph_3",
        "metadata_type": "article"
    },
    {
        "question": "What actions must the Commission take if a Member State raises objections against an authorization or if the Commission deems an authorization contrary to Union law or a Member State's compliance conclusion unfounded?",
        "ground_truth_answer": "If, within 15 calendar days of receipt of the notification, objections are raised by a Member State against an authorization issued by a market surveillance authority of another Member State, or if the Commission considers the authorization to be contrary to Union law, or the conclusion of the Member States regarding the compliance of the system to be unfounded, the Commission shall, without delay, enter into consultations with the relevant Member State. During these consultations, the operators concerned shall be consulted and given the possibility to present their views. Subsequently, the Commission shall decide whether the authorization is justified and address its decision to the Member State concerned and to the relevant operators.",
        "context_id": "article_46_paragraph_5",
        "metadata_type": "article"
    },
    {
        "question": "Which derogations from conformity assessment apply to high-risk AI systems related to products covered by Union harmonisation legislation listed in Section A of Annex I?",
        "ground_truth_answer": "Only the derogations from the conformity assessment established in that specific Union harmonisation legislation shall apply.",
        "context_id": "article_46_paragraph_7",
        "metadata_type": "article"
    },
    {
        "question": "What are the requirements for a provider concerning the EU declaration of conformity for high-risk AI systems?",
        "ground_truth_answer": "A provider must draw up a written, machine-readable, physical or electronically signed EU declaration of conformity for each high-risk AI system. This declaration must identify the specific high-risk AI system and be kept at the disposal of national competent authorities for 10 years after the system has been placed on the market or put into service. A copy of the EU declaration of conformity must also be submitted to the relevant national competent authorities upon request.",
        "context_id": "article_47_paragraph_1",
        "metadata_type": "article"
    },
    {
        "question": "What must an EU declaration of conformity state and contain regarding a high-risk AI system, and what language requirement applies?",
        "ground_truth_answer": "The EU declaration of conformity shall state that the high-risk AI system concerned meets the requirements set out in Section 2. It shall also contain the information set out in Annex V, and must be translated into a language that can be easily understood by the national competent authorities of the Member States in which the high-risk AI system is placed on the market or made available.",
        "context_id": "article_47_paragraph_2",
        "metadata_type": "article"
    },
    {
        "question": "What is required concerning the EU declaration of conformity when high-risk AI systems are also subject to other Union harmonisation legislation that mandates such a declaration?",
        "ground_truth_answer": "A single EU declaration of conformity must be drawn up, covering all Union law applicable to the high-risk AI system. This declaration shall include all necessary information to identify the Union harmonisation legislation it pertains to.",
        "context_id": "article_47_paragraph_3",
        "metadata_type": "article"
    },
    {
        "question": "Under what conditions shall a digital CE marking be used for high-risk AI systems provided digitally?",
        "ground_truth_answer": "For high-risk AI systems provided digitally, a digital CE marking shall be used only if it can be easily accessed via the interface from which that system is accessed or via an easily accessible machine-readable code or other electronic means.",
        "context_id": "article_48_paragraph_2",
        "metadata_type": "article"
    },
    {
        "question": "How must the CE marking be affixed for high-risk AI systems, and what are the alternative locations if direct affixing is not possible or warranted?",
        "ground_truth_answer": "For high-risk AI systems, the CE marking shall be affixed visibly, legibly, and indelibly. If direct affixing is not possible or not warranted due to the nature of the high-risk AI system, it shall be affixed to the packaging or to the accompanying documentation, as appropriate.",
        "context_id": "article_48_paragraph_3",
        "metadata_type": "article"
    },
    {
        "question": "Besides following the CE marking, where else must the identification number of the notified body be indicated for a high-risk AI system?",
        "ground_truth_answer": "The identification number shall also be indicated in any promotional material which mentions that the high-risk AI system fulfils the requirements for CE marking.",
        "context_id": "article_48_paragraph_4",
        "metadata_type": "article"
    },
    {
        "question": "Who is obligated to register high-risk AI systems in the EU database and when must this action occur, considering specific exceptions?",
        "ground_truth_answer": "The provider or, where applicable, the authorised representative, shall register themselves and their system in the EU database referred to in Article 71 before placing on the market or putting into service a high-risk AI system listed in Annex III, with the exception of high-risk AI systems referred to in point 2 of Annex III.",
        "context_id": "article_49_paragraph_1",
        "metadata_type": "article"
    },
    {
        "question": "When must a provider or, where applicable, their authorised representative register an AI system in the EU database referred to in Article 71, if the provider has concluded it is not high-risk according to Article 6(3)?",
        "ground_truth_answer": "They shall register themselves and that system in the EU database before placing the AI system on the market or putting it into service.",
        "context_id": "article_49_paragraph_2",
        "metadata_type": "article"
    },
    {
        "question": "Which specific deployers are required to register themselves and the use of certain high-risk AI systems in the EU database, and under what conditions?",
        "ground_truth_answer": "Deployers that are public authorities, Union institutions, bodies, offices or agencies, or persons acting on their behalf, are required to register themselves, select the system, and register its use in the EU database referred to in Article 71. This obligation applies before putting into service or using a high-risk AI system listed in Annex III, with the exception of high-risk AI systems listed in point 2 of Annex III.",
        "context_id": "article_49_paragraph_3",
        "metadata_type": "article"
    },
    {
        "question": "Where must the registration for high-risk AI systems specified in points 1, 6 and 7 of Annex III, used in law enforcement, migration, asylum, and border control management, be located?",
        "ground_truth_answer": "For high-risk AI systems referred to in points 1, 6 and 7 of Annex III, in the areas of law enforcement, migration, asylum and border control management, the registration must be in a secure non-public section of the EU database referred to in Article 71.",
        "context_id": "article_49_paragraph_4",
        "metadata_type": "article"
    },
    {
        "question": "For high-risk AI systems referred to in points 1, 6 and 7 of Annex III, used in law enforcement, migration, asylum, and border control management, what kind of section of the EU database is used for registration?",
        "ground_truth_answer": "For these high-risk AI systems, the registration is in a secure non-public section of the EU database referred to in Article 71.",
        "context_id": "article_49_paragraph_4_point_a",
        "metadata_type": "article"
    },
    {
        "question": "What information must be included for the registration of high-risk AI systems referred to in points 1, 6 and 7 of Annex III operating in law enforcement, migration, asylum, and border control management?",
        "ground_truth_answer": "For these high-risk AI systems, the registration must include only the information referred to in Section B, points 1 to 5, and points 8 and 9 of Annex VIII, as applicable.",
        "context_id": "article_49_paragraph_4_point_b",
        "metadata_type": "article"
    },
    {
        "question": "For high-risk AI systems specified in points 1, 6, and 7 of Annex III, operating in the domains of law enforcement, migration, asylum, and border control management, where must their registration be located?",
        "ground_truth_answer": "Their registration must be located in a secure non-public section of the EU database referred to in Article 71.",
        "context_id": "article_49_paragraph_4_point_c",
        "metadata_type": "article"
    },
    {
        "question": "For high-risk AI systems referred to in Annex III, points 1, 6, and 7, that are utilized in areas such as law enforcement, migration, asylum, and border control management, where is their registration located and what specific information must be included?",
        "ground_truth_answer": "Their registration shall be in a secure non-public section of the EU database referred to in Article 71 and shall include only the information referred to in points 1, 2, 3 and 5 of Annex IX, as applicable.",
        "context_id": "article_49_paragraph_4_point_d",
        "metadata_type": "article"
    },
    {
        "question": "Which entities are granted access to the secure non-public sections of the EU database for high-risk AI systems used in law enforcement, migration, asylum, and border control management?",
        "ground_truth_answer": "Only the Commission and national authorities referred to in Article 74(8) shall have access to the respective restricted sections of the EU database.",
        "context_id": "article_49_paragraph_4_paragraph_41",
        "metadata_type": "article"
    },
    {
        "question": "Where must high-risk AI systems referred to in point 2 of Annex III be registered?",
        "ground_truth_answer": "High-risk AI systems referred to in point 2 of Annex III must be registered at national level.",
        "context_id": "article_49_paragraph_5",
        "metadata_type": "article"
    },
    {
        "question": "Under what circumstances are providers of AI systems intended to interact directly with natural persons relieved of the obligation to inform those persons that they are interacting with an AI system?",
        "ground_truth_answer": "Providers are not obligated to inform natural persons that they are interacting with an AI system if it is obvious from the perspective of a reasonably well-informed, observant, and circumspect natural person, considering the circumstances and context of use. Additionally, this obligation does not apply to AI systems authorized by law for detecting, preventing, investigating, or prosecuting criminal offences, provided there are appropriate safeguards for third parties' rights and freedoms, unless those systems are made available for the public to report a criminal offence.",
        "context_id": "article_50_paragraph_1",
        "metadata_type": "article"
    },
    {
        "question": "What are the conditions under which the obligation for providers of AI systems to mark synthetic content as artificially generated or manipulated does not apply?",
        "ground_truth_answer": "The obligation to mark synthetic content does not apply to the extent the AI systems perform an assistive function for standard editing or do not substantially alter the input data provided by the deployer or the semantics thereof, or where authorised by law to detect, prevent, investigate or prosecute criminal offences.",
        "context_id": "article_50_paragraph_2",
        "metadata_type": "article"
    },
    {
        "question": "Under what specific conditions are deployers of AI systems for biometric categorisation and emotion recognition exempt from the obligation to inform natural persons and process personal data according to specified EU data protection regulations and directives?",
        "ground_truth_answer": "The obligation does not apply when these AI systems are used for biometric categorisation and emotion recognition, are permitted by law to detect, prevent, or investigate criminal offences, and are subject to appropriate safeguards for the rights and freedoms of third parties and in accordance with Union law.",
        "context_id": "article_50_paragraph_3",
        "metadata_type": "article"
    },
    {
        "question": "Under what circumstances are deployers of an AI system not required to disclose that artificially generated or manipulated text, published for informing the public on matters of public interest, has been used?",
        "ground_truth_answer": "Deployers of an AI system are not required to disclose that artificially generated or manipulated text, published for informing the public on matters of public interest, has been used where the use is authorised by law to detect, prevent, investigate or prosecute criminal offences, or where the AI-generated content has undergone a process of human review or editorial control and a natural or legal person holds editorial responsibility for the publication of the content.",
        "context_id": "article_50_paragraph_4",
        "metadata_type": "article"
    },
    {
        "question": "Under what conditions may the Commission adopt an implementing act to specify common rules for the implementation of obligations regarding the detection and labelling of artificially generated or manipulated content?",
        "ground_truth_answer": "If the Commission deems a code of practice regarding the detection and labelling of artificially generated or manipulated content not adequate, it may adopt an implementing act specifying common rules for the implementation of those obligations in accordance with the examination procedure laid down in Article 98(2).",
        "context_id": "article_50_paragraph_7",
        "metadata_type": "article"
    },
    {
        "question": "When is a general-purpose AI model presumed to have high impact capabilities according to Article 51, paragraph 2 of the EU AI Act?",
        "ground_truth_answer": "A general-purpose AI model is presumed to have high impact capabilities when the cumulative amount of computation used for its training, measured in floating point operations, is greater than 10^25.",
        "context_id": "article_51_paragraph_2",
        "metadata_type": "article"
    },
    {
        "question": "What is the purpose of the delegated acts that the Commission shall adopt in accordance with Article 97 regarding Article 51?",
        "ground_truth_answer": "The delegated acts shall serve to amend the thresholds listed in paragraphs 1 and 2 of Article 51, and to supplement benchmarks and indicators in light of evolving technological developments, such as algorithmic improvements or increased hardware efficiency, when necessary, for these thresholds to reflect the state of the art.",
        "context_id": "article_51_paragraph_3",
        "metadata_type": "article"
    },
    {
        "question": "What are the notification obligations for a provider of a general-purpose AI model that meets the condition referred to in Article 51(1), point (a)?",
        "ground_truth_answer": "The relevant provider shall notify the Commission without delay and in any event within two weeks after that requirement is met or it becomes known that it will be met. That notification shall include the information necessary to demonstrate that the relevant requirement has been met.",
        "context_id": "article_52_paragraph_1",
        "metadata_type": "article"
    },
    {
        "question": "How can the Commission designate a general-purpose AI model as presenting systemic risks?",
        "ground_truth_answer": "The Commission may designate a general-purpose AI model as presenting systemic risks either ex officio or following a qualified alert from the scientific panel pursuant to Article 90(1), point (a), and on the basis of criteria set out in Annex XIII.",
        "context_id": "article_52_paragraph_4",
        "metadata_type": "article"
    },
    {
        "question": "What are the timing requirements for a provider to request a reassessment of a general-purpose AI model's designation as having systemic risk?",
        "ground_truth_answer": "Providers may request an initial reassessment at the earliest six months after the designation decision. If the Commission, following its reassessment, decides to maintain the designation as a general-purpose AI model with systemic risk, providers may request further reassessment at the earliest six months after that subsequent decision.",
        "context_id": "article_52_paragraph_5",
        "metadata_type": "article"
    },
    {
        "question": "What obligation do providers of general-purpose AI models have regarding technical documentation?",
        "ground_truth_answer": "Providers of general-purpose AI models must draw up and keep up-to-date the technical documentation of the model, which includes its training and testing process and the results of its evaluation. This documentation must contain, at a minimum, the information set out in Annex XI and be provided, upon request, to the AI Office and the national competent authorities.",
        "context_id": "article_53_paragraph_1_point_a",
        "metadata_type": "article"
    },
    {
        "question": "What is one specific obligation for providers of general-purpose AI models concerning copyright and related rights?",
        "ground_truth_answer": "Providers of general-purpose AI models must put in place a policy to comply with Union law on copyright and related rights, and in particular, they must identify and comply with a reservation of rights expressed pursuant to Article 4(3) of Directive (EU) 2019/790, including through state-of-the-art technologies.",
        "context_id": "article_53_paragraph_1_point_c",
        "metadata_type": "article"
    },
    {
        "question": "What information are providers of general-purpose AI models required to draw up and make publicly available concerning their training content?",
        "ground_truth_answer": "Providers of general-purpose AI models are required to draw up and make publicly available a sufficiently detailed summary about the content used for training, which must adhere to a template provided by the AI Office.",
        "context_id": "article_53_paragraph_1_point_d",
        "metadata_type": "article"
    },
    {
        "question": "Under what conditions are providers of AI models exempt from the obligations outlined in paragraph 1, points (a) and (b) of Article 53, and what is a key limitation to this exemption?",
        "ground_truth_answer": "Providers of AI models are exempt from these obligations if the models are released under a free and open-source licence that permits access, usage, modification, and distribution, and if their parameters, including the weights, the information on the model architecture, and the information on model usage, are made publicly available. However, this exception does not apply to general-purpose AI models with systemic risks.",
        "context_id": "article_53_paragraph_2",
        "metadata_type": "article"
    },
    {
        "question": "What are the ways for providers of general-purpose AI models to demonstrate compliance with their obligations as outlined in paragraph 1 of Article 53?",
        "ground_truth_answer": "Providers of general-purpose AI models can rely on codes of practice within the meaning of Article 56 until a harmonised standard is published, or they can comply with European harmonised standards. If they do not adhere to an approved code of practice or do not comply with a European harmonised standard, they shall demonstrate alternative adequate means of compliance for assessment by the Commission.",
        "context_id": "article_53_paragraph_4",
        "metadata_type": "article"
    },
    {
        "question": "For what purpose is the Commission empowered to adopt delegated acts under Article 53, paragraph 5?",
        "ground_truth_answer": "The Commission is empowered to adopt delegated acts to detail measurement and calculation methodologies with a view to allowing for comparable and verifiable documentation. This is for the purpose of facilitating compliance with Annex XI, particularly points 2 (d) and (e).",
        "context_id": "article_53_paragraph_5",
        "metadata_type": "article"
    },
    {
        "question": "What obligation do providers established in third countries have prior to placing a general-purpose AI model on the Union market?",
        "ground_truth_answer": "Prior to placing a general-purpose AI model on the Union market, providers established in third countries shall, by written mandate, appoint an authorised representative which is established in the Union.",
        "context_id": "article_54_paragraph_1",
        "metadata_type": "article"
    },
    {
        "question": "What is one specific task that the mandate must empower an authorized representative to carry out under Article 54, paragraph 3, point a of the EU AI Act?",
        "ground_truth_answer": "The mandate must empower the authorised representative to verify that the technical documentation specified in Annex XI has been drawn up and all obligations referred to in Article 53 and, where applicable, Article 55 have been fulfilled by the provider.",
        "context_id": "article_54_paragraph_3_point_a",
        "metadata_type": "article"
    },
    {
        "question": "For how long after a general-purpose AI model has been placed on the market must an authorised representative keep a copy of the technical documentation specified in Annex XI available to the AI Office and national competent authorities?",
        "ground_truth_answer": "An authorised representative must keep a copy of the technical documentation specified in Annex XI available to the AI Office and national competent authorities for a period of 10 years after the general-purpose AI model has been placed on the market.",
        "context_id": "article_54_paragraph_3_point_b",
        "metadata_type": "article"
    },
    {
        "question": "What two actions must an authorised representative take if they believe a provider is acting contrary to its obligations under the EU AI Act?",
        "ground_truth_answer": "The authorised representative shall terminate the mandate and immediately inform the AI Office about the termination of the mandate and the reasons for it.",
        "context_id": "article_54_paragraph_5",
        "metadata_type": "article"
    },
    {
        "question": "Under what circumstances are providers of general-purpose AI models exempt from the obligations set out in this Article?",
        "ground_truth_answer": "Providers of general-purpose AI models are exempt if they are released under a free and open-source licence that allows for the access, usage, modification, and distribution of the model, and if their parameters, including the weights, the information on the model architecture, and the information on model usage, are made publicly available. This exemption does not apply if the general-purpose AI models present systemic risks.",
        "context_id": "article_54_paragraph_6",
        "metadata_type": "article"
    },
    {
        "question": "What specific actions are required from providers of general-purpose AI models with systemic risk regarding model evaluation, in addition to other listed obligations?",
        "ground_truth_answer": "Providers of general-purpose AI models with systemic risk shall perform model evaluation in accordance with standardised protocols and tools reflecting the state of the art, including conducting and documenting adversarial testing of the model with a view to identifying and mitigating systemic risks.",
        "context_id": "article_55_paragraph_1_point_a",
        "metadata_type": "article"
    },
    {
        "question": "What specific obligation do providers of general-purpose AI models with systemic risk have regarding systemic risks at Union level?",
        "ground_truth_answer": "Providers of general-purpose AI models with systemic risk must assess and mitigate possible systemic risks at Union level, including their sources, that may stem from the development, the placing on the market, or the use of these models.",
        "context_id": "article_55_paragraph_1_point_b",
        "metadata_type": "article"
    },
    {
        "question": "What are the specific obligations for providers of general-purpose AI models with systemic risk concerning serious incidents and corrective measures?",
        "ground_truth_answer": "Providers of general-purpose AI models with systemic risk are obligated to keep track of, document, and report, without undue delay, to the AI Office and, as appropriate, to national competent authorities, relevant information about serious incidents and possible corrective measures to address them.",
        "context_id": "article_55_paragraph_1_point_c",
        "metadata_type": "article"
    },
    {
        "question": "What specific cybersecurity obligation is placed upon providers of general-purpose AI models with systemic risk under Article 55 1 (d) of the EU AI Act?",
        "ground_truth_answer": "Providers of general-purpose AI models with systemic risk shall ensure an adequate level of cybersecurity protection for the general-purpose AI model itself, as well as its physical infrastructure.",
        "context_id": "article_55_paragraph_1_point_d",
        "metadata_type": "article"
    },
    {
        "question": "What must providers of general-purpose AI models with systemic risks do if they do not adhere to an approved code of practice or do not comply with a European harmonised standard?",
        "ground_truth_answer": "They shall demonstrate alternative adequate means of compliance for assessment by the Commission.",
        "context_id": "article_55_paragraph_2",
        "metadata_type": "article"
    },
    {
        "question": "What specific issues must codes of practice cover regarding the assessment and management of systemic risks at Union level, as aimed for by the AI Office and the Board?",
        "ground_truth_answer": "The codes of practice must cover the measures, procedures, and modalities for the assessment and management of the systemic risks at Union level, including their documentation. These must be proportionate to the risks, take into consideration their severity and probability, and account for the specific challenges of tackling those risks in light of how they may emerge and materialise along the AI value chain.",
        "context_id": "article_56_paragraph_2_point_d",
        "metadata_type": "article"
    },
    {
        "question": "What responsibilities do the AI Office and the Board have regarding the content and consideration of codes of practice at Union level?",
        "ground_truth_answer": "The AI Office and the Board shall aim to ensure that the codes of practice clearly set out their specific objectives, contain commitments or measures (including key performance indicators as appropriate) to achieve those objectives, and take due account of the needs and interests of all interested parties, including affected persons, at Union level.",
        "context_id": "article_56_paragraph_4",
        "metadata_type": "article"
    },
    {
        "question": "Which entity has the authority to approve a code of practice and grant it general validity within the Union?",
        "ground_truth_answer": "The Commission may, by way of an implementing act, approve a code of practice and give it a general validity within the Union.",
        "context_id": "article_56_paragraph_6",
        "metadata_type": "article"
    },
    {
        "question": "For providers of general-purpose AI models that do not present systemic risks, what are the potential limitations on their adherence to the codes of practice invited by the AI Office?",
        "ground_truth_answer": "For providers of general-purpose AI models not presenting systemic risks, their adherence to the codes of practice invited by the AI Office may be limited to the obligations provided for in Article 53, unless they explicitly declare their interest to join the full code.",
        "context_id": "article_56_paragraph_7",
        "metadata_type": "article"
    },
    {
        "question": "What actions may the Commission take if a code of practice is not finalized by 2 August 2025 or is deemed inadequate following the AI Office's assessment under paragraph 6 of Article 56?",
        "ground_truth_answer": "If a code of practice cannot be finalised by 2 August 2025, or if the AI Office deems it inadequate following its assessment under paragraph 6 of Article 56, the Commission may provide, by means of implementing acts, common rules for the implementation of the obligations provided for in Articles 53 and 55, including the issues set out in paragraph 2 of Article 56. These implementing acts shall be adopted in accordance with the examination procedure referred to in Article 98(2).",
        "context_id": "article_56_paragraph_9",
        "metadata_type": "article"
    },
    {
        "question": "By what date must AI regulatory sandboxes established by Member States be operational?",
        "ground_truth_answer": "AI regulatory sandboxes established by Member States must be operational by 2 August 2026.",
        "context_id": "article_57_paragraph_1",
        "metadata_type": "article"
    },
    {
        "question": "What obligations do Member States have concerning the resources of the competent authorities referred to in paragraphs 1 and 2 of Article 57?",
        "ground_truth_answer": "Member States shall ensure that the competent authorities referred to in paragraphs 1 and 2 allocate sufficient resources to comply with Article 57 effectively and in a timely manner.",
        "context_id": "article_57_paragraph_4",
        "metadata_type": "article"
    },
    {
        "question": "What conditions must be met regarding the sandbox plan for AI regulatory sandboxes, and who must agree on it?",
        "ground_truth_answer": "AI regulatory sandboxes must operate pursuant to a specific sandbox plan agreed between the providers or prospective providers and the competent authority.",
        "context_id": "article_57_paragraph_5",
        "metadata_type": "article"
    },
    {
        "question": "What responsibilities do competent authorities hold within the AI regulatory sandbox, and for what purpose?",
        "ground_truth_answer": "Competent authorities shall provide, as appropriate, guidance, supervision, and support within the AI regulatory sandbox with a view to identifying risks, particularly to fundamental rights, health, and safety, testing, mitigation measures, and their effectiveness concerning the obligations and requirements of this Regulation and, where relevant, other Union and national law supervised within the sandbox.",
        "context_id": "article_57_paragraph_6",
        "metadata_type": "article"
    },
    {
        "question": "What documentation must competent authorities provide to providers participating in the AI regulatory sandbox, and how can this documentation be used?",
        "ground_truth_answer": "Competent authorities must provide an exit report detailing the activities carried out in the sandbox and the related results and learning outcomes. Upon request, they must also provide a written proof of activities successfully carried out in the sandbox. Providers may use this documentation to demonstrate their compliance with the Regulation through the conformity assessment process or relevant market surveillance activities, with exit reports and written proof being taken positively into account by market surveillance authorities and notified bodies to accelerate conformity assessment procedures.",
        "context_id": "article_57_paragraph_7",
        "metadata_type": "article"
    },
    {
        "question": "What are the requirements for an exit report to be made publicly available under the EU AI Act?",
        "ground_truth_answer": "An exit report may be made publicly available through the single information platform if both the provider or prospective provider and the national competent authority explicitly agree.",
        "context_id": "article_57_paragraph_8",
        "metadata_type": "article"
    },
    {
        "question": "What must national competent authorities ensure regarding the involvement of other authorities when innovative AI systems in an AI regulatory sandbox process personal data or fall under the supervisory remit of other national authorities?",
        "ground_truth_answer": "National competent authorities must ensure that national data protection authorities and those other national or competent authorities are associated with the operation of the AI regulatory sandbox and involved in the supervision of those aspects, to the extent of their respective tasks and powers.",
        "context_id": "article_57_paragraph_10",
        "metadata_type": "article"
    },
    {
        "question": "What actions are required if significant risks to health and safety and fundamental rights identified during the development and testing of AI systems within an AI regulatory sandbox cannot be effectively mitigated?",
        "ground_truth_answer": "If no effective mitigation is possible for significant risks to health and safety and fundamental rights, national competent authorities shall have the power to temporarily or permanently suspend the testing process or the participation in the sandbox, and they shall inform the AI Office of such a decision.",
        "context_id": "article_57_paragraph_11",
        "metadata_type": "article"
    },
    {
        "question": "Under what conditions will prospective providers participating in the AI regulatory sandbox not face administrative fines for infringements of the EU AI Act?",
        "ground_truth_answer": "Prospective providers participating in the AI regulatory sandbox will not face administrative fines for infringements of the EU AI Act if they observe the specific plan and the terms and conditions for their participation and follow in good faith the guidance given by the national competent authority.",
        "context_id": "article_57_paragraph_12",
        "metadata_type": "article"
    },
    {
        "question": "What kind of information must national competent authorities include in their annual reports to the AI Office and the Board regarding the AI regulatory sandboxes?",
        "ground_truth_answer": "National competent authorities must include information on the progress and results of the implementation of the sandboxes, including best practices, incidents, lessons learnt and recommendations on their setup. Where relevant, they must also provide recommendations on the application and possible revision of the EU AI Act, including its delegated and implementing acts, and on the application of other Union law supervised by the competent authorities within the sandbox.",
        "context_id": "article_57_paragraph_16",
        "metadata_type": "article"
    },
    {
        "question": "For what purposes will the single and dedicated interface developed by the Commission for AI regulatory sandboxes be used?",
        "ground_truth_answer": "The interface will be used to allow stakeholders to interact with AI regulatory sandboxes, to raise enquiries with competent authorities, and to seek non-binding guidance on the conformity of innovative products, services, business models embedding AI technologies, in accordance with Article 62(1), point (c).",
        "context_id": "article_57_paragraph_17",
        "metadata_type": "article"
    },
    {
        "question": "What common principles must the Commission's implementing acts include regarding the AI regulatory sandboxes, to avoid fragmentation across the Union?",
        "ground_truth_answer": "The implementing acts must include common principles on procedures for the application, participation, monitoring, exiting from and termination of the AI regulatory sandbox, including the sandbox plan and the exit report.",
        "context_id": "article_58_paragraph_1_point_b",
        "metadata_type": "article"
    },
    {
        "question": "What is the timeframe within which national competent authorities must inform applicants of their decision regarding AI regulatory sandbox applications?",
        "ground_truth_answer": "National competent authorities must inform applicants of their decision within three months of the application.",
        "context_id": "article_58_paragraph_2_point_a",
        "metadata_type": "article"
    },
    {
        "question": "According to Article 58 2 (d) of the EU AI Act, what is the policy regarding access fees for AI regulatory sandboxes for SMEs, including start-ups?",
        "ground_truth_answer": "Access to the AI regulatory sandboxes shall be free of charge for SMEs, including start-ups. However, national competent authorities may recover exceptional costs in a fair and proportionate manner.",
        "context_id": "article_58_paragraph_2_point_d",
        "metadata_type": "article"
    },
    {
        "question": "Which relevant actors within the AI ecosystem should AI regulatory sandboxes facilitate the involvement of, as ensured by the implementing acts?",
        "ground_truth_answer": "AI regulatory sandboxes should facilitate the involvement of notified bodies, standardisation organisations, SMEs (including start-ups), enterprises, innovators, testing and experimentation facilities, research and experimentation labs, European Digital Innovation Hubs, centres of excellence, and individual researchers.",
        "context_id": "article_58_paragraph_2_point_f",
        "metadata_type": "article"
    },
    {
        "question": "What specific conditions must implementing acts ensure regarding the procedures, processes, and administrative requirements for AI regulatory sandboxes, particularly concerning SMEs and Union-wide recognition?",
        "ground_truth_answer": "Implementing acts must ensure that procedures, processes, and administrative requirements for application, selection, participation, and exiting an AI regulatory sandbox are simple, easily intelligible, and clearly communicated. This is intended to facilitate the participation of SMEs, including start-ups, with limited legal and administrative capacities. These requirements must also be streamlined across the Union to avoid fragmentation. Additionally, participation in an AI regulatory sandbox established by a Member State or the European Data Protection Supervisor must be mutually and uniformly recognised and carry the same legal effects across the Union.",
        "context_id": "article_58_paragraph_2_point_g",
        "metadata_type": "article"
    },
    {
        "question": "What specific aspects must implementing acts ensure AI regulatory sandboxes facilitate regarding tools and infrastructure for AI systems, according to Article 58 2 (i)?",
        "ground_truth_answer": "Implementing acts must ensure that AI regulatory sandboxes facilitate the development of tools and infrastructure for testing, benchmarking, assessing, and explaining dimensions of AI systems relevant for regulatory learning, such as accuracy, robustness, and cybersecurity. They must also facilitate measures to mitigate risks to fundamental rights and society at large.",
        "context_id": "article_58_paragraph_2_point_i",
        "metadata_type": "article"
    },
    {
        "question": "What types of services shall prospective providers in the AI regulatory sandboxes, particularly SMEs and start-ups, be directed to?",
        "ground_truth_answer": "Prospective providers in the AI regulatory sandboxes, especially SMEs and start-ups, shall be directed to pre-deployment services, such as guidance on the implementation of this Regulation, and other value-adding services including help with standardisation documents and certification, testing and experimentation facilities, European Digital Innovation Hubs, and centres of excellence.",
        "context_id": "article_58_paragraph_3",
        "metadata_type": "article"
    },
    {
        "question": "What actions are national competent authorities required to take when considering authorization for real-world testing supervised within an AI regulatory sandbox?",
        "ground_truth_answer": "When national competent authorities consider authorizing testing in real-world conditions supervised within an AI regulatory sandbox, they must specifically agree the terms and conditions of such testing and, in particular, the appropriate safeguards with the participants, with a view to protecting fundamental rights, health and safety. Furthermore, where appropriate, they shall cooperate with other national competent authorities to ensure consistent practices across the Union.",
        "context_id": "article_58_paragraph_4",
        "metadata_type": "article"
    },
    {
        "question": "What specific areas of substantial public interest must AI systems address for personal data to be processed in the AI regulatory sandbox, as per Article 59 1 (a) (i)?",
        "ground_truth_answer": "AI systems must address public safety and public health, including disease detection, diagnosis prevention, control and treatment, and improvement of healthcare systems.",
        "context_id": "article_59_paragraph_1_point_a_point_i",
        "metadata_type": "article"
    },
    {
        "question": "What are the specific areas in which AI systems must be developed to qualify for processing personal data in the AI regulatory sandbox, provided they safeguard substantial public interest?",
        "ground_truth_answer": "AI systems must be developed in one or more of the following areas: a high level of protection and improvement of the quality of the environment, protection of biodiversity, protection against pollution, green transition measures, or climate change mitigation and adaptation measures.",
        "context_id": "article_59_paragraph_1_point_a_point_ii",
        "metadata_type": "article"
    },
    {
        "question": "What specific areas are listed where AI systems developed for safeguarding substantial public interest can operate within the AI regulatory sandbox, under the condition that personal data lawfully collected for other purposes may be processed?",
        "ground_truth_answer": "AI systems must be developed in one or more of the following areas: safety and resilience of transport systems and mobility, critical infrastructure and networks.",
        "context_id": "article_59_paragraph_1_point_a_point_iv",
        "metadata_type": "article"
    },
    {
        "question": "What specific condition, related to the necessity of data type, must be met for personal data lawfully collected for other purposes to be processed in the AI regulatory sandbox for developing, training, and testing AI systems?",
        "ground_truth_answer": "For personal data to be processed in the AI regulatory sandbox, the data processed must be necessary for complying with one or more of the requirements referred to in Chapter III, Section 2, and those requirements cannot effectively be fulfilled by processing anonymised, synthetic, or other non-personal data.",
        "context_id": "article_59_paragraph_1_point_b",
        "metadata_type": "article"
    },
    {
        "question": "What types of mechanisms must be in place in the AI regulatory sandbox to address potential high risks to data subjects' rights and freedoms during experimentation?",
        "ground_truth_answer": "In the AI regulatory sandbox, effective monitoring mechanisms must be in place to identify if any high risks to the rights and freedoms of data subjects may arise during experimentation, along with response mechanisms to promptly mitigate those risks and, if necessary, stop the processing.",
        "context_id": "article_59_paragraph_1_point_c",
        "metadata_type": "article"
    },
    {
        "question": "What is a required condition concerning the data processing environment for personal data handled in the AI regulatory sandbox?",
        "ground_truth_answer": "Any personal data processed within the AI regulatory sandbox must reside in a functionally separate, isolated, and protected data processing environment under the control of the prospective provider, with access restricted solely to authorised persons.",
        "context_id": "article_59_paragraph_1_point_d",
        "metadata_type": "article"
    },
    {
        "question": "What are the restrictions on sharing personal data, both originally collected and newly created, when processed within an AI regulatory sandbox?",
        "ground_truth_answer": "Providers can only share originally collected data in accordance with Union data protection law, and any personal data created within the sandbox cannot be shared outside of it.",
        "context_id": "article_59_paragraph_1_point_e",
        "metadata_type": "article"
    },
    {
        "question": "What conditions apply to the processing of personal data in an AI regulatory sandbox regarding its impact on data subjects and their rights?",
        "ground_truth_answer": "Any processing of personal data in the context of the AI regulatory sandbox must neither lead to measures or decisions affecting the data subjects nor affect the application of their rights laid down in Union law on the protection of personal data.",
        "context_id": "article_59_paragraph_1_point_f",
        "metadata_type": "article"
    },
    {
        "question": "What is the requirement for personal data processed within an AI regulatory sandbox once participation in the sandbox concludes or its retention period ends?",
        "ground_truth_answer": "Any personal data processed in the context of the sandbox must be protected by appropriate technical and organisational measures and deleted once the participation in the sandbox has terminated or the personal data has reached the end of its retention period.",
        "context_id": "article_59_paragraph_1_point_g",
        "metadata_type": "article"
    },
    {
        "question": "What documentation must be maintained in the AI regulatory sandbox concerning the training, testing, and validation of an AI system for which personal data is processed?",
        "ground_truth_answer": "A complete and detailed description of the process and rationale behind the training, testing, and validation of the AI system, along with the testing results, must be kept as part of the technical documentation referred to in Annex IV.",
        "context_id": "article_59_paragraph_1_point_i",
        "metadata_type": "article"
    },
    {
        "question": "What is required to be published by competent authorities regarding AI projects in the regulatory sandbox when personal data is processed for development, training, and testing?",
        "ground_truth_answer": "When personal data is processed for developing, training, and testing AI systems in the AI regulatory sandbox, a short summary of the AI project, its objectives, and expected results must be published on the website of the competent authorities. This publication obligation does not extend to sensitive operational data related to the activities of law enforcement, border control, immigration, or asylum authorities.",
        "context_id": "article_59_paragraph_1_point_j",
        "metadata_type": "article"
    },
    {
        "question": "What are the requirements for law enforcement authorities to process personal data in AI regulatory sandboxes for the prevention, investigation, detection, or prosecution of criminal offences, or the execution of criminal penalties, including safeguarding against and preventing threats to public security?",
        "ground_truth_answer": "For these purposes, the processing of personal data in AI regulatory sandboxes by law enforcement authorities must be based on a specific Union or national law and is subject to the same cumulative conditions as referred to in paragraph 1 of Article 59.",
        "context_id": "article_59_paragraph_2",
        "metadata_type": "article"
    },
    {
        "question": "Who is permitted to conduct real-world testing of high-risk AI systems outside AI regulatory sandboxes, and under what general conditions?",
        "ground_truth_answer": "Testing of high-risk AI systems in real world conditions outside AI regulatory sandboxes may be conducted by providers or prospective providers of high-risk AI systems listed in Annex III. Such testing must be conducted in accordance with Article 60 and the real-world testing plan referred to in that Article, and without prejudice to the prohibitions under Article 5.",
        "context_id": "article_60_paragraph_1",
        "metadata_type": "article"
    },
    {
        "question": "What conditions must be met for providers to conduct AI system testing in real-world conditions, particularly concerning the approval process from the market surveillance authority?",
        "ground_truth_answer": "Providers or prospective providers may conduct testing in real-world conditions only if the market surveillance authority in the Member State where the testing is to be conducted has approved both the testing in real-world conditions and the real-world testing plan. If the market surveillance authority has not provided an answer within 30 days, the testing and plan are understood to have been approved. However, if national law does not provide for a tacit approval, the testing in real-world conditions shall remain subject to an explicit authorisation.",
        "context_id": "article_60_paragraph_4_point_b",
        "metadata_type": "article"
    },
    {
        "question": "What are the specific registration requirements for providers or prospective providers conducting real-world testing of high-risk AI systems referred to in points 1, 6, and 7 of Annex III concerning law enforcement, migration, asylum, and border control management?",
        "ground_truth_answer": "Providers or prospective providers of high-risk AI systems referred to in points 1, 6 and 7 of Annex III in the areas of law enforcement, migration, asylum and border control management, must register the testing in real-world conditions in the secure non-public section of the EU database according to Article 49(4), point (d), with a Union-wide unique single identification number and with the information specified therein.",
        "context_id": "article_60_paragraph_4_point_c",
        "metadata_type": "article"
    },
    {
        "question": "Under what condition can data collected and processed during real-world testing be transferred to third countries?",
        "ground_truth_answer": "Data collected and processed for the purpose of testing in real-world conditions may be transferred to third countries only if appropriate and applicable safeguards under Union law are implemented.",
        "context_id": "article_60_paragraph_4_point_e",
        "metadata_type": "article"
    },
    {
        "question": "What is the maximum duration for real-world testing of AI systems by providers or prospective providers, and under what conditions can it be extended?",
        "ground_truth_answer": "Real-world testing by providers or prospective providers must not last longer than necessary to achieve its objectives and in any case not longer than six months. This period may be extended for an additional six months, subject to prior notification to the market surveillance authority by the provider or prospective provider, accompanied by an explanation of the need for such an extension.",
        "context_id": "article_60_paragraph_4_point_f",
        "metadata_type": "article"
    },
    {
        "question": "What specific protection must be afforded to vulnerable persons, such as those with age or disability, when they are subjects of real-world AI testing, according to Article 60 4 (g) of the EU AI Act?",
        "ground_truth_answer": "Subjects of real-world AI testing who are persons belonging to vulnerable groups due to their age or disability must be appropriately protected.",
        "context_id": "article_60_paragraph_4_point_g",
        "metadata_type": "article"
    },
    {
        "question": "When a provider and a deployer cooperate to conduct real-world testing of an AI system, what type of agreement must they conclude, and what should it specify?",
        "ground_truth_answer": "When a provider or prospective provider organises real-world testing in cooperation with one or more deployers or prospective deployers, they shall conclude an agreement. This agreement must specify their respective roles and responsibilities to ensure compliance with the provisions for testing in real-world conditions under this Regulation and under other applicable Union and national law.",
        "context_id": "article_60_paragraph_4_point_h",
        "metadata_type": "article"
    },
    {
        "question": "Under what specific conditions can law enforcement conduct AI system testing in real-world conditions without seeking informed consent?",
        "ground_truth_answer": "In cases where seeking informed consent would prevent the AI system from being tested, the testing itself and its outcome must not have any negative effect on the subjects, and their personal data must be deleted after the test is performed.",
        "context_id": "article_60_paragraph_4_point_i",
        "metadata_type": "article"
    },
    {
        "question": "What are the requirements for oversight of real-world testing conducted by providers or prospective providers under Article 60 4 (j)?",
        "ground_truth_answer": "The testing in real-world conditions must be effectively overseen by the provider or prospective provider, as well as by deployers or prospective deployers, through persons who are suitably qualified in the relevant field and possess the necessary capacity, training, and authority to perform their tasks.",
        "context_id": "article_60_paragraph_4_point_j",
        "metadata_type": "article"
    },
    {
        "question": "What is one condition that must be met for providers or prospective providers to conduct real-world testing of an AI system?",
        "ground_truth_answer": "One condition for providers or prospective providers to conduct real-world testing of an AI system is that the predictions, recommendations, or decisions of the AI system can be effectively reversed and disregarded.",
        "context_id": "article_60_paragraph_4_point_k",
        "metadata_type": "article"
    },
    {
        "question": "What rights do subjects of real-world testing, or their legally designated representatives, possess regarding their participation and personal data?",
        "ground_truth_answer": "Subjects of testing in real-world conditions, or their legally designated representative, may withdraw from the testing at any time by revoking their informed consent, without any resulting detriment or having to provide any justification. They may also request the immediate and permanent deletion of their personal data, though the withdrawal of informed consent shall not affect activities already carried out.",
        "context_id": "article_60_paragraph_5",
        "metadata_type": "article"
    },
    {
        "question": "What specific powers must Member States confer on their market surveillance authorities regarding providers and prospective providers, and real-world testing, according to Article 60, paragraph 6?",
        "ground_truth_answer": "Member States shall confer on their market surveillance authorities the powers of requiring providers and prospective providers to provide information, of carrying out unannounced remote or on-site inspections, and of performing checks on the conduct of the testing in real world conditions and the related high-risk AI systems.",
        "context_id": "article_60_paragraph_6",
        "metadata_type": "article"
    },
    {
        "question": "What must a provider or prospective provider establish upon the termination of testing an AI system in real world conditions?",
        "ground_truth_answer": "Upon the termination of testing an AI system in real world conditions, the provider or prospective provider shall establish a procedure for the prompt recall of the AI system.",
        "context_id": "article_60_paragraph_7",
        "metadata_type": "article"
    },
    {
        "question": "What specific rights and guarantees must subjects of real-world testing be informed about when providing freely-given informed consent under Article 61 (c) of the EU AI Act?",
        "ground_truth_answer": "Subjects of real-world testing must be informed about their rights and the guarantees regarding their participation, particularly their right to refuse to participate in, and the right to withdraw from, testing in real world conditions at any time without any resulting detriment and without having to provide any justification.",
        "context_id": "article_61_paragraph_1_point_c",
        "metadata_type": "article"
    },
    {
        "question": "What specific information must be provided to subjects when obtaining their freely-given informed consent for testing an AI system in real-world conditions concerning the AI system's output?",
        "ground_truth_answer": "Subjects must be duly informed with concise, clear, relevant, and understandable information regarding the arrangements for requesting the reversal or the disregarding of the predictions, recommendations or decisions of the AI system.",
        "context_id": "article_61_paragraph_1_point_d",
        "metadata_type": "article"
    },
    {
        "question": "What specific action must Member States undertake regarding AI regulatory sandboxes for SMEs and start-ups with a registered office or branch in the Union?",
        "ground_truth_answer": "Member States must provide SMEs, including start-ups, having a registered office or a branch in the Union, with priority access to the AI regulatory sandboxes, to the extent that they fulfil the eligibility conditions and selection criteria. This priority access shall not preclude other SMEs, including start-ups, from accessing the sandboxes, provided they also fulfil the eligibility conditions and selection criteria.",
        "context_id": "article_62_paragraph_1_point_a",
        "metadata_type": "article"
    },
    {
        "question": "What considerations are required when setting conformity assessment fees for SME providers, including start-ups, under Article 43?",
        "ground_truth_answer": "When setting conformity assessment fees under Article 43, the specific interests and needs of SME providers, including start-ups, shall be taken into account, and those fees shall be reduced proportionately to their size, market size, and other relevant indicators.",
        "context_id": "article_62_paragraph_2",
        "metadata_type": "article"
    },
    {
        "question": "Which entities may comply with certain elements of the quality management system in a simplified manner according to Article 63(1) of the EU AI Act, and under what condition?",
        "ground_truth_answer": "Microenterprises, as defined by Recommendation 2003/361/EC, may comply with certain elements of the quality management system required by Article 17 in a simplified manner, provided that they do not have partner enterprises or linked enterprises within the meaning of that Recommendation.",
        "context_id": "article_63_paragraph_1",
        "metadata_type": "article"
    },
    {
        "question": "What does Article 63(2) clarify regarding the interpretation of exemptions under Article 63(1) for operators?",
        "ground_truth_answer": "Article 63(2) clarifies that Article 63(1) shall not be interpreted as exempting operators from fulfilling any other requirements or obligations laid down in the Regulation, including those established in Articles 9, 10, 11, 12, 13, 14, 15, 72, and 73.",
        "context_id": "article_63_paragraph_2",
        "metadata_type": "article"
    },
    {
        "question": "How is the Board, as detailed in Article 65, paragraph 2, composed, and who attends its meetings?",
        "ground_truth_answer": "The Board shall be composed of one representative per Member State. The European Data Protection Supervisor shall participate as observer, and the AI Office shall also attend the Board’s meetings without taking part in the votes. Additionally, other national and Union authorities, bodies or experts may be invited to the meetings by the Board on a case by case basis, where the issues discussed are of relevance for them.",
        "context_id": "article_65_paragraph_2",
        "metadata_type": "article"
    },
    {
        "question": "What specific empowerments must Member States ensure their representatives on the Board possess regarding the implementation of the EU AI Act?",
        "ground_truth_answer": "Member States must ensure their representatives on the Board are empowered to facilitate consistency and coordination between national competent authorities in their Member State regarding the implementation of this Regulation, including through the collection of relevant data and information for the purpose of fulfilling their tasks on the Board.",
        "context_id": "article_65_paragraph_4_point_c",
        "metadata_type": "article"
    },
    {
        "question": "By what majority do the designated representatives of the Member States adopt the Board's rules of procedure?",
        "ground_truth_answer": "The designated representatives of the Member States shall adopt the Board’s rules of procedure by a two-thirds majority.",
        "context_id": "article_65_paragraph_5",
        "metadata_type": "article"
    },
    {
        "question": "What is the specific function of the standing sub-group for market surveillance established by the Board?",
        "ground_truth_answer": "The standing sub-group for market surveillance established by the Board should act as the administrative cooperation group (ADCO) for this Regulation within the meaning of Article 30 of Regulation (EU) 2019/1020.",
        "context_id": "article_65_paragraph_6",
        "metadata_type": "article"
    },
    {
        "question": "What specific actions may the Board undertake to facilitate the consistent and effective application of this Regulation?",
        "ground_truth_answer": "The Board may contribute to the coordination among national competent authorities responsible for the application of this Regulation and, in cooperation with and subject to the agreement of the market surveillance authorities concerned, support joint activities of market surveillance authorities referred to in Article 74(11).",
        "context_id": "article_66_paragraph_1_point_a",
        "metadata_type": "article"
    },
    {
        "question": "What specific areas can the Board contribute to the harmonisation of administrative practices in the Member States?",
        "ground_truth_answer": "The Board may contribute to the harmonisation of administrative practices in the Member States, including in relation to the derogation from the conformity assessment procedures referred to in Article 46, the functioning of AI regulatory sandboxes, and testing in real world conditions referred to in Articles 57, 59 and 60.",
        "context_id": "article_66_paragraph_1_point_d",
        "metadata_type": "article"
    },
    {
        "question": "What specific matters can the AI Board issue recommendations and written opinions on, at the request of the Commission or on its own initiative, regarding the implementation and consistent application of the EU AI Act?",
        "ground_truth_answer": "The AI Board may issue recommendations and written opinions on the evaluation and review of the Regulation pursuant to Article 112 (including as regards the serious incident reports referred to in Article 73), the functioning of the EU database referred to in Article 71, the preparation of the delegated or implementing acts, and possible alignments of this Regulation with the Union harmonisation legislation listed in Annex I.",
        "context_id": "article_66_paragraph_1_point_e_point_ii",
        "metadata_type": "article"
    },
    {
        "question": "What specific types of matters can the Board issue recommendations and written opinions on, regarding the requirements set out in Chapter III, Section 2 of the Regulation?",
        "ground_truth_answer": "The Board may issue recommendations and written opinions on technical specifications or existing standards regarding the requirements set out in Chapter III, Section 2, among other relevant matters related to the implementation and consistent and effective application of the Regulation.",
        "context_id": "article_66_paragraph_1_point_e_point_iii",
        "metadata_type": "article"
    },
    {
        "question": "What specific actions can the Board take, either at the request of the Commission or on its own initiative, concerning the consistent and effective application of the EU AI Act, particularly regarding the use of harmonised standards or common specifications?",
        "ground_truth_answer": "The Board may issue recommendations and written opinions on any relevant matters related to the implementation of this Regulation and to its consistent and effective application, including on the use of harmonised standards or common specifications referred to in Articles 40 and 41.",
        "context_id": "article_66_paragraph_1_point_e_point_iv",
        "metadata_type": "article"
    },
    {
        "question": "What specific aspect related to the evolving typology of AI value chains can the Board issue recommendations and written opinions on?",
        "ground_truth_answer": "The Board can issue recommendations and written opinions on trends concerning the evolving typology of AI value chains, particularly regarding the resulting implications in terms of accountability.",
        "context_id": "article_66_paragraph_1_point_e_point_vi",
        "metadata_type": "article"
    },
    {
        "question": "Regarding the consistent and effective application of the EU AI Act, what specific matters related to amendments or revisions can the Board issue recommendations and written opinions on?",
        "ground_truth_answer": "The Board may issue recommendations and written opinions on the potential need for amendment to Annex III in accordance with Article 7, and on the potential need for possible revision of Article 5 pursuant to Article 112, taking into account relevant available evidence and the latest developments in technology.",
        "context_id": "article_66_paragraph_1_point_e_point_vii",
        "metadata_type": "article"
    },
    {
        "question": "What specific actions can the Board take to facilitate a shared understanding among market operators and competent authorities of the relevant concepts provided for in this Regulation?",
        "ground_truth_answer": "The Board may facilitate the development of common criteria and a shared understanding among market operators and competent authorities of the relevant concepts provided for in this Regulation, including by contributing to the development of benchmarks.",
        "context_id": "article_66_paragraph_1_point_g",
        "metadata_type": "article"
    },
    {
        "question": "In which specific fields may the Board cooperate with other Union institutions, bodies, offices, agencies, and relevant Union expert groups and networks to facilitate the consistent and effective application of the Regulation?",
        "ground_truth_answer": "The Board may cooperate in the fields of product safety, cybersecurity, competition, digital and media services, financial services, consumer protection, data and fundamental rights protection.",
        "context_id": "article_66_paragraph_1_point_h",
        "metadata_type": "article"
    },
    {
        "question": "What specific actions may the Board take to assist in developing the necessary expertise for the implementation of this Regulation?",
        "ground_truth_answer": "The Board may assist national competent authorities and the Commission in developing the organisational and technical expertise required for the implementation of this Regulation, which includes contributing to the assessment of training needs for staff of Member States involved in implementing this Regulation.",
        "context_id": "article_66_paragraph_1_point_j",
        "metadata_type": "article"
    },
    {
        "question": "What specific actions can the Board undertake to assist the AI Office concerning AI regulatory sandboxes?",
        "ground_truth_answer": "The Board may assist the AI Office in supporting national competent authorities in the establishment and development of AI regulatory sandboxes, and facilitate cooperation and information-sharing among AI regulatory sandboxes.",
        "context_id": "article_66_paragraph_1_point_k",
        "metadata_type": "article"
    },
    {
        "question": "What specific types of opinions may the Board receive from Member States to facilitate the consistent and effective application of the Regulation?",
        "ground_truth_answer": "The Board may receive opinions from Member States on qualified alerts regarding general-purpose AI models. Additionally, it may receive opinions on national experiences and practices concerning the monitoring and enforcement of AI systems, particularly those integrating general-purpose AI models.",
        "context_id": "article_66_paragraph_1_point_o",
        "metadata_type": "article"
    },
    {
        "question": "What balancing considerations must be applied to the membership of the advisory forum under the EU AI Act?",
        "ground_truth_answer": "The membership of the advisory forum must represent a balanced selection of stakeholders, including industry, start-ups, SMEs, civil society, and academia. It must also be balanced with regard to commercial and non-commercial interests, and within the commercial interests category, with regard to SMEs and other undertakings.",
        "context_id": "article_67_paragraph_2",
        "metadata_type": "article"
    },
    {
        "question": "Which entities are permanent members of the advisory forum?",
        "ground_truth_answer": "The Fundamental Rights Agency, ENISA, the European Committee for Standardization (CEN), the European Committee for Electrotechnical Standardization (CENELEC), and the European Telecommunications Standards Institute (ETSI) shall be permanent members of the advisory forum.",
        "context_id": "article_67_paragraph_5",
        "metadata_type": "article"
    },
    {
        "question": "What is the Commission required to do regarding the establishment of a scientific panel of independent experts, and how will the implementing act for this be adopted?",
        "ground_truth_answer": "The Commission is required to make provisions, by means of an implementing act, on the establishment of a scientific panel of independent experts (the ‘scientific panel’) intended to support the enforcement activities under this Regulation. That implementing act shall be adopted in accordance with the examination procedure referred to in Article 98(2).",
        "context_id": "article_68_paragraph_1",
        "metadata_type": "article"
    },
    {
        "question": "What specific task does the scientific panel undertake to support the implementation and enforcement of the EU AI Act concerning general-purpose AI models and systems?",
        "ground_truth_answer": "The scientific panel supports the implementation and enforcement of the EU AI Act concerning general-purpose AI models and systems by alerting the AI Office of possible systemic risks at Union level of general-purpose AI models, in accordance with Article 90.",
        "context_id": "article_68_paragraph_3_point_a_point_i",
        "metadata_type": "article"
    },
    {
        "question": "What specific task related to general-purpose AI models with systemic risk does the scientific panel perform to support the implementation and enforcement of the EU AI Act?",
        "ground_truth_answer": "The scientific panel provides advice on the classification of general-purpose AI models with systemic risk.",
        "context_id": "article_68_paragraph_3_point_a_point_iii",
        "metadata_type": "article"
    },
    {
        "question": "What is the responsibility of the AI Office regarding potential conflicts of interest concerning the experts on the scientific panel?",
        "ground_truth_answer": "The AI Office shall establish systems and procedures to actively manage and prevent potential conflicts of interest for the experts on the scientific panel.",
        "context_id": "article_68_paragraph_4",
        "metadata_type": "article"
    },
    {
        "question": "What provisions must be included in the implementing act referenced in Article 68, paragraph 1, concerning the scientific panel and its members?",
        "ground_truth_answer": "The implementing act must include provisions on the conditions, procedures, and detailed arrangements for the scientific panel and its members to issue alerts, and to request the assistance of the AI Office for the performance of the scientific panel's tasks.",
        "context_id": "article_68_paragraph_5",
        "metadata_type": "article"
    },
    {
        "question": "Where will the structure and the level of fees for expert advice and support, and the scale and structure of recoverable costs, be specified?",
        "ground_truth_answer": "The structure and the level of fees, as well as the scale and structure of recoverable costs, shall be set out in the implementing act referred to in Article 68(1).",
        "context_id": "article_69_paragraph_2",
        "metadata_type": "article"
    },
    {
        "question": "How must the national competent authorities established or designated by each Member State exercise their powers for the purposes of this Regulation?",
        "ground_truth_answer": "The national competent authorities must exercise their powers independently, impartially, and without bias so as to safeguard the objectivity of their activities and tasks, and to ensure the application and implementation of this Regulation.",
        "context_id": "article_70_paragraph_1",
        "metadata_type": "article"
    },
    {
        "question": "By what date must Member States make publicly available information on how competent authorities and single points of contact can be contacted?",
        "ground_truth_answer": "Member States must make publicly available information on how competent authorities and single points of contact can be contacted, through electronic communication means, by 2 August 2025.",
        "context_id": "article_70_paragraph_2",
        "metadata_type": "article"
    },
    {
        "question": "What specific competences and expertise are required for the personnel of national competent authorities under the EU AI Act?",
        "ground_truth_answer": "The personnel of national competent authorities must have an in-depth understanding of AI technologies, data and data computing, personal data protection, cybersecurity, fundamental rights, health and safety risks, and knowledge of existing standards and legal requirements.",
        "context_id": "article_70_paragraph_3",
        "metadata_type": "article"
    },
    {
        "question": "What information are Member States required to report to the Commission by 2 August 2025 and once every two years thereafter?",
        "ground_truth_answer": "Member States are required to report to the Commission on the status of the financial and human resources of the national competent authorities, including an assessment of their adequacy.",
        "context_id": "article_70_paragraph_6",
        "metadata_type": "article"
    },
    {
        "question": "Which entity acts as the competent authority for the supervision of Union institutions, bodies, offices, or agencies when they fall within the scope of this Regulation?",
        "ground_truth_answer": "The European Data Protection Supervisor shall act as the competent authority for their supervision.",
        "context_id": "article_70_paragraph_9",
        "metadata_type": "article"
    },
    {
        "question": "What categories of AI systems are required to have their information included in the EU database established and maintained by the Commission in collaboration with the Member States?",
        "ground_truth_answer": "The EU database must contain information concerning high-risk AI systems referred to in Article 6(2) which are registered in accordance with Articles 49 and 60, and AI systems that are not considered as high-risk pursuant to Article 6(3) and which are registered in accordance with Article 6(4) and Article 49.",
        "context_id": "article_71_paragraph_1",
        "metadata_type": "article"
    },
    {
        "question": "What are the accessibility requirements for information registered in the EU database in accordance with Article 49, and how do these requirements differ for information registered in accordance with Article 60?",
        "ground_truth_answer": "With the exception of the section referred to in Article 49(4) and Article 60(4), point (c), information contained in the EU database registered in accordance with Article 49 shall be accessible and publicly available in a user-friendly, easily navigable, and machine-readable manner. In contrast, information registered in accordance with Article 60 shall be accessible only to market surveillance authorities and the Commission, unless the prospective provider or provider has given consent for also making the information accessible to the public.",
        "context_id": "article_71_paragraph_4",
        "metadata_type": "article"
    },
    {
        "question": "What specific personal data must the EU database contain, and for what purpose?",
        "ground_truth_answer": "The EU database must contain the names and contact details of natural persons who are responsible for registering the system and have the legal authority to represent the provider or the deployer, as applicable. This personal data is included only in so far as necessary for collecting and processing information in accordance with this Regulation.",
        "context_id": "article_71_paragraph_5",
        "metadata_type": "article"
    },
    {
        "question": "What is the obligation of providers concerning post-market monitoring systems according to Article 72(1)?",
        "ground_truth_answer": "Providers shall establish and document a post-market monitoring system in a manner that is proportionate to the nature of the AI technologies and the risks of the high-risk AI system.",
        "context_id": "article_72_paragraph_1",
        "metadata_type": "article"
    },
    {
        "question": "What are the operational requirements for a post-market monitoring system concerning high-risk AI systems under the EU AI Act?",
        "ground_truth_answer": "The post-market monitoring system must actively and systematically collect, document, and analyse relevant data on the performance of high-risk AI systems throughout their lifetime. This data, which may come from deployers or other sources, is used to allow the provider to evaluate the continuous compliance of AI systems with the requirements set out in Chapter III, Section 2. Where relevant, post-market monitoring must also include an analysis of the interaction with other AI systems. However, this obligation does not extend to sensitive operational data of deployers which are law-enforcement authorities.",
        "context_id": "article_72_paragraph_2",
        "metadata_type": "article"
    },
    {
        "question": "What is the deadline for the Commission to adopt an implementing act regarding the post-market monitoring plan, and what will that act establish?",
        "ground_truth_answer": "The Commission shall adopt an implementing act by 2 February 2026. This act will lay down detailed provisions establishing a template for the post-market monitoring plan and the list of elements to be included in the plan.",
        "context_id": "article_72_paragraph_3",
        "metadata_type": "article"
    },
    {
        "question": "What choice do providers of high-risk AI systems covered by Union harmonisation legislation (listed in Section A of Annex I) have regarding post-market monitoring systems and plans if such systems and plans are already established under that legislation?",
        "ground_truth_answer": "Providers shall have a choice of integrating the necessary elements described in paragraphs 1, 2 and 3 using the template referred to in paragraph 3 into systems and plans already existing under that legislation, provided that it achieves an equivalent level of protection. This choice is provided to ensure consistency, avoid duplications, and minimise additional burdens.",
        "context_id": "article_72_paragraph_4",
        "metadata_type": "article"
    },
    {
        "question": "What is the reporting obligation for providers of high-risk AI systems concerning serious incidents?",
        "ground_truth_answer": "Providers of high-risk AI systems placed on the Union market shall report any serious incident to the market surveillance authorities of the Member States where that incident occurred.",
        "context_id": "article_73_paragraph_1",
        "metadata_type": "article"
    },
    {
        "question": "What is the deadline for submitting a report after a provider or deployer becomes aware of a serious incident involving an AI system?",
        "ground_truth_answer": "The report must be made immediately after the provider establishes a causal link or reasonable likelihood of such a link between the AI system and the serious incident, and in any event, not later than 15 days after the provider or, where applicable, the deployer, becomes aware of the serious incident. The severity of the incident should also be taken into account when determining the reporting period.",
        "context_id": "article_73_paragraph_2",
        "metadata_type": "article"
    },
    {
        "question": "What is the deadline for providing the report in the event of a widespread infringement or a serious incident?",
        "ground_truth_answer": "In the event of a widespread infringement or a serious incident as defined in Article 3, point (49)(b), the report shall be provided immediately, and not later than two days after the provider or, where applicable, the deployer becomes aware of that incident.",
        "context_id": "article_73_paragraph_3",
        "metadata_type": "article"
    },
    {
        "question": "What is the reporting deadline for a serious incident involving the death of a person that is causally linked to a high-risk AI system?",
        "ground_truth_answer": "In the event of a death causally linked to a high-risk AI system, the report must be provided immediately after the provider or deployer establishes or suspects a causal relationship, but not later than 10 days after the date on which they become aware of the serious incident.",
        "context_id": "article_73_paragraph_4",
        "metadata_type": "article"
    },
    {
        "question": "What are the provider's obligations regarding investigations and cooperation after reporting a serious incident concerning an AI system?",
        "ground_truth_answer": "Following the reporting of a serious incident, the provider must, without delay, perform the necessary investigations, which include a risk assessment of the incident and corrective action. The provider shall cooperate with the competent authorities, and where relevant with the notified body concerned, during these investigations. Furthermore, the provider shall not alter the AI system in a way that may affect any subsequent evaluation of the incident's causes, prior to informing the competent authorities of such action.",
        "context_id": "article_73_paragraph_6",
        "metadata_type": "article"
    },
    {
        "question": "To whom must serious incidents be reported for high-risk AI systems that are safety components of devices or devices themselves covered by Regulations (EU) 2017/745 and (EU) 2017/746?",
        "ground_truth_answer": "For such high-risk AI systems, serious incidents must be reported to the national competent authority chosen for that purpose by the Member States where the incident occurred, limited to those referred to in Article 3, point (49)(c) of this Regulation.",
        "context_id": "article_73_paragraph_10",
        "metadata_type": "article"
    },
    {
        "question": "What are the annual reporting obligations of market surveillance authorities to the Commission and national competition authorities, as specified in Article 74(2)?",
        "ground_truth_answer": "As part of their reporting obligations, market surveillance authorities shall annually report to the Commission and relevant national competition authorities any information identified in the course of market surveillance activities that may be of potential interest for the application of Union law on competition rules. They shall also annually report to the Commission about the use of prohibited practices that occurred during that year and about the measures taken.",
        "context_id": "article_74_paragraph_2",
        "metadata_type": "article"
    },
    {
        "question": "Under what conditions may a Member State designate an alternative market surveillance authority for high-risk AI systems related to products covered by the Union harmonisation legislation listed in Section A of Annex I?",
        "ground_truth_answer": "By derogation from the general rule, Member States may designate another relevant authority to act as a market surveillance authority in appropriate circumstances, provided they ensure coordination with the relevant sectoral market surveillance authorities responsible for the enforcement of the Union harmonisation legislation listed in Annex I.",
        "context_id": "article_74_paragraph_3",
        "metadata_type": "article"
    },
    {
        "question": "For high-risk AI systems used by financial institutions regulated by Union financial services law, who serves as the market surveillance authority under this Regulation, and under what specific condition?",
        "ground_truth_answer": "For high-risk AI systems placed on the market, put into service, or used by financial institutions regulated by Union financial services law, the market surveillance authority is the relevant national authority responsible for the financial supervision of those institutions under that legislation, provided that the placing on the market, putting into service, or the use of the AI system is in direct connection with the provision of those financial services.",
        "context_id": "article_74_paragraph_6",
        "metadata_type": "article"
    },
    {
        "question": "What information are national market surveillance authorities supervising regulated credit institutions, which are participating in the Single Supervisory Mechanism, required to report to the European Central Bank?",
        "ground_truth_answer": "National market surveillance authorities supervising regulated credit institutions, which are participating in the Single Supervisory Mechanism established by Regulation (EU) No 1024/2013, should report, without delay, to the European Central Bank any information identified in the course of their market surveillance activities that may be of potential interest for the prudential supervisory tasks of the European Central Bank.",
        "context_id": "article_74_paragraph_7",
        "metadata_type": "article"
    },
    {
        "question": "For which specific high-risk AI systems must Member States designate market surveillance authorities, and what types of entities can be designated for this role, while respecting which crucial limitation?",
        "ground_truth_answer": "Member States must designate market surveillance authorities for high-risk AI systems listed in point 1 of Annex III, in so far as they are used for law enforcement purposes, border management, and justice and democracy, and for high-risk AI systems listed in points 6, 7 and 8 of Annex III. They can designate either the competent data protection supervisory authorities under Regulation (EU) 2016/679 or Directive (EU) 2016/680, or any other authority designated pursuant to the same conditions laid down in Articles 41 to 44 of Directive (EU) 2016/680. A crucial limitation is that market surveillance activities shall in no way affect the independence of judicial authorities, or otherwise interfere with their activities when acting in their judicial capacity.",
        "context_id": "article_74_paragraph_8",
        "metadata_type": "article"
    },
    {
        "question": "What is the purpose of joint activities, including joint investigations, that market surveillance authorities and the Commission may propose under Article 74, paragraph 11, specifically concerning high-risk AI systems presenting a serious risk across multiple Member States?",
        "ground_truth_answer": "The purpose of these joint activities is to promote compliance, identify non-compliance, raise awareness, or provide guidance in relation to the Regulation with respect to specific categories of high-risk AI systems that are found to present a serious risk across two or more Member States.",
        "context_id": "article_74_paragraph_11",
        "metadata_type": "article"
    },
    {
        "question": "What type of access must providers grant to market surveillance authorities concerning documentation and data for high-risk AI systems, and through what potential technical means?",
        "ground_truth_answer": "Providers must grant market surveillance authorities full access to the documentation, as well as the training, validation, and testing data sets used for the development of high-risk AI systems, where relevant and limited to what is necessary to fulfil their tasks. This access may be provided, where appropriate and subject to security safeguards, through application programming interfaces (API) or other relevant technical means and tools enabling remote access.",
        "context_id": "article_74_paragraph_12",
        "metadata_type": "article"
    },
    {
        "question": "What conditions must be fulfilled for market surveillance authorities to be granted access to the source code of a high-risk AI system?",
        "ground_truth_answer": "Market surveillance authorities shall be granted access to the source code of a high-risk AI system upon a reasoned request and only when access to the source code is necessary to assess the conformity of that system with the requirements set out in Chapter III, Section 2.",
        "context_id": "article_74_paragraph_13_point_a",
        "metadata_type": "article"
    },
    {
        "question": "Under what specific conditions can market surveillance authorities be granted access to the source code of a high-risk AI system?",
        "ground_truth_answer": "Market surveillance authorities can be granted access to the source code of a high-risk AI system upon a reasoned request, and only when testing or auditing procedures and verifications based on the data and documentation provided by the provider have been exhausted or proved insufficient.",
        "context_id": "article_74_paragraph_13_point_b",
        "metadata_type": "article"
    },
    {
        "question": "Under what specific conditions does the AI Office have powers to monitor and supervise an AI system, and what extent of powers does it possess for these tasks?",
        "ground_truth_answer": "The AI Office has powers to monitor and supervise compliance of an AI system when that AI system is based on a general-purpose AI model, and both the model and the system are developed by the same provider. To fulfill these monitoring and supervision tasks, the AI Office shall have all the powers of a market surveillance authority as provided for in its relevant Section and Regulation (EU) 2019/1020.",
        "context_id": "article_75_paragraph_1",
        "metadata_type": "article"
    },
    {
        "question": "What actions must relevant market surveillance authorities take if they have sufficient reason to consider general-purpose AI systems, usable directly by deployers for high-risk purposes, to be non-compliant with the EU AI Act's requirements?",
        "ground_truth_answer": "They shall cooperate with the AI Office to carry out compliance evaluations, and shall inform the Board and other market surveillance authorities accordingly.",
        "context_id": "article_75_paragraph_2",
        "metadata_type": "article"
    },
    {
        "question": "What is the procedure for a market surveillance authority to obtain information from the AI Office if it is unable to conclude an investigation of a high-risk AI system due to lack of access to general-purpose AI model information?",
        "ground_truth_answer": "If a market surveillance authority is unable to conclude its investigation of a high-risk AI system because it cannot access certain information related to a general-purpose AI model despite having made all appropriate efforts, it may submit a reasoned request to the AI Office. The AI Office shall then supply the applicant authority with the relevant information without delay, and in any event within 30 days, while the market surveillance authorities must safeguard the confidentiality of this information.",
        "context_id": "article_75_paragraph_3",
        "metadata_type": "article"
    },
    {
        "question": "What is the responsibility of market surveillance authorities when AI systems supervised within an AI regulatory sandbox undergo testing in real world conditions?",
        "ground_truth_answer": "Market surveillance authorities shall verify compliance with Article 60 as part of their supervisory role for the AI regulatory sandbox.",
        "context_id": "article_76_paragraph_2",
        "metadata_type": "article"
    },
    {
        "question": "What powers are granted to national public authorities or bodies that supervise fundamental rights concerning documentation related to high-risk AI systems under the EU AI Act?",
        "ground_truth_answer": "National public authorities or bodies which supervise or enforce fundamental rights, including the right to non-discrimination, in relation to high-risk AI systems referred to in Annex III, have the power to request and access any documentation created or maintained under this Regulation. This documentation must be in accessible language and format, and access is permitted when necessary for effectively fulfilling their mandates within the limits of their jurisdiction. The relevant public authority or body must also inform the market surveillance authority of the Member State concerned of any such request.",
        "context_id": "article_77_paragraph_1",
        "metadata_type": "article"
    },
    {
        "question": "By what date must Member States identify and publish a list of the public authorities or bodies referred to in paragraph 1, and what further actions are required regarding this list?",
        "ground_truth_answer": "By 2 November 2024, each Member State must identify the public authorities or bodies referred to in paragraph 1 and make a list of them publicly available. Member States must also notify this list to the Commission and to the other Member States, and keep the list up to date.",
        "context_id": "article_77_paragraph_2",
        "metadata_type": "article"
    },
    {
        "question": "Under what conditions can a public authority or body request the market surveillance authority to organise testing of a high-risk AI system?",
        "ground_truth_answer": "A public authority or body may make a reasoned request to the market surveillance authority to organise testing of a high-risk AI system through technical means when the documentation referred to in paragraph 1 is insufficient to ascertain whether an infringement of obligations under Union law protecting fundamental rights has occurred.",
        "context_id": "article_77_paragraph_3",
        "metadata_type": "article"
    },
    {
        "question": "What specific types of information and data must be protected confidentially by the Commission, market surveillance authorities, notified bodies, and other persons involved in applying this Regulation?",
        "ground_truth_answer": "Those involved in applying this Regulation must protect the intellectual property rights and confidential business information or trade secrets of a natural or legal person, including source code, except in cases referred to in Article 5 of Directive (EU) 2016/943.",
        "context_id": "article_78_paragraph_1_point_a",
        "metadata_type": "article"
    },
    {
        "question": "What specific obligations do authorities have regarding the data they request for assessing AI system risks and exercising their powers under this Regulation?",
        "ground_truth_answer": "Authorities involved in the application of this Regulation must request only data that is strictly necessary for the assessment of the risk posed by AI systems and for the exercise of their powers. Additionally, they must implement adequate and effective cybersecurity measures to protect the security and confidentiality of the obtained information and data, and delete the collected data as soon as it is no longer needed for its original purpose, in accordance with applicable Union or national law.",
        "context_id": "article_78_paragraph_2",
        "metadata_type": "article"
    },
    {
        "question": "What is the specific obligation regarding the location of technical documentation when law enforcement, immigration, or asylum authorities act as providers of high-risk AI systems referred to in point 1, 6 or 7 of Annex III?",
        "ground_truth_answer": "When law enforcement, immigration or asylum authorities are providers of high-risk AI systems referred to in point 1, 6 or 7 of Annex III, the technical documentation referred to in Annex IV shall remain within the premises of those authorities.",
        "context_id": "article_78_paragraph_3",
        "metadata_type": "article"
    },
    {
        "question": "Under what conditions may the Commission and Member States exchange confidential information with regulatory authorities of third countries?",
        "ground_truth_answer": "The Commission and Member States may exchange confidential information with regulatory authorities of third countries where necessary, in accordance with relevant provisions of international and trade agreements, and with third countries with which they have concluded bilateral or multilateral confidentiality arrangements guaranteeing an adequate level of confidentiality.",
        "context_id": "article_78_paragraph_5",
        "metadata_type": "article"
    },
    {
        "question": "How are AI systems that present a risk to be understood according to Article 79, paragraph 1?",
        "ground_truth_answer": "AI systems presenting a risk shall be understood as a ‘product presenting a risk’ as defined in Article 3, point 19 of Regulation (EU) 2019/1020, provided they present risks to the health or safety, or to fundamental rights, of persons.",
        "context_id": "article_79_paragraph_1",
        "metadata_type": "article"
    },
    {
        "question": "What actions can a market surveillance authority require from an operator if an AI system is found to be non-compliant with the requirements and obligations of the EU AI Act?",
        "ground_truth_answer": "Upon finding non-compliance, the market surveillance authority can require the relevant operator to take all appropriate corrective actions to bring the AI system into compliance, to withdraw the AI system from the market, or to recall it. These actions must be completed within a period the market surveillance authority may prescribe, and in any event within the shorter of 15 working days, or as provided for in the relevant Union harmonisation legislation.",
        "context_id": "article_79_paragraph_2_part_2",
        "metadata_type": "article"
    },
    {
        "question": "What actions must a market surveillance authority take if an AI system operator fails to implement adequate corrective measures within the specified timeframe?",
        "ground_truth_answer": "If an AI system operator does not take adequate corrective action within the referred period, the market surveillance authority shall take all appropriate provisional measures to prohibit or restrict the AI system’s being made available on its national market or put into service, to withdraw the product or the standalone AI system from that market, or to recall it. The authority shall also without undue delay notify the Commission and the other Member States of these measures.",
        "context_id": "article_79_paragraph_5",
        "metadata_type": "article"
    },
    {
        "question": "What information must be included in the notification concerning non-compliant AI systems, as specified in Article 79, paragraph 6?",
        "ground_truth_answer": "The notification must include all available details, such as information necessary for the identification of the non-compliant AI system, its origin and supply chain, the nature of the non-compliance alleged and the risk involved, the nature and duration of the national measures taken, and the arguments put forward by the relevant operator.",
        "context_id": "article_79_paragraph_6",
        "metadata_type": "article"
    },
    {
        "question": "Under what conditions is a provisional measure taken by a market surveillance authority deemed justified, as specified in Article 79, paragraph 8?",
        "ground_truth_answer": "A provisional measure taken by a market surveillance authority shall be deemed justified if, within three months of receipt of the notification, no objection has been raised by either a market surveillance authority of a Member State or by the Commission.",
        "context_id": "article_79_paragraph_8",
        "metadata_type": "article"
    },
    {
        "question": "What action must a market surveillance authority take if it has sufficient reason to believe an AI system, classified by its provider as non-high-risk under Article 6(3), is in fact high-risk?",
        "ground_truth_answer": "If a market surveillance authority has sufficient reason to consider that an AI system classified by the provider as non-high-risk pursuant to Article 6(3) is indeed high-risk, the authority shall carry out an evaluation of the AI system concerned. This evaluation is performed to assess its classification as a high-risk AI system, based on the conditions set out in Article 6(3) and the Commission guidelines.",
        "context_id": "article_80_paragraph_1",
        "metadata_type": "article"
    },
    {
        "question": "What happens if a provider fails to bring an AI system into compliance with the requirements and obligations of the Regulation within the period specified in Article 80, paragraph 2?",
        "ground_truth_answer": "If a provider of an AI system does not bring the AI system into compliance with the requirements and obligations laid down in this Regulation within the period referred to in paragraph 2 of Article 80, the provider shall be subject to fines in accordance with Article 99.",
        "context_id": "article_80_paragraph_4",
        "metadata_type": "article"
    },
    {
        "question": "Which provisions apply if the provider of an AI system does not take adequate corrective action within the period referred to in Article 80(2)?",
        "ground_truth_answer": "If the provider of the AI system concerned does not take adequate corrective action within the period referred to in paragraph 2 of Article 80, then Article 79(5) to (9) shall apply.",
        "context_id": "article_80_paragraph_6",
        "metadata_type": "article"
    },
    {
        "question": "What penalty applies to an AI system provider if a market surveillance authority determines that the provider misclassified an AI system as non-high-risk to circumvent the requirements in Chapter III, Section 2?",
        "ground_truth_answer": "If a market surveillance authority establishes that an AI system was misclassified by the provider as non-high-risk in order to circumvent the application of requirements in Chapter III, Section 2, the provider shall be subject to fines in accordance with Article 99.",
        "context_id": "article_80_paragraph_7",
        "metadata_type": "article"
    },
    {
        "question": "What actions and timelines must the Commission follow when a market surveillance authority of a Member State objects to a measure taken by another market surveillance authority, or when the Commission considers a national measure to be contrary to Union law?",
        "ground_truth_answer": "The Commission shall without undue delay enter into consultation with the market surveillance authority of the relevant Member State and the operator or operators, and shall evaluate the national measure. Based on this evaluation, the Commission shall decide whether the national measure is justified within six months (or within 60 days in the case of non-compliance with the prohibition of the AI practices referred to in Article 5), starting from the notification referred to in Article 79(5). The Commission must then notify its decision to the market surveillance authority of the Member State concerned and inform all other market surveillance authorities.",
        "context_id": "article_81_paragraph_1",
        "metadata_type": "article"
    },
    {
        "question": "What actions are required from Member States if the Commission considers a measure taken by a relevant Member State concerning an AI system to be justified?",
        "ground_truth_answer": "If the Commission considers the measure taken by the relevant Member State to be justified, all Member States shall ensure that they take appropriate restrictive measures in respect of the AI system concerned, such as requiring its withdrawal from their market without undue delay, and shall inform the Commission accordingly.",
        "context_id": "article_81_paragraph_2",
        "metadata_type": "article"
    },
    {
        "question": "What action must a Member State's market surveillance authority take if, after evaluation and consultation, it finds that a high-risk AI system complies with the Regulation but still presents a risk to the health or safety of persons, fundamental rights, or other aspects of public interest protection?",
        "ground_truth_answer": "The market surveillance authority must require the relevant operator to take all appropriate measures to ensure that the AI system concerned, when placed on the market or put into service, no longer presents that risk without undue delay, within a period it may prescribe.",
        "context_id": "article_82_paragraph_1",
        "metadata_type": "article"
    },
    {
        "question": "What specific details must Member States include when informing the Commission and other Member States of a finding under paragraph 1?",
        "ground_truth_answer": "Member States must include all available details, specifically the data necessary for the identification of the AI system concerned, the origin and the supply chain of the AI system, the nature of the risk involved, and the nature and duration of the national measures taken.",
        "context_id": "article_82_paragraph_3",
        "metadata_type": "article"
    },
    {
        "question": "What measures must a market surveillance authority take if non-compliance with a high-risk AI system persists?",
        "ground_truth_answer": "If non-compliance persists, the market surveillance authority of the Member State concerned shall take appropriate and proportionate measures to restrict or prohibit the high-risk AI system being made available on the market or to ensure that it is recalled or withdrawn from the market without delay.",
        "context_id": "article_83_paragraph_2",
        "metadata_type": "article"
    },
    {
        "question": "Who is entitled to submit complaints regarding infringements of the EU AI Act, and how are these complaints to be managed by the market surveillance authorities?",
        "ground_truth_answer": "Any natural or legal person who has grounds to believe there has been an infringement of the provisions of the EU AI Act may submit complaints to the relevant market surveillance authority. These complaints must be taken into account for conducting market surveillance activities and handled according to the dedicated procedures established by the market surveillance authorities, without prejudice to other administrative or judicial remedies.",
        "context_id": "article_85_paragraph_1",
        "metadata_type": "article"
    },
    {
        "question": "Under what conditions does an affected person have the right to obtain explanations from a deployer regarding a decision made using a high-risk AI system?",
        "ground_truth_answer": "An affected person has this right if a decision taken by the deployer is based on the output from a high-risk AI system listed in Annex III (with the exception of systems listed under point 2 thereof), and the decision produces legal effects or similarly significantly affects that person in a way they consider to have an adverse impact on their health, safety, or fundamental rights.",
        "context_id": "article_86_paragraph_1",
        "metadata_type": "article"
    },
    {
        "question": "Who holds exclusive powers to supervise and enforce Chapter V, and to which entity does this body entrust the implementation of these tasks?",
        "ground_truth_answer": "The Commission holds exclusive powers to supervise and enforce Chapter V, and it entrusts the implementation of these tasks to the AI Office.",
        "context_id": "article_88_paragraph_1",
        "metadata_type": "article"
    },
    {
        "question": "What actions is the AI Office permitted to take to monitor compliance with the Regulation by providers of general-purpose AI models?",
        "ground_truth_answer": "For the purpose of carrying out its assigned tasks under this Section, the AI Office may take the necessary actions to monitor the effective implementation and compliance with this Regulation by providers of general-purpose AI models, including their adherence to approved codes of practice.",
        "context_id": "article_89_paragraph_1",
        "metadata_type": "article"
    },
    {
        "question": "What information must a downstream provider's complaint include when alleging an infringement of this Regulation?",
        "ground_truth_answer": "A downstream provider's complaint must be duly reasoned and indicate at least a description of the relevant facts, the provisions of this Regulation concerned, and the reason why the downstream provider considers that the provider of the general-purpose AI model concerned infringed this Regulation.",
        "context_id": "article_89_paragraph_2_point_b",
        "metadata_type": "article"
    },
    {
        "question": "Under what condition may the scientific panel issue a qualified alert to the AI Office, according to Article 90, paragraph 1, point b?",
        "ground_truth_answer": "The scientific panel may provide a qualified alert to the AI Office if it has reason to suspect that a general-purpose AI model meets the conditions referred to in Article 51.",
        "context_id": "article_90_paragraph_1_point_b",
        "metadata_type": "article"
    },
    {
        "question": "Who shall the AI Office inform regarding any measure taken according to Articles 91 to 94?",
        "ground_truth_answer": "The AI Office shall inform the Board of any measure according to Articles 91 to 94.",
        "context_id": "article_90_paragraph_2",
        "metadata_type": "article"
    },
    {
        "question": "What may the Commission request from a provider of a general-purpose AI model?",
        "ground_truth_answer": "The Commission may request the documentation drawn up by the provider in accordance with Articles 53 and 55, or any additional information that is necessary for assessing the provider's compliance with the Regulation.",
        "context_id": "article_91_paragraph_1",
        "metadata_type": "article"
    },
    {
        "question": "Under what conditions may the Commission issue a request for information to a provider of a general-purpose AI model?",
        "ground_truth_answer": "The Commission may issue a request for information to a provider of a general-purpose AI model upon a duly substantiated request from the scientific panel, provided that the access to information is necessary and proportionate for the fulfilment of the scientific panel's tasks under Article 68(2).",
        "context_id": "article_91_paragraph_3",
        "metadata_type": "article"
    },
    {
        "question": "What elements must be included in a request for information as per Article 91, paragraph 4 of the EU AI Act?",
        "ground_truth_answer": "A request for information shall state its legal basis and purpose, specify the information required, set a period for providing the information, and indicate the fines provided for in Article 101 for supplying incorrect, incomplete, or misleading information.",
        "context_id": "article_91_paragraph_4",
        "metadata_type": "article"
    },
    {
        "question": "If a lawyer supplies information on behalf of a client who is a general-purpose AI model provider, who remains fully responsible if the supplied information is incomplete, incorrect, or misleading?",
        "ground_truth_answer": "The clients shall remain fully responsible if the information supplied by their lawyers is incomplete, incorrect or misleading.",
        "context_id": "article_91_paragraph_5",
        "metadata_type": "article"
    },
    {
        "question": "Under what circumstances may the AI Office conduct evaluations of a general-purpose AI model?",
        "ground_truth_answer": "The AI Office, after consulting the Board, may conduct evaluations of a general-purpose AI model to assess the provider's compliance with obligations under this Regulation when the information gathered pursuant to Article 91 is insufficient.",
        "context_id": "article_92_paragraph_1_point_a",
        "metadata_type": "article"
    },
    {
        "question": "Under what conditions may the AI Office conduct evaluations of general-purpose AI models to investigate systemic risks at Union level?",
        "ground_truth_answer": "The AI Office may conduct evaluations of general-purpose AI models to investigate systemic risks at Union level, after consulting the Board, especially following a qualified alert from the scientific panel in accordance with Article 90(1), point (a).",
        "context_id": "article_92_paragraph_1_point_b",
        "metadata_type": "article"
    },
    {
        "question": "What criteria must independent experts appointed by the Commission to carry out evaluations meet?",
        "ground_truth_answer": "Independent experts appointed for this task must meet the criteria outlined in Article 68(2).",
        "context_id": "article_92_paragraph_2",
        "metadata_type": "article"
    },
    {
        "question": "How may the Commission request access to a general-purpose AI model for the purposes of Article 92, paragraph 1?",
        "ground_truth_answer": "The Commission may request access to the general-purpose AI model through APIs or further appropriate technical means and tools, including source code.",
        "context_id": "article_92_paragraph_3",
        "metadata_type": "article"
    },
    {
        "question": "What information must be included in a request for access according to Article 92, paragraph 4?",
        "ground_truth_answer": "A request for access shall state the legal basis, the purpose and reasons of the request, the period within which the access is to be provided, and the fines provided for in Article 101 for failure to provide access.",
        "context_id": "article_92_paragraph_4",
        "metadata_type": "article"
    },
    {
        "question": "Who is responsible for supplying requested information or providing access on behalf of a general-purpose AI model provider?",
        "ground_truth_answer": "The providers of the general-purpose AI model concerned or its representative shall supply the information requested. In cases where the provider is a legal person, company or firm, or lacks legal personality, the persons authorised to represent them by law or by their statutes shall provide the requested access on behalf of the provider.",
        "context_id": "article_92_paragraph_5",
        "metadata_type": "article"
    },
    {
        "question": "What specific details must be set out by the implementing acts that the Commission is required to adopt regarding evaluations?",
        "ground_truth_answer": "The implementing acts must set out the detailed arrangements and the conditions for the evaluations, including the detailed arrangements for involving independent experts, and the procedure for the selection thereof.",
        "context_id": "article_92_paragraph_6",
        "metadata_type": "article"
    },
    {
        "question": "What action may the AI Office take prior to requesting access to a general-purpose AI model, and for what purpose?",
        "ground_truth_answer": "Prior to requesting access to the general-purpose AI model, the AI Office may initiate a structured dialogue with the provider of the general-purpose AI model to gather more information on the internal testing of the model, internal safeguards for preventing systemic risks, and other internal procedures and measures the provider has taken to mitigate such risks.",
        "context_id": "article_92_paragraph_7",
        "metadata_type": "article"
    },
    {
        "question": "What specific action may the Commission request providers to undertake concerning the obligations detailed in Articles 53 and 54?",
        "ground_truth_answer": "Where necessary and appropriate, the Commission may request providers to take appropriate measures to comply with the obligations set out in Articles 53 and 54.",
        "context_id": "article_93_paragraph_1_point_a",
        "metadata_type": "article"
    },
    {
        "question": "Under what conditions may the Commission request providers to implement mitigation measures?",
        "ground_truth_answer": "The Commission may request providers to implement mitigation measures where it is necessary and appropriate, and where the evaluation carried out in accordance with Article 92 has given rise to serious and substantiated concern of a systemic risk at Union level.",
        "context_id": "article_93_paragraph_1_point_b",
        "metadata_type": "article"
    },
    {
        "question": "Under what conditions may the Commission request providers to restrict the making available on the market, withdraw, or recall a model?",
        "ground_truth_answer": "The Commission may request providers to restrict the making available on the market, withdraw, or recall a model where it is necessary and appropriate.",
        "context_id": "article_93_paragraph_1_point_c",
        "metadata_type": "article"
    },
    {
        "question": "What actions may the Commission take if the provider of a general-purpose AI model with systemic risk offers commitments to implement mitigation measures to address a systemic risk at Union level during a structured dialogue?",
        "ground_truth_answer": "If the provider of a general-purpose AI model with systemic risk offers commitments to implement mitigation measures to address a systemic risk at Union level during a structured dialogue, the Commission may, by decision, make those commitments binding and declare that there are no further grounds for action.",
        "context_id": "article_93_paragraph_3",
        "metadata_type": "article"
    },
    {
        "question": "Which article of Regulation (EU) 2019/1020 applies to the providers of general-purpose AI models, and with what condition?",
        "ground_truth_answer": "Article 18 of Regulation (EU) 2019/1020 shall apply mutatis mutandis to the providers of the general-purpose AI model, without prejudice to more specific procedural rights provided for in this Regulation.",
        "context_id": "article_94_paragraph_1",
        "metadata_type": "article"
    },
    {
        "question": "What is the primary purpose of the codes of conduct encouraged by the AI Office and the Member States, and which AI systems are these codes intended to cover?",
        "ground_truth_answer": "The primary purpose of the codes of conduct is to foster the voluntary application of some or all of the requirements set out in Chapter III, Section 2 to AI systems other than high-risk AI systems, taking into account available technical solutions and industry best practices.",
        "context_id": "article_95_paragraph_1",
        "metadata_type": "article"
    },
    {
        "question": "What is an element that codes of conduct, facilitated by the AI Office and Member States, might include concerning the voluntary application of specific requirements to AI systems?",
        "ground_truth_answer": "Codes of conduct might include elements such as assessing and minimising the impact of AI systems on environmental sustainability, including as regards energy-efficient programming and techniques for the efficient design, training and use of AI.",
        "context_id": "article_95_paragraph_2_point_b",
        "metadata_type": "article"
    },
    {
        "question": "What specific elements should codes of conduct concerning the voluntary application of requirements to AI systems facilitate regarding the design of AI systems?",
        "ground_truth_answer": "Codes of conduct should facilitate an inclusive and diverse design of AI systems, including through the establishment of inclusive and diverse development teams and the promotion of stakeholders’ participation in that process.",
        "context_id": "article_95_paragraph_2_point_d",
        "metadata_type": "article"
    },
    {
        "question": "What specific elements are mentioned that codes of conduct, facilitated by the AI Office and the Member States, should address concerning the impact of AI systems?",
        "ground_truth_answer": "The codes of conduct should address elements such as assessing and preventing the negative impact of AI systems on vulnerable persons or groups of vulnerable persons, including as regards accessibility for persons with a disability, as well as on gender equality.",
        "context_id": "article_95_paragraph_2_point_e",
        "metadata_type": "article"
    },
    {
        "question": "Who is permitted to draw up codes of conduct for AI systems?",
        "ground_truth_answer": "Codes of conduct for AI systems may be drawn up by individual providers or deployers of AI systems, by organisations representing them, or by both. This process can include the involvement of any interested stakeholders and their representative organisations, such as civil society organisations and academia.",
        "context_id": "article_95_paragraph_3",
        "metadata_type": "article"
    },
    {
        "question": "What specific areas will the Commission's guidelines on the practical implementation of this Regulation particularly address?",
        "ground_truth_answer": "The Commission's guidelines will particularly address the application of the requirements and obligations referred to in Articles 8 to 15 and in Article 25.",
        "context_id": "article_96_paragraph_1_point_a",
        "metadata_type": "article"
    },
    {
        "question": "What specific details must the Commission's guidelines include regarding the relationship between this Regulation and other Union law?",
        "ground_truth_answer": "The Commission's guidelines must include detailed information on the relationship of this Regulation with the Union harmonisation legislation listed in Annex I, as well as with other relevant Union law, including as regards consistency in their enforcement.",
        "context_id": "article_96_paragraph_1_point_e",
        "metadata_type": "article"
    },
    {
        "question": "What specific factors must the Commission consider when developing guidelines for the practical implementation of this Regulation?",
        "ground_truth_answer": "When developing these guidelines, the Commission must pay particular attention to the needs of SMEs (including start-ups), local public authorities, and sectors most likely to be affected by this Regulation. The guidelines must also take due account of the generally acknowledged state of the art on AI, as well as relevant harmonised standards and common specifications referred to in Articles 40 and 41, or those harmonised standards or technical specifications set out pursuant to Union harmonisation law.",
        "context_id": "article_96_paragraph_1_paragraph_11",
        "metadata_type": "article"
    },
    {
        "question": "For what initial period is the power to adopt delegated acts conferred on the Commission, and when does this period begin?",
        "ground_truth_answer": "The power to adopt delegated acts referred to in Article 6(6) and (7), Article 7(1) and (3), Article 11(3), Article 43(5) and (6), Article 47(5), Article 51(3), Article 52(4) and Article 53(5) and (6) shall be conferred on the Commission for a period of five years from 1 August 2024.",
        "context_id": "article_97_paragraph_2",
        "metadata_type": "article"
    },
    {
        "question": "Which bodies can revoke the delegation of power referred to in Article 6(6) and (7), Article 7(1) and (3), Article 11(3), Article 43(5) and (6), Article 47(5), Article 51(3), Article 52(4) and Article 53(5) and (6), and when does a decision of revocation take effect?",
        "ground_truth_answer": "The delegation of power may be revoked at any time by the European Parliament or by the Council. A decision of revocation shall take effect the day following its publication in the Official Journal of the European Union or at a later date specified therein.",
        "context_id": "article_97_paragraph_3",
        "metadata_type": "article"
    },
    {
        "question": "Under what conditions does a delegated act, adopted pursuant to specific articles like Article 6(6) or (7) or Article 43(5) or (6) of the EU AI Act, enter into force?",
        "ground_truth_answer": "A delegated act adopted pursuant to specified articles shall enter into force only if no objection has been expressed by either the European Parliament or the Council within a period of three months of notification of that act to both bodies. Alternatively, it can enter into force if, before the expiry of that period, the European Parliament and the Council have both informed the Commission that they will not object. This three-month period may be extended by an additional three months at the initiative of either the European Parliament or the Council.",
        "context_id": "article_97_paragraph_6",
        "metadata_type": "article"
    },
    {
        "question": "What are the administrative fines for non-compliance with the prohibited AI practices mentioned in Article 5?",
        "ground_truth_answer": "Non-compliance with the prohibition of the AI practices referred to in Article 5 shall be subject to administrative fines of up to EUR 35,000,000 or, if the offender is an undertaking, up to 7% of its total worldwide annual turnover for the preceding financial year, whichever is higher.",
        "context_id": "article_99_paragraph_3",
        "metadata_type": "article"
    },
    {
        "question": "Which specific articles outlining requirements and obligations for notified bodies, if not complied with, can result in administrative fines under Article 99(4)(f)?",
        "ground_truth_answer": "Non-compliance with the requirements and obligations of notified bodies pursuant to Article 31, Article 33(1), (3) and (4), or Article 34 can lead to administrative fines.",
        "context_id": "article_99_paragraph_4_point_f",
        "metadata_type": "article"
    },
    {
        "question": "What are the administrative fines for supplying incorrect, incomplete, or misleading information to notified bodies or national competent authorities under Article 99, paragraph 5 of the EU AI Act?",
        "ground_truth_answer": "Administrative fines for supplying incorrect, incomplete or misleading information to notified bodies or national competent authorities can be up to EUR 7,500,000 or, if the offender is an undertaking, up to 1% of its total worldwide annual turnover for the preceding financial year, whichever is higher.",
        "context_id": "article_99_paragraph_5",
        "metadata_type": "article"
    },
    {
        "question": "How are fines determined for SMEs, including start-ups, under Article 99 of the EU AI Act?",
        "ground_truth_answer": "For SMEs, including start-ups, each fine referred to in Article 99 shall be up to the percentages or amount referred to in paragraphs 3, 4 and 5 of that Article, whichever of those is lower.",
        "context_id": "article_99_paragraph_6",
        "metadata_type": "article"
    },
    {
        "question": "When deciding on an administrative fine, what specific circumstance related to previous fines imposed by other authorities should be taken into account?",
        "ground_truth_answer": "When deciding on an administrative fine, authorities should consider whether administrative fines have already been applied by other authorities to the same operator for infringements of other Union or national law, provided these infringements result from the same activity or omission that constitutes a relevant infringement of this Regulation.",
        "context_id": "article_99_paragraph_7_point_c",
        "metadata_type": "article"
    },
    {
        "question": "What specific aspect of cooperation does the European Data Protection Supervisor consider when deciding whether to impose an administrative fine or determine its amount on Union institutions, bodies, offices, and agencies?",
        "ground_truth_answer": "The European Data Protection Supervisor considers the degree of cooperation in order to remedy the infringement and mitigate its possible adverse effects, including compliance with any measures previously ordered by the Supervisor against the Union institution, body, office, or agency concerned with regard to the same subject matter.",
        "context_id": "article_100_paragraph_1_point_d",
        "metadata_type": "article"
    },
    {
        "question": "What is the maximum administrative fine for non-compliance with the prohibition of the AI practices referred to in Article 5?",
        "ground_truth_answer": "Non-compliance with the prohibition of the AI practices referred to in Article 5 shall be subject to administrative fines of up to EUR 1,500,000.",
        "context_id": "article_100_paragraph_2",
        "metadata_type": "article"
    },
    {
        "question": "What is the maximum administrative fine for non-compliance with the EU AI Act's requirements or obligations, excluding those specified in Article 5?",
        "ground_truth_answer": "Non-compliance with the AI system's requirements or obligations under the Regulation, other than those laid down in Article 5, shall be subject to administrative fines of up to EUR 750,000.",
        "context_id": "article_100_paragraph_3",
        "metadata_type": "article"
    },
    {
        "question": "What opportunity must the European Data Protection Supervisor provide to a Union institution, body, office, or agency before taking decisions pursuant to Article 100?",
        "ground_truth_answer": "Before taking decisions pursuant to Article 100, the European Data Protection Supervisor must give the Union institution, body, office or agency which is the subject of the proceedings the opportunity of being heard on the matter regarding the possible infringement.",
        "context_id": "article_100_paragraph_4",
        "metadata_type": "article"
    },
    {
        "question": "What are the limitations on the entitlement of parties concerned to access the European Data Protection Supervisor’s file in proceedings under Article 100, paragraph 5?",
        "ground_truth_answer": "The entitlement to access the European Data Protection Supervisor’s file is subject to the legitimate interest of individuals or undertakings in the protection of their personal data or business secrets.",
        "context_id": "article_100_paragraph_5",
        "metadata_type": "article"
    },
    {
        "question": "What is the maximum fine the Commission may impose on providers of general-purpose AI models?",
        "ground_truth_answer": "The Commission may impose fines on providers of general-purpose AI models not exceeding 3% of their annual total worldwide turnover in the preceding financial year or EUR 15,000,000, whichever amount is higher.",
        "context_id": "article_101_paragraph_1",
        "metadata_type": "article"
    },
    {
        "question": "What are the maximum fines the Commission may impose on providers of general-purpose AI models, according to Article 101 (a)?",
        "ground_truth_answer": "The Commission may impose on providers of general-purpose AI models fines not exceeding 3 % of their annual total worldwide turnover in the preceding financial year or EUR 15,000,000, whichever is higher.",
        "context_id": "article_101_paragraph_1_point_a",
        "metadata_type": "article"
    },
    {
        "question": "Under what specific conditions can the Commission impose fines on providers of general-purpose AI models if their actions are found to be intentional or negligent?",
        "ground_truth_answer": "The Commission can impose fines if a provider of general-purpose AI models intentionally or negligently failed to comply with a request for a document or for information pursuant to Article 91, or supplied incorrect, incomplete or misleading information.",
        "context_id": "article_101_paragraph_1_point_b",
        "metadata_type": "article"
    },
    {
        "question": "Under what conditions can the Commission impose fines on providers of general-purpose AI models?",
        "ground_truth_answer": "The Commission can impose fines on providers of general-purpose AI models when it finds that the provider intentionally or negligently failed to comply with a measure requested under Article 93.",
        "context_id": "article_101_paragraph_1_point_c",
        "metadata_type": "article"
    },
    {
        "question": "What are the potential fines that the Commission may impose on providers of general-purpose AI models who intentionally or negligently fail to make available access to their models for evaluation pursuant to Article 92?",
        "ground_truth_answer": "The Commission may impose on providers of general-purpose AI models fines not exceeding 3% of their annual total worldwide turnover in the preceding financial year or EUR 15,000,000, whichever is higher.",
        "context_id": "article_101_paragraph_1_point_d",
        "metadata_type": "article"
    },
    {
        "question": "What criteria does the Commission take into account when determining the amount of a fine or periodic penalty payment imposed on providers of general-purpose AI models?",
        "ground_truth_answer": "When fixing the amount of a fine or periodic penalty payment, the Commission considers the nature, gravity, and duration of the infringement, while also observing the principles of proportionality and appropriateness. Additionally, the Commission takes into account commitments made in accordance with Article 93(3) or in relevant codes of practice as per Article 56.",
        "context_id": "article_101_paragraph_1_paragraph_11",
        "metadata_type": "article"
    },
    {
        "question": "What powers does the Court of Justice of the European Union possess regarding decisions of the Commission to fix a fine under Article 101?",
        "ground_truth_answer": "The Court of Justice of the European Union has unlimited jurisdiction to review decisions of the Commission fixing a fine under Article 101. It may cancel, reduce, or increase the fine imposed.",
        "context_id": "article_101_paragraph_5",
        "metadata_type": "article"
    },
    {
        "question": "By what date must AI systems that are components of large-scale IT systems established by the legal acts listed in Annex X, and placed on the market or put into service before 2 August 2027, be brought into compliance with this Regulation?",
        "ground_truth_answer": "AI systems that are components of the large-scale IT systems established by the legal acts listed in Annex X, and placed on the market or put into service before 2 August 2027, shall be brought into compliance with this Regulation by 31 December 2030.",
        "context_id": "article_111_paragraph_1",
        "metadata_type": "article"
    },
    {
        "question": "By what date must providers and deployers of high-risk AI systems intended for use by public authorities comply with the requirements and obligations of this Regulation?",
        "ground_truth_answer": "Providers and deployers of high-risk AI systems intended to be used by public authorities must comply with the requirements and obligations of this Regulation by 2 August 2030.",
        "context_id": "article_111_paragraph_2",
        "metadata_type": "article"
    },
    {
        "question": "By what date must providers of general-purpose AI models that were placed on the market before 2 August 2025 comply with the obligations laid down in this Regulation?",
        "ground_truth_answer": "Providers of general-purpose AI models that were placed on the market before 2 August 2025 must comply with the obligations laid down in this Regulation by 2 August 2027.",
        "context_id": "article_111_paragraph_3",
        "metadata_type": "article"
    },
    {
        "question": "How frequently is the Commission required to assess the need for amendments to Annex III and the list of prohibited AI practices in Article 5?",
        "ground_truth_answer": "The Commission is required to assess the need for amendment of the list set out in Annex III and of the list of prohibited AI practices laid down in Article 5 once a year following the entry into force of the Regulation, and until the end of the period of the delegation of power laid down in Article 97.",
        "context_id": "article_112_paragraph_1",
        "metadata_type": "article"
    },
    {
        "question": "What must the Commission's report on the evaluation and review of this Regulation include?",
        "ground_truth_answer": "The Commission's report on the evaluation and review of this Regulation must include an assessment regarding the structure of enforcement and the possible need for a Union agency to resolve any identified shortcomings.",
        "context_id": "article_112_paragraph_3",
        "metadata_type": "article"
    },
    {
        "question": "What are the specific evaluation tasks the Commission is mandated to perform regarding the AI Office by 2 August 2028, and to whom must the evaluation report be submitted?",
        "ground_truth_answer": "By 2 August 2028, the Commission shall evaluate the functioning of the AI Office, whether it has been given sufficient powers and competences to fulfil its tasks, and whether it would be relevant and needed to upgrade its enforcement competences and increase its resources for proper implementation and enforcement of the Regulation. The Commission must submit a report on its evaluation to the European Parliament and to the Council.",
        "context_id": "article_112_paragraph_5",
        "metadata_type": "article"
    },
    {
        "question": "What is the Commission's obligation regarding reporting on the energy-efficient development of general-purpose AI models, including the reporting frequency, content, recipients, and public availability?",
        "ground_truth_answer": "By 2 August 2028 and every four years thereafter, the Commission shall submit a report. This report must review the progress on the development of standardisation deliverables for the energy-efficient development of general-purpose AI models, and assess the need for further measures or actions, including binding ones. The report shall be submitted to the European Parliament and to the Council, and it shall be made public.",
        "context_id": "article_112_paragraph_6",
        "metadata_type": "article"
    },
    {
        "question": "What is the timeline and purpose of the Commission's recurring evaluation concerning the impact and effectiveness of voluntary codes of conduct for AI systems?",
        "ground_truth_answer": "By 2 August 2028 and every three years thereafter, the Commission is required to evaluate the impact and effectiveness of voluntary codes of conduct. The purpose of this evaluation is to assess how these codes foster the application of the requirements set out in Chapter III, Section 2 for AI systems other than high-risk AI systems, and possibly other additional requirements for AI systems other than high-risk AI systems, including those related to environmental sustainability.",
        "context_id": "article_112_paragraph_7",
        "metadata_type": "article"
    },
    {
        "question": "Under what circumstances is the Commission required to submit proposals to amend the Regulation, according to Article 112, paragraph 10?",
        "ground_truth_answer": "The Commission is required to submit appropriate proposals to amend the Regulation if necessary, taking into account developments in technology, the effect of AI systems on health and safety and on fundamental rights, and in light of the state of progress in the information society.",
        "context_id": "article_112_paragraph_10",
        "metadata_type": "article"
    },
    {
        "question": "What must be taken into account by any amendment to this Regulation pursuant to paragraph 10, or by relevant delegated or implementing acts, that concern sectoral Union harmonisation legislation listed in Section B of Annex I?",
        "ground_truth_answer": "Such amendments or acts must take into account the regulatory specificities of each sector, and the existing governance, conformity assessment, and enforcement mechanisms and authorities established within those sectors.",
        "context_id": "article_112_paragraph_12",
        "metadata_type": "article"
    },
    {
        "question": "By what date must the Commission carry out an assessment of the enforcement of this Regulation and report on it?",
        "ground_truth_answer": "The Commission must carry out an assessment of the enforcement of this Regulation and report on it by 2 August 2031.",
        "context_id": "article_112_paragraph_13",
        "metadata_type": "article"
    },
    {
        "question": "From what date do Chapters I and II of this Regulation apply?",
        "ground_truth_answer": "Chapters I and II of this Regulation shall apply from 2 February 2025.",
        "context_id": "article_113_paragraph_1_point_a",
        "metadata_type": "article"
    },
    {
        "question": "Which specific chapters and articles of the Regulation apply from 2 August 2025, and is there any exception to this early application date?",
        "ground_truth_answer": "Chapter III Section 4, Chapter V, Chapter VII, Chapter XII, and Article 78 of the Regulation shall apply from 2 August 2025, with the exception of Article 101.",
        "context_id": "article_113_paragraph_1_point_b",
        "metadata_type": "article"
    },
    {
        "question": "From what date do Article 6(1) and its corresponding obligations under this Regulation become applicable?",
        "ground_truth_answer": "Article 6(1) and the corresponding obligations in this Regulation shall apply from 2 August 2027.",
        "context_id": "article_113_paragraph_1_point_c",
        "metadata_type": "article"
    },
    {
        "question": "Which specific Union harmonisation legislation is identified in Annex I, point 20, and what are its particular areas of concern regarding unmanned aircraft?",
        "ground_truth_answer": "Annex I, point 20, identifies Regulation (EU) 2018/1139 of the European Parliament and of the Council of 4 July 2018. This regulation is concerned with the design, production, and placing on the market of unmanned aircraft and their engines, propellers, parts, and equipment to control them remotely, as referred to in Article 2(1), points (a) and (b) of that Regulation.",
        "context_id": "annex_i_item_1_section_point_20",
        "metadata_type": "annex"
    },
    {
        "question": "What criminal offences are referred to in Annex II, as mentioned in Article 5(1), first subparagraph, point (h)(iii)?",
        "ground_truth_answer": "The criminal offences referred to are: terrorism, trafficking in human beings, sexual exploitation of children, and child pornography, illicit trafficking in narcotic drugs or psychotropic substances, illicit trafficking in weapons, munitions or explosives, murder, grievous bodily injury, illicit trade in human organs or tissue, illicit trafficking in nuclear or radioactive materials, kidnapping, illegal restraint or hostage-taking, crimes within the jurisdiction of the International Criminal Court, unlawful seizure of aircraft or ships, rape, environmental crime, organised or armed robbery, sabotage, and participation in a criminal organisation involved in one or more of the offences listed above.",
        "context_id": "anx_II_part_1",
        "metadata_type": "annex"
    },
    {
        "question": "Which criminal offences are referred to in Article 5(1), first subparagraph, point (h)(iii) according to Annex II, Part 2?",
        "ground_truth_answer": "The criminal offences referred to in Article 5(1), first subparagraph, point (h)(iii) include terrorism, trafficking in human beings, sexual exploitation of children and child pornography, illicit trafficking in narcotic drugs or psychotropic substances, illicit trafficking in weapons, munitions or explosives, murder, grievous bodily injury, illicit trade in human organs or tissue, illicit trafficking in nuclear or radioactive materials, kidnapping, illegal restraint or hostage-taking, crimes within the jurisdiction of the International Criminal Court, unlawful seizure of aircraft or ships, rape, environmental crime, organised or armed robbery, sabotage, and participation in a criminal organisation involved in one or more of the listed offences.",
        "context_id": "anx_II_part_2",
        "metadata_type": "annex"
    },
    {
        "question": "Which specific biometric AI systems are not included in the high-risk classification under ANNEX III 1. (a) of the EU AI Act?",
        "ground_truth_answer": "AI systems intended to be used for biometric verification, where the sole purpose is to confirm that a specific natural person is the person he or she claims to be, are not included in the high-risk classification under ANNEX III 1. (a).",
        "context_id": "annex_iii_point_1_point_a",
        "metadata_type": "annex"
    },
    {
        "question": "Which type of AI system, involving biometrics, is classified as high-risk under Annex III 1. (b) of the EU AI Act?",
        "ground_truth_answer": "An AI system intended to be used for biometric categorisation, according to sensitive or protected attributes or characteristics based on the inference of those attributes or characteristics.",
        "context_id": "annex_iii_point_1_point_b",
        "metadata_type": "annex"
    },
    {
        "question": "Which specific type of AI system, when intended for emotion recognition and permitted under relevant Union or national law, is classified as high-risk according to ANNEX III 1. (c)?",
        "ground_truth_answer": "AI systems intended to be used for emotion recognition, provided their use is permitted under relevant Union or national law, are classified as high-risk pursuant to ANNEX III 1. (c).",
        "context_id": "annex_iii_point_1_point_c",
        "metadata_type": "annex"
    },
    {
        "question": "Which specific AI systems are classified as high-risk under Article 6(2) when intended for use as safety components within critical infrastructure?",
        "ground_truth_answer": "AI systems intended to be used as safety components in the management and operation of critical digital infrastructure, road traffic, or in the supply of water, gas, heating or electricity are classified as high-risk under Article 6(2).",
        "context_id": "annex_iii_point_2",
        "metadata_type": "annex"
    },
    {
        "question": "Which specific AI systems in the area of education and vocational training are classified as high-risk under Annex III, point 3, (a) of the EU AI Act?",
        "ground_truth_answer": "AI systems intended to be used to determine access or admission or to assign natural persons to educational and vocational training institutions at all levels are classified as high-risk under Annex III, point 3, (a).",
        "context_id": "annex_iii_point_3_point_a",
        "metadata_type": "annex"
    },
    {
        "question": "Which specific AI systems are classified as high-risk under the 'Education and vocational training' area, as per Annex III, point 3(b) of the EU AI Act?",
        "ground_truth_answer": "AI systems intended to be used to evaluate learning outcomes are classified as high-risk. This includes situations where those outcomes are used to steer the learning process of natural persons in educational and vocational training institutions at all levels.",
        "context_id": "annex_iii_point_3_point_b",
        "metadata_type": "annex"
    },
    {
        "question": "Which AI systems within the education and vocational training sector are classified as high-risk under Annex III 3. (c) of the EU AI Act?",
        "ground_truth_answer": "AI systems intended to be used for the purpose of assessing the appropriate level of education that an individual will receive or will be able to access, in the context of or within educational and vocational training institutions at all levels.",
        "context_id": "annex_iii_point_3_point_c",
        "metadata_type": "annex"
    },
    {
        "question": "According to Annex III, point 3 (d) of the EU AI Act, what specific type of AI system used in education and vocational training is classified as high-risk under Article 6(2)?",
        "ground_truth_answer": "AI systems intended to be used for monitoring and detecting prohibited behaviour of students during tests in the context of or within educational and vocational training institutions at all levels.",
        "context_id": "annex_iii_point_3_point_d",
        "metadata_type": "annex"
    },
    {
        "question": "Which specific AI systems are designated as high-risk under the EU AI Act in the domain of employment, workers’ management, and access to self-employment?",
        "ground_truth_answer": "AI systems intended for the recruitment or selection of natural persons are considered high-risk. This specifically includes systems used to place targeted job advertisements, analyse and filter job applications, and evaluate candidates.",
        "context_id": "annex_iii_point_4_point_a",
        "metadata_type": "annex"
    },
    {
        "question": "Which AI systems intended for use in employment, workers’ management, and access to self-employment are classified as high-risk pursuant to Article 6(2)?",
        "ground_truth_answer": "AI systems intended to be used to make decisions affecting terms of work-related relationships, the promotion or termination of work-related contractual relationships, to allocate tasks based on individual behaviour or personal traits or characteristics, or to monitor and evaluate the performance and behaviour of persons in such relationships are considered high-risk.",
        "context_id": "annex_iii_point_4_point_b",
        "metadata_type": "annex"
    },
    {
        "question": "Which specific AI systems, when used by public authorities for essential public services and benefits, are classified as high-risk under Annex III 5.(a) of the EU AI Act?",
        "ground_truth_answer": "AI systems intended to be used by public authorities or on behalf of public authorities that evaluate the eligibility of natural persons for essential public assistance benefits and services, including healthcare services, or that grant, reduce, revoke, or reclaim such benefits and services, are classified as high-risk.",
        "context_id": "annex_iii_point_5_point_a",
        "metadata_type": "annex"
    },
    {
        "question": "Under ANNEX III 5. (b) of the EU AI Act, which AI systems used for evaluating creditworthiness are classified as high-risk?",
        "ground_truth_answer": "AI systems intended to be used to evaluate the creditworthiness of natural persons or establish their credit score are classified as high-risk, with the exception of AI systems used for the purpose of detecting financial fraud.",
        "context_id": "annex_iii_point_5_point_b",
        "metadata_type": "annex"
    },
    {
        "question": "Which specific AI systems are classified as high-risk under Annex III 5. (c) of the EU AI Act when used in the context of life and health insurance?",
        "ground_truth_answer": "AI systems intended to be used for risk assessment and pricing in relation to natural persons in the case of life and health insurance are classified as high-risk AI systems pursuant to Article 6(2).",
        "context_id": "annex_iii_point_5_point_c",
        "metadata_type": "annex"
    },
    {
        "question": "Which specific AI systems related to emergency services are classified as high-risk under Annex III, point 5(d) of the EU AI Act concerning access to essential public services?",
        "ground_truth_answer": "AI systems intended to evaluate and classify emergency calls by natural persons, AI systems used to dispatch or establish priority in the dispatching of emergency first response services (including by police, firefighters, and medical aid), and emergency healthcare patient triage systems are classified as high-risk.",
        "context_id": "annex_iii_point_5_point_d",
        "metadata_type": "annex"
    },
    {
        "question": "Which high-risk AI systems are described in ANNEX III 6. (a) concerning law enforcement, and under what conditions?",
        "ground_truth_answer": "According to ANNEX III 6. (a), high-risk AI systems in law enforcement are those intended to be used by or on behalf of law enforcement authorities, or by Union institutions, bodies, offices or agencies in support of law enforcement authorities, for the purpose of assessing the risk of a natural person becoming the victim of criminal offences. Their use is considered high-risk only in so far as it is permitted under relevant Union or national law.",
        "context_id": "annex_iii_point_6_point_a",
        "metadata_type": "annex"
    },
    {
        "question": "Under Annex III, point 6, point b of the EU AI Act, which specific AI systems used in law enforcement are categorized as high-risk?",
        "ground_truth_answer": "AI systems intended to be used by or on behalf of law enforcement authorities or by Union institutions, bodies, offices or agencies in support of law enforcement authorities as polygraphs or similar tools.",
        "context_id": "annex_iii_point_6_point_b",
        "metadata_type": "annex"
    },
    {
        "question": "According to Annex III, point 6, sub-point c, which specific AI systems used in law enforcement are classified as high-risk?",
        "ground_truth_answer": "AI systems intended to be used by or on behalf of law enforcement authorities, or by Union institutions, bodies, offices or agencies, in support of law enforcement authorities to evaluate the reliability of evidence in the course of the investigation or prosecution of criminal offences, are classified as high-risk.",
        "context_id": "annex_iii_point_6_point_c",
        "metadata_type": "annex"
    },
    {
        "question": "Which specific AI systems intended for use in law enforcement are classified as high-risk under Annex III, point 6(d)?",
        "ground_truth_answer": "Under Annex III, point 6(d), high-risk AI systems in law enforcement include those intended to be used by law enforcement authorities for assessing the risk of a natural person offending or re-offending (not solely based on the profiling of natural persons as referred to in Article 3(4) of Directive (EU) 2016/680), or to assess personality traits and characteristics or past criminal behaviour of natural persons or groups.",
        "context_id": "annex_iii_point_6_point_d",
        "metadata_type": "annex"
    },
    {
        "question": "What type of AI system, when used in law enforcement, is classified as high-risk under ANNEX III 6. (e) of the EU AI Act?",
        "ground_truth_answer": "Under ANNEX III 6. (e), AI systems intended to be used by or on behalf of law enforcement authorities or by Union institutions, bodies, offices or agencies in support of law enforcement authorities for the profiling of natural persons as referred to in Article 3(4) of Directive (EU) 2016/680 in the course of the detection, investigation or prosecution of criminal offences are classified as high-risk.",
        "context_id": "annex_iii_point_6_point_e",
        "metadata_type": "annex"
    },
    {
        "question": "Which AI systems are classified as high-risk under Article 6(2) when used as polygraphs or similar tools by public authorities or Union bodies in migration, asylum, and border control management?",
        "ground_truth_answer": "AI systems intended to be used by or on behalf of competent public authorities or by Union institutions, bodies, offices or agencies as polygraphs or similar tools are classified as high-risk in the area of migration, asylum, and border control management, provided their use is permitted under relevant Union or national law.",
        "context_id": "annex_iii_point_7_point_a",
        "metadata_type": "annex"
    },
    {
        "question": "What is the specific purpose of AI systems that are considered high-risk under Annex III 7. (b) in the domain of migration, asylum, and border control management?",
        "ground_truth_answer": "AI systems considered high-risk under Annex III 7. (b) in this domain are intended to be used by or on behalf of competent public authorities or by Union institutions, bodies, offices or agencies to assess a risk, including a security risk, a risk of irregular migration, or a health risk, posed by a natural person who intends to enter or who has entered into the territory of a Member State.",
        "context_id": "annex_iii_point_7_point_b",
        "metadata_type": "annex"
    },
    {
        "question": "What specific types of AI systems are classified as high-risk under ANNEX III 7. (c) concerning migration, asylum, and border control management?",
        "ground_truth_answer": "High-risk AI systems in this area include those intended for use by or on behalf of competent public authorities or by Union institutions, bodies, offices or agencies to assist competent public authorities for the examination of applications for asylum, visa or residence permits, for associated complaints regarding the eligibility of natural persons applying for a status, and for related assessments of the reliability of evidence.",
        "context_id": "annex_iii_point_7_point_c",
        "metadata_type": "annex"
    },
    {
        "question": "Regarding high-risk AI systems under Article 6(2) in the context of migration, asylum, and border control management, for what specific purpose are these systems intended to be used that makes them high-risk, and what activity is explicitly excluded from this classification?",
        "ground_truth_answer": "AI systems are classified as high-risk if they are intended to be used by or on behalf of competent public authorities, or by Union institutions, bodies, offices or agencies, for the purpose of detecting, recognising or identifying natural persons. The verification of travel documents is explicitly excluded from this classification.",
        "context_id": "annex_iii_point_7_point_d",
        "metadata_type": "annex"
    },
    {
        "question": "Which specific functions of AI systems used in the administration of justice and democratic processes are classified as high-risk under Article 6(2) of the EU AI Act, according to Annex III?",
        "ground_truth_answer": "AI systems intended to be used by a judicial authority or on their behalf to assist a judicial authority in researching and interpreting facts and the law and in applying the law to a concrete set of facts, or to be used in a similar way in alternative dispute resolution, are classified as high-risk.",
        "context_id": "annex_iii_point_8_point_a",
        "metadata_type": "annex"
    },
    {
        "question": "Which specific types of AI systems used in democratic processes are NOT classified as high-risk under ANNEX III 8. (b) of the EU AI Act?",
        "ground_truth_answer": "AI systems to the output of which natural persons are not directly exposed are not classified as high-risk. This includes tools used to organise, optimise, or structure political campaigns from an administrative or logistical point of view.",
        "context_id": "annex_iii_point_8_point_b",
        "metadata_type": "annex"
    },
    {
        "question": "What information regarding the development process and third-party components must be included in the detailed description within the technical documentation for an AI system?",
        "ground_truth_answer": "The detailed description within the technical documentation must include the methods and steps performed for the development of the AI system. Additionally, it must cover, where relevant, recourse to pre-trained systems or tools provided by third parties and how those were used, integrated, or modified by the provider.",
        "context_id": "annex_iv_point_2_point_a",
        "metadata_type": "annex"
    },
    {
        "question": "What specific details about an AI system's design specifications and development process are required in its technical documentation?",
        "ground_truth_answer": "The technical documentation must include a detailed description of the elements of the AI system and of the process for its development. This includes the general logic of the AI system and of the algorithms, key design choices (including their rationale and assumptions, particularly regarding target persons or groups), the main classification choices, what the system is designed to optimise for and the relevance of its parameters, the description of the expected output and output quality, and decisions about any possible trade-offs made regarding technical solutions to comply with Chapter III, Section 2 requirements.",
        "context_id": "annex_iv_point_2_point_b",
        "metadata_type": "annex"
    },
    {
        "question": "What specific information related to an AI system's development must be included in the technical documentation?",
        "ground_truth_answer": "The technical documentation must include a description of the system architecture, detailing how software components build on or feed into each other and integrate into the overall processing, as well as the computational resources used to develop, train, test, and validate the AI system.",
        "context_id": "annex_iv_point_2_point_c",
        "metadata_type": "annex"
    },
    {
        "question": "What specific data-related information must be included in the technical documentation concerning the development process of an AI system, as applicable?",
        "ground_truth_answer": "As applicable to the relevant AI system, the technical documentation must contain a detailed description of the process for its development, including data requirements in terms of datasheets describing the training methodologies and techniques, the training data sets used (including a general description, provenance, scope, and main characteristics), how the data was obtained and selected, labelling procedures (e.g., for supervised learning), and data cleaning methodologies (e.g., outliers detection).",
        "context_id": "annex_iv_point_2_point_d",
        "metadata_type": "annex"
    },
    {
        "question": "What types of assessments are required to be part of the detailed description of an AI system's development process in the technical documentation, according to Annex IV, point 2, point (e) of the EU AI Act?",
        "ground_truth_answer": "The detailed description of an AI system's development process within the technical documentation must include an assessment of the human oversight measures needed in accordance with Article 14, and an assessment of the technical measures needed to facilitate the interpretation of the outputs of AI systems by the deployers, in accordance with Article 13(3), point (d).",
        "context_id": "annex_iv_point_2_point_e",
        "metadata_type": "annex"
    },
    {
        "question": "What specific details must the technical documentation include concerning pre-determined changes to an AI system and its continuous compliance with requirements?",
        "ground_truth_answer": "The technical documentation must include, where applicable, a detailed description of pre-determined changes to the AI system and its performance. Additionally, it must contain all relevant information related to the technical solutions adopted to ensure continuous compliance of the AI system with the requirements set out in Chapter III, Section 2.",
        "context_id": "annex_iv_point_2_point_f",
        "metadata_type": "annex"
    },
    {
        "question": "What specific information must the technical documentation for an AI system contain concerning its validation and testing procedures?",
        "ground_truth_answer": "The technical documentation for an AI system must contain a detailed description of the validation and testing procedures used, including information about the validation and testing data used and their main characteristics. It must also include metrics used to measure accuracy, robustness, compliance with other relevant requirements set out in Chapter III, Section 2, as well as potentially discriminatory impacts, and test logs and all test reports dated and signed by the responsible persons, including with regard to pre-determined changes as referred to under point (f).",
        "context_id": "annex_iv_point_2_point_g",
        "metadata_type": "annex"
    },
    {
        "question": "What detailed information regarding an AI system's performance and potential risks must be included in the technical documentation, as specified in Annex IV, point 3 of the EU AI Act?",
        "ground_truth_answer": "The technical documentation must include detailed information about the AI system's capabilities and limitations in performance, specifically mentioning the degrees of accuracy for specific persons or groups of persons on which the system is intended to be used and the overall expected level of accuracy in relation to its intended purpose. It must also detail the foreseeable unintended outcomes and sources of risks to health and safety, fundamental rights, and discrimination in view of the AI system's intended purpose.",
        "context_id": "annex_iv_point_3",
        "metadata_type": "annex"
    },
    {
        "question": "What information regarding standards and technical specifications must be included in the technical documentation referenced in Article 11(1) for an AI system?",
        "ground_truth_answer": "The technical documentation must contain either a list of harmonised standards applied in full or in part, the references of which have been published in the Official Journal of the European Union; or, where no such harmonised standards have been applied, a detailed description of the solutions adopted to meet the requirements set out in Chapter III, Section 2, including a list of other relevant standards and technical specifications applied.",
        "context_id": "annex_iv_point_7",
        "metadata_type": "annex"
    },
    {
        "question": "What specific information concerning post-market performance evaluation must be included in the technical documentation for an AI system, as referred to in Article 11(1)?",
        "ground_truth_answer": "The technical documentation shall contain a detailed description of the system in place to evaluate the AI system performance in the post-market phase in accordance with Article 72, including the post-market monitoring plan referred to in Article 72(3).",
        "context_id": "annex_iv_point_9",
        "metadata_type": "annex"
    },
    {
        "question": "What statement must be included in the EU declaration of conformity if an AI system processes personal data?",
        "ground_truth_answer": "Where an AI system involves the processing of personal data, the EU declaration of conformity shall contain a statement that that AI system complies with Regulations (EU) 2016/679 and (EU) 2018/1725 and Directive (EU) 2016/680.",
        "context_id": "annex_v_point_5",
        "metadata_type": "annex"
    },
    {
        "question": "What information, if applicable, must the EU declaration of conformity contain regarding a notified body, the conformity assessment procedure, and any issued certificate?",
        "ground_truth_answer": "Where applicable, the EU declaration of conformity shall contain the name and identification number of the notified body, a description of the conformity assessment procedure performed, and identification of the certificate issued.",
        "context_id": "annex_v_point_7",
        "metadata_type": "annex"
    },
    {
        "question": "What information must be included in the notification to the provider or its authorised representative concerning the assessment of the quality management system?",
        "ground_truth_answer": "The notification must contain the conclusions of the assessment of the quality management system and the reasoned assessment decision.",
        "context_id": "annex_vii_item_2_section_point_32",
        "metadata_type": "annex"
    },
    {
        "question": "What is the procedure for a provider to modify an approved quality management system or the list of AI systems covered by it, as outlined in ANNEX VII 3.4. of the EU AI Act?",
        "ground_truth_answer": "The provider must bring any intended changes to the approved quality management system or the list of AI systems covered by it to the attention of the notified body. The notified body will then examine the proposed changes, decide if the modified quality management system still meets the requirements from point 3.2 or if a reassessment is necessary, and subsequently notify the provider of its decision, including the conclusions of the examination and the reasoned assessment decision.",
        "context_id": "annex_vii_item_2_section_point_34",
        "metadata_type": "annex"
    },
    {
        "question": "What specific types of datasets is the notified body granted access to when examining technical documentation, and how can this access be provided under ANNEX VII 4.3.?",
        "ground_truth_answer": "The notified body shall be granted full access to the training, validation, and testing data sets used. This access, where relevant and limited to what is necessary, can be provided through API or other relevant technical means and tools enabling remote access, subject to security safeguards.",
        "context_id": "annex_vii_item_3_section_point_43",
        "metadata_type": "annex"
    },
    {
        "question": "What actions must a notified body take if it is not satisfied with the tests carried out by the provider during the examination of technical documentation?",
        "ground_truth_answer": "If the notified body is not satisfied with the tests carried out by the provider, the notified body shall itself directly carry out adequate tests, as appropriate.",
        "context_id": "annex_vii_item_3_section_point_44",
        "metadata_type": "annex"
    },
    {
        "question": "Under what specific conditions can a notified body be granted access to the training and trained models of a high-risk AI system, including its relevant parameters?",
        "ground_truth_answer": "A notified body can be granted access when it is necessary to assess the conformity of the high-risk AI system with the requirements set out in Chapter III, Section 2. This access is only granted after all other reasonable means to verify conformity have been exhausted and proven insufficient, and upon a reasoned request. Such access is also subject to existing Union law on the protection of intellectual property and trade secrets.",
        "context_id": "annex_vii_item_3_section_point_45",
        "metadata_type": "annex"
    },
    {
        "question": "What must a notified body's reasoned assessment decision include if it refuses to issue a Union technical documentation assessment certificate because the AI system does not meet the requirement relating to its training data?",
        "ground_truth_answer": "If an AI system's non-compliance is due to the data used to train it, the notified body's reasoned assessment decision refusing to issue the Union technical documentation assessment certificate must contain specific considerations on the quality data used to train the AI system, in particular on the reasons for non-compliance.",
        "context_id": "annex_vii_item_3_section_point_46_part_2",
        "metadata_type": "annex"
    },
    {
        "question": "What is the procedure a Notified Body follows when assessing changes to an AI system that could impact its compliance or intended purpose, after a Union technical documentation assessment certificate has been issued?",
        "ground_truth_answer": "When informed by the provider about changes to an AI system that could affect its compliance or intended purpose, the Notified Body which issued the Union technical documentation assessment certificate must assess these changes. The Notified Body then decides whether the changes necessitate a new conformity assessment in accordance with Article 43(4) or can be handled by a supplement to the existing certificate. If a supplement is decided upon, the Notified Body assesses the changes, notifies the provider of its decision, and issues a supplement to the Union technical documentation assessment certificate if the changes are approved.",
        "context_id": "annex_vii_item_3_section_point_47",
        "metadata_type": "annex"
    },
    {
        "question": "What are the specific duties of a notified body concerning the surveillance of an approved quality management system, as outlined in ANNEX VII 5.3.?",
        "ground_truth_answer": "The notified body is required to carry out periodic audits to ensure that the provider maintains and applies the quality management system and must provide the provider with an audit report. Furthermore, during these audits, the notified body has the discretion to conduct additional tests on AI systems that have received a Union technical documentation assessment certificate.",
        "context_id": "annex_vii_item_4_section_point_53",
        "metadata_type": "annex"
    },
    {
        "question": "Which specific piece of information, normally required for high-risk AI systems to be registered under Article 49(1), is exempted from submission for systems used in law enforcement or migration, asylum, and border control management as referred to in Annex III, points 1, 6 and 7?",
        "ground_truth_answer": "Electronic instructions for use.",
        "context_id": "annex_viii_item_0_section_point_12",
        "metadata_type": "annex"
    },
    {
        "question": "For AI systems to be registered in accordance with Article 49(2), what specific information must providers submit and keep up to date concerning an AI system being classified as not-high-risk?",
        "ground_truth_answer": "Providers must submit and keep up to date the condition or conditions under Article 6(3) based on which the AI system is considered to be not-high-risk.",
        "context_id": "annex_viii_item_1_section_point_6",
        "metadata_type": "annex"
    },
    {
        "question": "What specific information must providers of high-risk AI systems submit regarding their AI system's risk classification, according to ANNEX VIII 7. Section B?",
        "ground_truth_answer": "Providers must submit a short summary of the grounds on which the AI system is considered to be not-high-risk, in application of the procedure under Article 6(3). This information must also be kept up to date.",
        "context_id": "annex_viii_item_1_section_point_7",
        "metadata_type": "annex"
    },
    {
        "question": "What specific information related to fundamental rights impact assessments must deployers of high-risk AI systems submit and keep up to date when registering in accordance with Article 49(3)?",
        "ground_truth_answer": "Deployers of high-risk AI systems must submit and keep up to date a summary of the findings of the fundamental rights impact assessment conducted in accordance with Article 27.",
        "context_id": "annex_viii_item_2_section_point_4",
        "metadata_type": "annex"
    },
    {
        "question": "What specific data protection-related information must deployers submit for high-risk AI systems to be registered in accordance with Article 49(3)?",
        "ground_truth_answer": "Deployers must submit a summary of the data protection impact assessment carried out in accordance with Article 35 of Regulation (EU) 2016/679 or Article 27 of Directive (EU) 2016/680, as specified in Article 26(8) of this Regulation, where applicable.",
        "context_id": "annex_viii_item_2_section_point_5",
        "metadata_type": "annex"
    },
    {
        "question": "What specific technical details must be included in the general description of a general-purpose AI model as part of its technical documentation?",
        "ground_truth_answer": "The general description of a general-purpose AI model within its technical documentation must include information on the architecture and number of parameters.",
        "context_id": "annex_xi_item_0_section_point_1_point_d",
        "metadata_type": "annex"
    },
    {
        "question": "What information regarding inputs and outputs must be included in the general description of a general-purpose AI model within its technical documentation?",
        "ground_truth_answer": "The general description of a general-purpose AI model must include the modality (e.g., text, image) and format of its inputs and outputs.",
        "context_id": "annex_xi_item_0_section_point_1_point_e",
        "metadata_type": "annex"
    },
    {
        "question": "How can the energy consumption of a general-purpose AI model be determined for technical documentation if its exact consumption is unknown?",
        "ground_truth_answer": "If the energy consumption of a general-purpose AI model is unknown, its energy consumption may be based on information about computational resources used.",
        "context_id": "annex_xi_item_0_section_point_2_point_a",
        "metadata_type": "annex"
    },
    {
        "question": "What information regarding the design specifications and training process must be included in the technical documentation for general-purpose AI models?",
        "ground_truth_answer": "The technical documentation for general-purpose AI models must include the design specifications of the model and training process, training methodologies and techniques, key design choices including their rationale and assumptions, what the model is designed to optimise for, and the relevance of the different parameters.",
        "context_id": "annex_xi_item_0_section_point_2_point_b",
        "metadata_type": "annex"
    },
    {
        "question": "What information regarding the data used for training, testing, and validation must be included in the technical documentation for general-purpose AI models?",
        "ground_truth_answer": "The technical documentation must include information on the type and provenance of data, curation methodologies (e.g., cleaning, filtering), the number of data points, their scope and main characteristics, how the data was obtained and selected, and all other measures to detect the unsuitability of data sources and methods to detect identifiable biases, where applicable.",
        "context_id": "annex_xi_item_0_section_point_2_point_c",
        "metadata_type": "annex"
    },
    {
        "question": "What specific information related to the training of general-purpose AI models must be included in the technical documentation, according to Annex XI, Section 1, Point 2 (d)?",
        "ground_truth_answer": "The technical documentation must include information on the computational resources used to train the model (e.g., number of floating point operations), training time, and other relevant details related to the training. Additionally, where the energy consumption of the model is unknown, it may be based on information about computational resources used.",
        "context_id": "annex_xi_item_0_section_point_2_point_d",
        "metadata_type": "annex"
    },
    {
        "question": "What specific elements must be included in the evaluation strategies provided by providers of general-purpose AI models with systemic risk?",
        "ground_truth_answer": "The evaluation strategies must include evaluation criteria, metrics, and the methodology on the identification of limitations.",
        "context_id": "annex_xi_item_1_section_point_1",
        "metadata_type": "annex"
    },
    {
        "question": "What specific measures must providers of general-purpose AI models with systemic risk describe regarding adversarial testing and model adaptations?",
        "ground_truth_answer": "Providers of general-purpose AI models with systemic risk must provide a detailed description of the measures put in place for conducting internal and/or external adversarial testing (e.g. red teaming), and for model adaptations, including alignment and fine-tuning.",
        "context_id": "annex_xi_item_1_section_point_2",
        "metadata_type": "annex"
    },
    {
        "question": "What type of description is required from providers of general-purpose AI models with systemic risk regarding their system architecture?",
        "ground_truth_answer": "Providers of general-purpose AI models with systemic risk are required to provide a detailed description of the system architecture, explaining how software components build or feed into each other and integrate into the overall processing.",
        "context_id": "annex_xi_item_1_section_point_3",
        "metadata_type": "annex"
    },
    {
        "question": "What two specific technical aspects of a general-purpose AI model must be detailed in its general description, as per the information referred to in Article 53(1), point (b)?",
        "ground_truth_answer": "The general description of a general-purpose AI model must include its architecture and number of parameters.",
        "context_id": "annex_xii_point_1_point_f",
        "metadata_type": "annex"
    },
    {
        "question": "What information about the inputs and outputs of a general-purpose AI model must be included in its general description according to Annex XII?",
        "ground_truth_answer": "The general description of a general-purpose AI model must include information on the modality (e.g., text, image) and format of its inputs and outputs.",
        "context_id": "annex_xii_point_1_point_g",
        "metadata_type": "annex"
    },
    {
        "question": "What examples of technical means must be included in the description of a general-purpose AI model's development process to facilitate its integration into AI systems?",
        "ground_truth_answer": "The description must include technical means such as instructions for use, infrastructure, and tools.",
        "context_id": "annex_xii_point_2_point_a",
        "metadata_type": "annex"
    },
    {
        "question": "What information must be included regarding the inputs and outputs when describing the elements of a model and its development process, according to Annex XII 2. (b)?",
        "ground_truth_answer": "According to Annex XII 2. (b), the description must include the modality (e.g. text, image, etc.) and format of the inputs and outputs and their maximum size (e.g. context window length, etc.).",
        "context_id": "annex_xii_point_2_point_b",
        "metadata_type": "annex"
    },
    {
        "question": "What specific information must be included regarding the data used for training, testing, and validation within the description of an AI model's development process?",
        "ground_truth_answer": "The description of an AI model's development process must include information on the type and provenance of data, as well as curation methodologies, for the data used in training, testing, and validation.",
        "context_id": "annex_xii_point_2_point_c",
        "metadata_type": "annex"
    },
    {
        "question": "For the purpose of determining that a general-purpose AI model has capabilities or an impact equivalent to those set out in Article 51(1), point (a), how is the amount of computation used for training the model measured or indicated?",
        "ground_truth_answer": "The amount of computation used for training the model is measured in floating point operations or indicated by a combination of other variables such as estimated cost of training, estimated time required for the training, or estimated energy consumption for the training.",
        "context_id": "annex_xiii_point_c",
        "metadata_type": "annex"
    },
    {
        "question": "What criteria does the Commission consider to determine if a general-purpose AI model has capabilities or an impact equivalent to those set out in Article 51(1), point (a)?",
        "ground_truth_answer": "The Commission considers the input and output modalities of the model (such as text to text, text to image, multi-modality), the state-of-the-art thresholds for determining high-impact capabilities for each modality, and the specific type of inputs and outputs (e.g., biological sequences).",
        "context_id": "annex_xiii_point_d",
        "metadata_type": "annex"
    },
    {
        "question": "What criteria will the Commission consider when determining if a general-purpose AI model has capabilities or an impact equivalent to those set out in Article 51(1), point (a)?",
        "ground_truth_answer": "The Commission shall take into account the benchmarks and evaluations of capabilities of the model, including considering the number of tasks without additional training, adaptability to learn new, distinct tasks, its level of autonomy and scalability, and the tools it has access to.",
        "context_id": "annex_xiii_point_e",
        "metadata_type": "annex"
    },
    {
        "question": "When is a high impact on the internal market presumed for a general-purpose AI model?",
        "ground_truth_answer": "A high impact on the internal market is presumed when a general-purpose AI model has been made available to at least 10,000 registered business users established in the Union.",
        "context_id": "annex_xiii_point_f",
        "metadata_type": "annex"
    }
]